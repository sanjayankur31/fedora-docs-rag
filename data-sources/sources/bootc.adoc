
video::bf1xqjLeA9M[youtube,width=640,height=480,leveloffset=+4]



Before going into any further details, you may wonder about the benefits of bootable containers and why should pay attention to this technology?  Let's go through some of the main benefits.

Linux already sits at the core of containers. Bootable containers take Linux’s role a step further, letting you manage the entire OS through container-based tooling and concepts, including https://www.redhat.com/en/topics/devops/what-is-gitops[GitOps] and continuous integration/continuous delivery (CI/CD). This streamlined approach helps address the challenges of managing Linux at scale, whether you're pushing patches to different locations or bridging gaps between the operations team and the application development cycle.

While there are dozens of security benefits of image-based systems, we want to highlight the fact that the OS is shipped in the form of container images.  That means we can make use of the advancements of the past decade in container security such as advanced container security tools to address patching, scanning, validation and signing.  We can now apply container security scans on the kernel, drivers, the bootloader and more.

Similar to the security benefits, bootable containers integrate into a vast ecosystem of tools and technologies which have emerged around containers.  With bootable containers, we can build, deploy and manage our Linux systems at new scale and speed.  A consistent feedback from early adopters of bootable containers is the observation of a simplified toolchain for managing all these tasks in less time.

{bootc-upstream}/[Bootc] is at the core and center of bootable containers. It is a CLI tool that ships with a number of systemd services and timers to manage a bootable container. Among other things, bootc is responsible for downloading and queuing updates, and can be used by other higher-level tools to manage the system and inspect the system status. For that reason, bootc is an integral part of each bootable container image. For more details, please refer to the {bootc-upstream}/[bootc documentation].

Bootc systems follow the concept of an immutable operating system. Apart from the following two exceptions,  `/etc` and `/var`, all directories are mounted read-only once deployed on a physical or virtual machine. However, during a container build the entire file-system is writable.

The fact that most parts of the file system are mounted read-only is an important attribute of deployed bootable containers and something to consider carefully when preparing workloads and updates. For more information, see the xref:filesystem.adoc[Filesystem Layout page] which elaborate in great detail on the exact behavior.

At the moment, there are three distributions shipping bootable containers:

- https://fedoraproject.org/[Fedora]
- https://www.centos.org/[CentOS Stream]
- https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux[Red Hat Enterprise Linux (RHEL)]

The base images of https://fedoraproject.org[Fedora] and https://www.centos.org/centos-stream/[CentOS Stream] are listed and continuously updated on the xref:base-images.adoc[base images page]. The RHEL bootc images can be found in the https://catalog.redhat.com/software/containers/explore[Red Hat Ecosystem Catalog]; working with these requires a Red Hat account. You can get a no-cost subscription by https://developers.redhat.com/register[joining the Red Hat Developer program] in just a few clicks. You further need to login to the https://catalog.redhat.com/[Red Hat Container Registry] and register your machine with subscription-manager which is well explained in the https://www.redhat.com/en/blog/image-mode-red-hat-enterprise-linux-quick-start-guide[release blog post]. If you are using Podman Desktop, you might install the https://developers.redhat.com/articles/2024/05/07/podman-desktop-red-hat-developer-subscription[Red Hat Account Extension] which automates most of the process.

As mentioned above, bootable containers can be built with existing tooling such as https://github.com/containers/common/blob/main/docs/Containerfile.5.md[Containerfiles] and Podman. That means you can use any existing bootc base image and customize it to your needs (e.g., install further packages, copy files from the host, run config scripts, etc.) in a container build as exemplified in the following Containerfile:

[source,subs="attributes"]
----
FROM {container-fedora-full}
RUN dnf install -y [system agents] [dependencies] && dnf clean all
COPY [unpackaged application]
COPY [configuration files]
RUN [config scripts]
----

In case the bootable container will be run in `podman-bootc`, see the xref:#_podman_bootc[podman-bootc] section for setting up `podman-machine` and building the image.

For more details on building derived bootc images, see the xref:building-containers.adoc[page in the docs]. Note that bootc is still under development. On rare occasions you might encounter problems you can find in the https://gitlab.com/fedora/bootc/tracker[upstream issue tracker] (requires a GitLab account).

Updates for bootable containers happen in the form of pulling a new image and (re-)booting into it. But how do we install a fresh bootc system? While bootc supports {bootc-upstream}/bootc-install.html#using-bootc-install-to-existing-root[installing a bootc container on top of an existing system], it is more common to convert a bootable container into a so-called disk image, such as ISO, raw or qcow2 to provision a new system.

You can convert a bootable container into a disk image with the https://github.com/osbuild/bootc-image-builder[bootc-image-builder] which itself needs to be executed inside a container. `bootc-image-builder` is a feature-rich tool that further enables you to inject users, SSH keys, and define a partition layout. For more details, see the https://github.com/osbuild/bootc-image-builder?tab=readme-ov-file#-arguments[upstream documentation]. Podman Desktop also ships with a https://github.com/containers/podman-desktop-extension-bootc[bootc extension] which allows you to build and convert bootc images in just a few clicks. For more information on the extension, please see the https://developers.redhat.com/articles/2024/05/07/podman-desktop-red-hat-developer-subscription#bootc_extension_for_podman_desktop[release blog post]. Once converted, you can boot a disk image by using libvirt and qemu and other virtualization tools as described in xref:qemu-and-libvirt.adoc[in the docs].

If you are using https://podman-desktop.io[Podman Desktop], you may install the https://github.com/containers/podman-desktop-extension-bootc?tab=readme-ov-file#example-images[Podman Desktop bootc extension] to manage bootc images and automatically convert them to disk images.


You may of course run the container you've built as a container, but there are important
considerations; more on this in xref:provisioning-container.adoc[Running as a container].

https://github.com/containers/podman-bootc[podman-bootc] enables a more bootc-native experience. It is a CLI tool that allows you to easily run a local bootc image in a VM and get shell access to it. Under the hood, podman-bootc uses `bootc install to-disk` to make a disk image from the container image.

podman-bootc requires the `podman-machine` package, which provides a podman machine with a rootful connection. The bootable container has to be built inside the running machine.

NOTE: The package `podman-machine` is not required on MacOS and the locally built container can be used with `podman-bootc` as it is.

The commands to run the Fedora bootc image via podman-bootc may look as follows:

[source]
----
# Initialize a new machine with root being the default user
$ podman machine init --rootful --now
----

or start an already initialized rootful podman machine:

[source]
----
$ podman machine start
----

You can list available podman machine connections to get the connection name after starting a podman machine by:

----
$ podman system connection list
Name                         URI                                                          Identity                                                   Default     ReadWrite
podman-machine-default       ssh://core@127.0.0.1:38717/run/user/1000/podman/podman.sock  /home/user/.local/share/containers/podman/machine/machine  false       true
podman-machine-default-root  ssh://root@127.0.0.1:38717/run/podman/podman.sock            /home/user/.local/share/containers/podman/machine/machine  true        true
----

Use connection `podman-machine-default-root` as a connection for `podman build` to build the image:

----
$ podman -c podman-machine-default-root build -t <repository>/<tag> <Containerfile-dir>
----

The path `<Containerfile-dir>` is a path to the directory with only `Containerfile` and files needed for build, `<repository>/<tag>` defines how the image will be named images list.

In the end run the new machine based on the Fedora base image and specify the desired filesystem:

[source]
----
$ podman-bootc run --filesystem=xfs <repository>:<tag>
----

where `<repository>:<tag>` are the values set to the image during build, and they are visible when listing available images, e.g.:

----
$ podman -c podman-machine-default-root images
REPOSITORY                   TAG         IMAGE ID      CREATED         SIZE
quay.io/user/bootc        cups        6abca60ceab2  46 minutes ago  2.05 GB
quay.io/fedora/fedora-bootc  41          7b03571fae24  3 hours ago     1.66 GB
----

podman-bootc is still under development but perfectly capable of supporting your development flow. For installation instructions and further details, please refer to the https://github.com/containers/podman-bootc[bootc upstream docs] and xref:podman-bootc-cli.adoc[section in the Fedora docs]. If you are running on a Mac, please run `brew install --head podman-bootc` to build and install the latest version.

There are no default interactive users other than root in the base image. In the default full base images, OpenSSH is running, but there are no hardcoded credentials (passwords or SSH keys). That means that you cannot log into a booted VM without further work (e.g., injecting SSH keys for the root users). For that reason, podman-bootc automatically injects SSH keys such that you can easily get access to the VM. See the xref:authentication.adoc[section on authentication] in the docs for more information on this topic.

At this point, you know how to build a bootable container image and how to convert it into a disk image that you may want to install on your machine. Before installing Fedora/CentOS bootc, it’s recommended that you have created a customized derived container image, but this is not a hard requirement, as it is possible to enable basic system access via e.g. injecting SSH keys with kickstart or with {bootc-upstream}/bootc-install.html[bootc-install] and the `--root-ssh-authorized-keys` flag. You can find a list of examples and detailed instructions xref:bare-metal.adoc[here]. You can further find documentation on how to provision public cloud instances on xref:provisioning-aws.adoc[AWS] or xref:provisioning-gcp.adoc[GCP] and how to install on xref:provisioning-vsphere.adoc[vSphere].


IMPORTANT: Local package layering is not yet available for bootc. Track the issue https://gitlab.com/fedora/bootc/tracker/-/issues/4[here].

The figure below depicts the life cycle of a bootable container and the different steps required from building to deploying to updating bootc systems. Once a bootable container image has been built, you may convert it to a disk image. The disk image can then be used to install the content in the target environment (e.g., a public cloud instance). You might also want to push the container image to your target container registry.

image::updates.png[]

To update existing systems, the process can be repeated: that is building a new image and pushing it to the registry. Once pushed, bootc on the deployed systems can pull down the new image from the registry and reboot into the new image.

There are several ways how an individual system can be updated:

- By default, bootc systems perform **time-based updates** via a systemd timer.
- For **event-based updates**, the `bootc-fetch-apply-updates.service` can be triggered.
- **Manual updates** can be performed by running `bootc-upgrade` and rebooting the system.

An example CI/CD pipeline for a bootc image is outlined in this article,
https://developers.redhat.com/articles/2024/11/22/creating-cicd-pipelines-image-mode-rhel#[How to create CI/CD pipelines for image mode for RHEL].

Bootc further supports rollbacks via the bootc-rollback command. For more details, please refer to the xref:auto-updates[auto-updates section] which further elaborates on disconnected and bootloader updates.

The following hands-on demo shows the update process of a local VM:

video::fccox6sGCWA[youtube,width=640,height=480]


At that point, we have learned the nature of and the concepts behind bootable containers. From building on top of available bootc base images to testing them locally with podman-bootc to understanding how to update already deployed systems.

The suggested next steps are to try out the technology and browse through the other pages of this documentation.



Bugs can be reported to:

- Fedora: <https://gitlab.com/fedora/bootc/base-images>
- CentOS: <https://gitlab.com/redhat/centos-stream/containers/bootc>

You can find links to further community channels on the xref:community.adoc[community] page.

This project closely tracks CentOS and Fedora, and aims to tightly integrate with their underlying lifecycle as well as release and test infrastructure, and in particular the CentOS Stream versions are the upstream for the RHEL product.

These are the default "full" images:

- CentOS Stream 9: `{container-c9s}` (https://gitlab.com/redhat/centos-stream/containers/bootc[source repo])
- Fedora: `{container-fedora-full}` (https://gitlab.com/fedora/bootc/base-images[source repo])
- CentOS Stream 10 (in development): `{container-c10s}` (https://gitlab.com/redhat/centos-stream/containers/bootc[source repo])

For RHEL, see https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/index[RHEL 9 Image Mode].  Note that at the current time the RHEL product is Tech Preview status and this is also true of the upstream; the contents and interfaces of the base images are still subject to change.


The content set for the default images are intentionally quite large.

- `bootc`: Included in the container to perform in-place upgrades "day 2"
- `kernel`: There's a kernel
- `systemd`: systemd is included and is configured to run by default when executed as a container too
- `NetworkManager`: Full support for many complex networking types
- `podman`: Support for OCI containers
- Filesystem tools, support for LUKS, LVM, RAID etc.
- Lots of other supporting tools, such as `sos`, `jq` etc.


However, the image does *not* include hypervisor specific agents.
You may install them in derived builds. For example, on Amazon Web Services, you may want to install `cloud-init`; but this is also not required.

See xref:cloud-agents.adoc[Cloud agents] for more information.


There are also demonstration images that are much smaller that are intended to be used more as a development reference.
These images are not currently published on Quay.io but are available in the https://gitlab.com/fedora/bootc/base-images/container_registry/6333306[GitLab container registry].

This image has almost nothing; just `kernel systemd bootc`; everything else (including e.g. networking) you need to add.

Thanks for your interest in the {projname} project!  There
are multiple avenues for communication.



Use the https://discussion.fedoraproject.org/tag/bootc-initiative[bootc-initiative tag] on the
Fedora discussion forum.


There is a https://matrix.to/#/#bootc:fedoraproject.org[Matrix channel].


There is an https://gitlab.com/fedora/bootc/tracker[issue tracker repository] where
you can report bugs.


Folks working on this initiative are meeting weekly on:

- Tuesday, https://time.is/15:00+UTC[15:00 UTC]

Meetings usually happen via text chat on Matrix in the
https://matrix.to/#/#meeting-1:fedoraproject.org[#meeting-1 Matrix channel].

Sometimes, we make schedule video meetings and those will take place in
https://meet.google.com/poh-xmxm-qyc[meet.google.com/poh-xmxm-qyc].

See the following calendar entries in Fedora's Calendar:

- https://calendar.fedoraproject.org/SIGs/2026/1/20/#m11261[Weekly meeting for the Image Mode Initiative]

The notes are written in https://etherpad.opensuse.org/p/bootc-initiative-meetings[an ephemeral etherpad]
and then made available in the meeting-notes folder in the
https://gitlab.com/fedora/bootc/tracker/-/tree/main/meeting-notes?ref_type=heads[issue tracker repo].


There is also {bootc-git}/discussions[bootc upstream discussions].
* Building containers

The original Docker container model of using "layers" to model
applications has been extremely successful. This project
aims to apply the same technique for bootable host systems - using
standard OCI/Docker containers as a transport and delivery format
for base operating system updates.

With bootable containers, you can build and customize the entire host OS with
the same tools as for application containers.  That means you can build on top
of base bootc images with Dockerfiles and tailor the OS to your needs.


It is crucial to understand that a container-build environment has certain
limitations that impact the commands we can run in a Containerfile.

The kernel, for instance, is shared with the one of the host environment.
Hence, changes to kernel tunables (sysctl) should be made in the form of
configuration files as explained in the xref:sysctl.adoc[system-configuration docs].

You may find solutions in our https://gitlab.com/fedora/bootc/examples[examples repository].


Some projects like firewalld include CLI configuration tools (e.g., `firewall-cmd`)
that default to assuming they are part of a running system. However, in a
container build environment the daemon is not running, and these commands may
fail. There is no one-size-fits-all solution to commands failing due to missing
D-Bus. In many cases, the commands are used to configure the system which can
also be done via configuration files.

Let's take a look at the following two examples:

* `firewall-cmd` depends on D-Bus and will fail when being executed in a
Containerfile.  However, we can use the `firewall-offline-cmd` which is built
for exactly such use-cases where the firewalld daemon is not running.

* `tuned` is a profile-based power-management daemon that is usually being
controled with the `tuned-adm` client that communicates over D-Bus.
In this case, we can revert to configuring `tuned` via its configuration
files in `/etc/tuned`.  For instance, the active profile can be written to
`/etc/tuned/active_profile` instead of running `tuned-adm profile <profile>`.


systemd is not running in a standard container-build environment.  Hence,
certain commands that expect a running system daemon may not work as expected
and error out.  Starting a systemd service via `systemctl start`, for instance,
will not work. Configuring a service to start at boot time via `systemctl
enable`, however, works.


Commands used to configure the system can usually be replaced by writing to the
individual configuration files.  However, if you run into a situation where a
command must be executed on an installed Linux system, using a
https://docs.fedoraproject.org/en-US/quick-docs/systemd-understanding-and-administering/#_creating_new_systemd_service[systemd service]
that fires at system runtime (e.g., during boot) may the right approach.

Let's assume that an exemplary `/usr/bin/important-cmd` does not run correctly
due limitations of the container-build environment.  We can move executing this
command into a systemd service that fires at boot time:

.important-cmd.service
[source,subs="attributes"]
----
[Unit]
Description=Run important-cmd at runtime instead of build time.
[Service]
Type=simple
ExecStart=/usr/bin/important-cmd
[Install]
WantedBy=multi-user.target
----

The systemd service can be copied to
`/usr/lib/systemd/system/important-cmd.service` in the Containerfile where a
`RUN systemctl enable important-cmd.service` would instruct it to be started at
boot once the network is available.




Bootc containers are shipped as ordinary OCI containers and are intended to be
usable as part of a container build process, but are primarily designed to run
on booted physical/virtual machines via bootc.  Hence, there is a number of
things to consider when building and running bootc OCI containers.

There is a number of systemd services that setup the filesystem, among other
things.  For instance, the root's home directory is not present but created by
a systemd service on boot/init.  That implies that a bootc container is not
always the best environment to, for instance, compile a project. We recommend
https://docs.docker.com/build/building/multi-stage/[multi-stage builds] for
that purpose and compile the source in a build stage from which build artifacts
can be copied into the final stage to create a derived image.


Do *not* attempt to invoke `dnf -y update` (or `upgrade`) in general. While
some things will work correctly, others will not (especially at the moment
kernel and bootloader updates). We will aim to fix much of this over
time, but still you should instead prefer only explicitly pulling in
updates (or reversions) that you need.

The secondary reason to avoid this: Often people choose image-based updates
for their predicability, and you can easily "pin" the base image by digest
for example. The default for dnf repositories is to "float" - so what
happens with an image build today could be different tomorrow. Packages can
be locked with extra effort.

If you want to make large scale changes to the base image, instead
look at using the xref:building-from-scratch.adoc[from scratch build].


We recommend running the `bootc container lint` command as a final stage during a
container build in Containerfile.  This command will perform a number of
checks inside the container image and throw an error in case of issues.

.Containerfile
[source,dockerfile,subs="attributes"]
----
FROM quay.io/fedora/fedora-bootc:42
# Customization steps
RUN bootc container lint
----


On recent images we introduced kernel-install integration which allows you to use DNF to manage kernel packages.

This allows you to have a container file that is as simple as:
.Containerfile
[source,dockerfile,subs="attributes"]
----
FROM quay.io/fedora/fedora-bootc:42
RUN &lt;&lt;EOF
set -xeuo pipefail
dnf -y downgrade kernel
dnf clean all
bootc container lint
EOF
----

An example for kernel-rt on centos is:

.Containerfile
[source,dockerfile,subs="attributes"]
----
FROM quay.io/centos-bootc/centos-bootc:stream10
RUN &lt;&lt;EOF
set -xeuo pipefail
dnf config-manager --set-enabled rt
dnf config-manager --set-enabled nfv
(echo 'remove kernel kernel-modules-core'; echo 'install kernel-rt-modules-extra kernel-rt-modules-kvm'; echo run) | dnf -y shell
dnf clean all
bootc container lint
EOF
----

If you are on an older image, you can use `rpm-ostree override replace` or
`rpm-ostree override remove pkg --install new-pkg` to manage your kernel.
An example of using rpm-ostree is available at our https://gitlab.com/fedora/bootc/examples[examples repository].


You may want to build a derived bootc image on a GitHub project via GitHub Actions.  Since bootc-based images can grow in size quickly, you are likely to run into disk-space issues on the Action runner.  Adding the following first step to the Action may solve the space issue:

[source,yaml]
----
# Based on https://github.com/orgs/community/discussions/25678
- name: Delete huge unnecessary tools folder
run: rm -rf /opt/hostedtoolcache
----

For an example project on GitHub using the Buildah and Podman Actions, please visit https://github.com/nzwulfin/cicd-bootc[github.com/nzwulfin/cicd-bootc].


While one can add
https://github.com/opencontainers/image-spec/blob/main/config.md[Container configuration metadata]
(e.g., environment, exposed ports, default users) to an OCI container image,
bootc generally ignores that. In practice, that means that certain things may
work when being run as an ordinary OCI container via Podman but won't work once
booted. For instance, you may use the `ENV foo=bar` instruction in a Container
file which will be visible in a Podman container but it won't be propagated to
the booted system.

For details and recommendations, please refer to the
{bootc-upstream}/building/bootc-runtime.html[bootc-runtime documentation].


One scenario that may occur is when one wants to perform e.g. *just*
a kernel patch to a running system; or other targeted fixes, without
rolling in other changes. One recommendation for this type of
scenario is (assuming you have orchestration/management software) to
have an inventory of which container image (identified by `@sha256`)
is in use for each machine you want to target for specific updates.
Then create a container build which has just the changes you want,
and have the targeted nodes use that container image via e.g.
`bootc switch`.


At the current time, the role of `bootc` is solely to boot and
upgrade from a single container image. This is a very simplistic
model, but it is one that captures many use cases.

In particular, the default assumption is that *code* and *configuration*
for the base OS are tightly bound. Systems which update
one or the other asynchronously often lead to problems with
skew.


A webserver is the classic case of something that can be run
as a container on a generic host alongside other workloads.
However, many systems today still follow a "1:1" model between application
and a virtual machine. Migrating to a container build for
this can be an important stepping stone into eventually
lifting the workload into an application container itself.

Additionally in practice, even some containerized workloads
have such strong bindings/requirememnts for the host system
that they effectively require a 1:1 binding. Production
databases often fall into this class.


Nevertheless, here's a classic static http webserver example;
an illustrative aspect is that we move content from `/var` into `/usr`.
It expects an `index.html` colocated with the Containerfile.

.Containerfile
[source,dockerfile,subs="attributes"]
----
FROM {container-fedora-full}
# The default package drops content in /var/www, and on bootc systems
# we have /var as a machine-local mount by default. Because this content
# should be read-only (at runtime) and versioned with the container image,
# we move it to /usr/share/www instead.
RUN dnf -y install httpd && \
systemctl enable httpd && \
mv /var/www /usr/share/www && \
echo 'd /var/log/httpd 0700 - - -' > /usr/lib/tmpfiles.d/httpd-log.conf && \
sed -ie 's,/var/www,/usr/share/www,' /etc/httpd/conf/httpd.conf
# Further, we also disable the default index.html which includes the operating
# system information (bad idea from a fingerprinting perspective), and crucially
# we inject our own content as part of the container image build.
# This is a key point: In this model, the webserver content is lifecycled exactly
# with the container image build, and you can change it "day 2" by updating
# the image. The content is underneath the /usr readonly bind mount - it
# should not be mutated per machine.
RUN rm /usr/share/httpd/noindex -rf
COPY index.html /usr/share/www/html
EXPOSE 80
----


In contrast, this example demonstrates a webserver
as a "referenced" container image via
{podman-docs}/podman-systemd.unit.5.html[podman-systemd]
that is also configured for automatic updates.

[subs="attributes"]
--
This reference example is maintained in {git-examples}/app-podman-systemd[app-podman-systemd].
--

.caddy.container
[source]
----
[Unit]
Description=Run a demo webserver

[Container]
# This image happens to be multiarch and somewhat maintained
Image=docker.io/library/caddy
PublishPort=80:80
AutoUpdate=registry

[Install]
WantedBy=default.target
----

.Containerfile
[source,dockerfile,subs="attributes"]
----
# In this example, a simple "podman-systemd" unit which runs
# an application container via {podman-docs}/podman-systemd.unit.5.html
# that is also configured for automatic updates via
# {podman-docs}/podman-auto-update.1.html
FROM {container-c9s}
COPY caddy.container /usr/share/containers/systemd
# Enable the simple "automatic update containers" timer, in the same way
# that there is a simplistic bootc upgrade timer. However, you can
# obviously also customize this as you like; for example, using
# other tooling like Watchtower or explicit out-of-band control over container
# updates via e.g. Ansible or other custom logic.
RUN systemctl enable podman-auto-update.timer
----


The container images above are just illustrative demonstrations that
are not useful standalone. It is highly likely that you will want to
run other container images, and perform other customizations.

Among the most likely additions is configuring a mechanism for remote
SSH; see xref:authentication.adoc[Authentication, Users, and Groups].


Often packaging scripts may invoke `useradd`. This can cause "state drift"
in the case where `/etc/passwd` is also locally modified on the system,
and transient `/etc` is not in use.

More on this in {bootc-upstream}/building/users-and-groups.html#adding-users-and-credentials-statically-in-the-container-build[bootc upstream].

If the user does not *own* any content shipped in `/usr` and it runs
as a systemd unit, then it's often a good candidate to convert to
systemd `DynamicUser=yes`, which has numerous advantages in general.
Using `DynamicUser` will also help take care of ownership of e.g.
`/var/lib/somedaemon` (`StateDirectory` and more).

However, porting to `DynamicUser=yes` can be somewhat involved
in complex cases. If the RPM does contain files owned by the allocated
user, but that content is just in e.g. `/var/lib/somedaemon`
or `/var/log/somedaemon`, then often the best fix is to drop that
content from the RPM (you can `%ghost` it to mark it as owned)
and switch to creating it at runtime via https://www.freedesktop.org/software/systemd/man/latest/tmpfiles.d.html[systemd-tmpfiles].

You can then also switch to creating the user via https://www.freedesktop.org/software/systemd/man/latest/systemd-sysusers.html[systemd-sysusers].

And at that point, you can also drop the `%post` from the RPM which
allocates the user.


This occurs in the case of things like setuid/setgid binaries.
The first solution: Avoid setuid/setgid binaries entirely!
Usually, there's a better approach to the problem domain.

Another case is where a daemon wants to drop privileges
but wants to access its configuration state in `/etc`.
For example, polkit does this in `/etc/polkit-1/rules.d`.
One solution here is to use e.g. `BindReadOnlyPaths=`
to mount the source directory into the namespace
of the daemon.

If you are in this situation, then there is no
solution other than statically allocating the user, which
requires global coordination. You can request it e.g.
via https://docs.fedoraproject.org/en-US/packaging-guidelines/UsersAndGroups/#_soft_static_allocation[Fedora].
But this should be avoided to the greatest extent
possible.


See the {bootc-upstream}/building/guidance.html[bootc upstream guidance].

Many configuration changes to a Linux system boil down effectively to
writing configuration files into `/etc` or `/usr` - those operations
translate seamlessly into booted hosts via a `COPY` instruction
or similar in a container build.


Custom bootc image uses the 'artifact pattern' to reference
other container images as reusable components. This approach
enables you to manage and reuse software pieces independently
across multiple container images, rather than duplicating configuration
in each image.

Copy any configuration from referenced images before applying
additional customizations. This step should be performed early
in the build process to ensure consistency and avoid conflicts.


[source,dockerfile,subs="attributes"]
----
FROM {container-c10s} as builder
RUN /usr/libexec/bootc-base-imagectl build-rootfs --manifest=standard /target-rootfs

# This container image uses the "artifact pattern"; it has some
# basic configuration we expect to apply to multiple container images.
# Define baseconfig for the image, leveraging the "artifact pattern" to reference a reusable container artifact.
FROM quay.io/exampleos/baseconfig@sha256:.... as baseconfig

FROM scratch
COPY --from=builder /target-rootfs/ /

# Copy configuration artifacts from the baseconfig container image to our custom image.
COPY --from=baseconfig /usr/ /usr/

# ... insert here arbitrary build steps, as above
LABEL containers.bootc 1
LABEL ostree.bootable 1
RUN bootc container lint
----


See {git-examples}[Examples] for many examples of
container image definitions!

Commonly when building a bootc system, you will want to manage and maintain
your own version numbers and branding for the resulting system.

The default base images use a `*-release` package (e.g. `fedora-release`, `centos-release`, `redhat-release`, etc.)
that contains key files such as `/usr/lib/os-release` (symlinked from `/etc/os-release`).
For more on the os-release file, see https://www.freedesktop.org/software/systemd/man/latest/os-release.html[this reference].


A useful starting point is updating the https://www.freedesktop.org/software/systemd/man/latest/os-release.html#VARIANT=[`VARIANT` fields].

For example:

.Containerfile
[source]
----
RUN echo VARIANT="My Custom bootc OS" && echo VARIANT_ID=com.gitlab.customos.foo >> /usr/lib/os-release
----


The base images include an https://github.com/opencontainers/image-spec/blob/main/annotations.md#pre-defined-annotation-keys[OCI standard] label `org.opencontainers.image.version`.
As part of your build process, you may want to override it.
When doing so, it is also recommended to update the `OSTREE_VERSION` in the `os-release` file,
but this is not required. In the future, it is likely that the base images will use
the newer https://www.freedesktop.org/software/systemd/man/latest/os-release.html#IMAGE_VERSION=[IMAGE_VERSION]
key; using this now is recommended.


You can also entirely replace the os-release package, e.g.:

[source]
----
RUN dnf -y swap fedora-release generic-release --allowerasing && dnf clean all
----

The `generic-release` package is included in Fedora to denote the system is not Fedora, but is a derivative.
You can fork the `generic-release` package and provide your own custom one here.


Creating "from scratch" base bootc images provides control the base image content, allowing you to own the OS stream and tailor the environment to suit specific needs and workflows.

While a core premise of the bootc model is that rich control over Linux system customization
can be accomplished with a "default" container built with the provided base image that works
on a wide range of environments and systems without any additional overhead associated with maintaining a from scratch base image.
Also, as part of the default derivation, it is possible to xref:os-release-and-versions.adoc[swap the kernel and other fundamental components].

[source]
----
FROM <base image>
RUN ...
----

However, some use cases want even more control. Specifically:

- An organization building a bootc system may need to ensure the base image version carries a set of packages at exactly specific versions.
- An organization may want to start from a minimal base image, adding only the necessary packages.



As an organization deploying a bootc system, I may want to ensure
the base image version carries a set of packages at
exactly specific versions (perhaps defined by a lockfile,
or an rpm-md repository). There are many tools which
manage snapshots of yum (rpm-md) repositories.

There are currently issues where it won't quite work to e.g.
`dnf -y upgrade selinux-policy-targeted`.


Today there is just one default image produced by the project,
which is now known as "standard". It is roughly a headless
server-oriented installation (although it can be used for desktops
as well), and comes with a lot of opinionated packages for
networking, CLI tooling, etc.

This project also has a "minimal" content set definition which
is not currently shipped as a distinct container, but can
be *built* from the standard image.


Like layered bootc images that use the default base image, from scratch base
images are still derived from the base container. They will not automatically
consume changes to the default base image unless they are part of a container pipeline.

For more background on the bootc update model, see xref:getting-started.adoc#_updating_bootable_containers[updating bootc images] here and an xref:fedora-coreos.adoc[overview of derived images] here.

If you come from the "package" world, you may already use tools like Pulp/Satellte/Artifactory
to set up "snapshots/versions" of packages in your base image and help bootc
more seamlessly fit into your existing workflow.

If you come from the CoreOS world, this is a key difference when interacting
with the base OS image.  Read about the https://docs.fedoraproject.org/en-US/fedora-coreos/faq/#_how_does_fedora_coreos_relate_to_fedora_bootc[differences between
Fedora CoreOS and Fedora bootc here].


Most, but not all content from the base image comes from RPMs.
There is some additional non-RPM content, as well as postprocessing
that operates on the filesystem root. At the current time the
implementation of the base image build uses `rpm-ostree`,
but this is considered an implementation detail subject to change.


The core operation is `bootc-base-imagectl build-rootfs`.

This command takes just one required argument:

- A path to the target root filesystem which will be generated as
a directory. The target should not already exist (but its parent must exist).

Additionally, `--manifest` can be provided to choose the input set of packages
and configuration. The two default images are:

- `standard`: The default image.
- `minimal`: A quite small root content set, effectively `bootc`, `systemd`, `kernel`, `dnf`
plus their hard dependencies, as well as a small set of non-RPM content tweaks to
e.g. enable systemd persistent journaling.

For more information on the available content sets, run `bootc-base-imagectl list`.

The set of packages is currently not officially supported to configure directly.
The general intention is that you can start from either image, and then add, change
or remove content from it in a secondary build phase. Especially building up from
the `minimal` image should cover many use cases that want to control the content
set to a high degree.

A key goal is that this avoids "forking" by default, ensuring that by default
when we change the base image (adding new packages usually) or provide
updates or fixes for the build operation, from scratch builds will
inherit those changes by default.

A "source root" can also be provided to enable "cross builds". More on this below.


`bootc-base-imagectl list` will enumerate available configurations that
can be selected by passing `--manifest` to `build-rootfs`.


The current implementation uses `rpm-ostree` on a manifest (treefile)
embedded in the container image itself.  To emphasize: the implementation of
this command (especially the configuration files that it reads) are subject to change
in the future.


We're generating a new root filesystem, not modifying the existing container.
This is most easily done by using container features (mount namespacing
in particular) that is not enabled by default in many container build
environments today.

In summary, you must provide at least these arguments to e.g. `podman build`:

[source]
----
--cap-add=all --security-opt=label=type:container_runtime_t --device /dev/fuse
----

It is a goal to reduce these required capabilities; see https://gitlab.com/fedora/bootc/base-images/-/issues/43[this tracker issue].

Note however ultimately the goal here is to deploy this container image as a
base operating system on a physical or virtualized environment, where
inherently more trust is required regardless.


[source,dockerfile,subs="attributes"]
----
# Begin with a standard bootc base image that serves as a "builder" for our from scratch image.
FROM {container-c10s} as builder
# Configure and override source RPM repositories, if necessary. This step is required when referencing specific content views or target mirrored/snapshotted/pinned versions of content.
RUN rm -rf /etc/yum.repos.d/*
COPY mypinnedcontent.repo /etc/yum.repos.d/
# Build the root file system using the specified repositories and non-RPM content from the "builder" base image.
# If no repositories are defined, the default build will be used. You can modify the scope of packages in the base image by changing the manifest between the "standard" and "minimal" sets.
RUN /usr/libexec/bootc-base-imagectl build-rootfs --manifest=standard /target-rootfs

# Create a new, empty image from scratch.
FROM scratch
# Copy the root file system built in the previous step into this image.
COPY --from=builder /target-rootfs/ /

# Apply customizations to the image. This syntax uses "heredocs" https://www.docker.com/blog/introduction-to-heredocs-in-dockerfiles/ to pass multi-line arguments in a more readable format.
RUN &lt;&lt;EORUN
# Set pipefail to display failures within the heredoc and avoid false-positive successful builds.
set -xeuo pipefail
# Install necessary packages, run scripts, etc.
dnf -y install emacs
# Remove leftover build artifacts from installing packages in the final built image.
dnf clean all
rm /var/{log,cache,lib}/* -rf
# Run the bootc linter to avoid encountering certain bugs and maintain content quality. Place this as the final command in your last run invoaction.
bootc container lint
EORUN

# Define required labels for this bootc image to be recognized as such.
LABEL containers.bootc 1
LABEL ostree.bootable 1
# https://pagure.io/fedora-kiwi-descriptions/pull-request/52
ENV container=oci
# Optional labels that only apply when running this image as a container. These keep the default entry point running under systemd.
STOPSIGNAL SIGRTMIN+3
CMD ["/sbin/init"]
----


[source,dockerfile,subs="attributes"]
----
# Begin with a standard bootc base image that is reused as a "builder" for the custom image.
FROM {container-c10s} as builder
# Configure and override source RPM repositories, if necessary. This step is not required when building up from minimal unless referencing specific content views or target mirrored/snapshotted/pinned versions of content.
# Add additional repositories to apply customizations to the image. However, referencing a custom manifest in this step is not currently supported without forking the code.
# Build the root file system using the specified repositories and non-RPM content from the "builder" base image.
# If no repositories are defined, the default build will be used. You can modify the scope of packages in the base image by changing the manifest between the "standard" and "minimal" sets.
RUN /usr/libexec/bootc-base-imagectl build-rootfs --manifest=minimal /target-rootfs

# Create a new, empty image from scratch.
FROM scratch
# Copy the root file system built in the previous step into this image.
COPY --from=builder /target-rootfs/ /

# Apply customizations to the image. This syntax uses "heredocs" https://www.docker.com/blog/introduction-to-heredocs-in-dockerfiles/ to pass multi-line arguments in a more readable format.
RUN &lt;&lt;EORUN
# Set pipefail to display failures within the heredoc and avoid false-positive successful builds.
set -xeuo pipefail
# Install required packages for our custom bootc image.
# Note that using a minimal manifest means we need to add critical components specific to our use case and environment.
# For example, install networking and SSH - not every use case will want SSH, so it's not included in minimal
dnf -y install NetworkManager openssh-server
# Remove leftover build artifacts from installing packages from the final built image.
dnf clean all
rm /var/{log,cache,lib}/* -rf
# Run the bootc linter to avoid encountering certain bugs and maintain content quality. Place this as the final command in your last run invoaction.
bootc container lint
EORUN

# Define required labels for this bootc image to be recognized as such.
LABEL containers.bootc 1
LABEL ostree.bootable 1
# https://pagure.io/fedora-kiwi-descriptions/pull-request/52
ENV container=oci
# Optional labels that only apply when running this image as a container. These keep the default entry point running under systemd.
STOPSIGNAL SIGRTMIN+3
CMD ["/sbin/init"]
----


IMPORTANT: This `rechunk` subcommand is not supported in rootless podman.

The output of the above will be an image with a single large layer (tarball),
which means every change will result in copying (pushing to the registry, pulling
for clients) the single large tarball.

There is
https://coreos.github.io/rpm-ostree/build-chunked-oci/[support in
rpm-ostree] to take a large image like this and "rechunk" it.  This
process will
https://coreos.github.io/rpm-ostree/container/#creating-chunked-images[optimize]
the ordering and grouping of installed packages into many layers in
the final image, which provides better network efficiency since a
number of layers may be reused without incurring a transfer.

The `bootc-base-imagectl` tool provides a `rechunk` subcommand to
interact with `rpm-ostree` and perform this rechunking.  It is
recommended to use the `bootc-base-imagectl rechunk` command and not
`rpm-ostree` directly, since the implementation details may change in
the future.


Currently, `bootc-base-imagectl` is shipped as part of the
centos-stream bootc images.  It is convenient to use these
pre-existing images in order to rechunk "from scratch" base images.  This can
be done by mapping the host containers-storage into the container and
running `bootc-base-imagectl` within to do the rechunking operation.

A previously-built base image named
`quay.io/exampleos/fedora-bootc:single` can be optimized with the
following command (as root):

[source]
----
# podman run --rm --privileged -v /var/lib/containers:/var/lib/containers \
quay.io/centos-bootc/centos-bootc:stream10 \
/usr/libexec/bootc-base-imagectl rechunk \
quay.io/exampleos/fedora-bootc:single \
quay.io/exampleos/fedora-bootc:chunked
----


The rechunker automatically groups files into layers based on RPM
package ownership. However, you can override this behavior using the
`user.component` extended attribute to assign specific files or
directories to custom-named layers.

This is useful when you have custom content (not from RPMs) that you
want to group together for better layer caching and sharing across
builds.

[source,dockerfile]
----
# Assign a single file to a custom layer
RUN setfattr -n user.component -v "my-apps" /usr/bin/my-custom-app

# Assign an entire directory tree to a layer
RUN setfattr -n user.component -v "my-lib" /usr/share/my-lib
----

When a directory has the `user.component` attribute set, all files and
subdirectories within it are automatically included in that layer.
You can override specific files or subdirectories by setting a
different `user.component` value on them:

[source,dockerfile]
----
RUN <<EORUN
set -euxo pipefail
mkdir -p /usr/share/my-lib/docs
# ... populate the directory ...

# All content under my-lib goes to "my-lib" layer
setfattr -n user.component -v "my-lib" /usr/share/my-lib
# Override: docs go to a separate "docs" layer
setfattr -n user.component -v "docs" /usr/share/my-lib/docs
EORUN
----

NOTE: Layers created via `user.component` are given priority over the
automatic package-based layers during rechunking.

There are a number of ways to embed containerized workloads into a bootc image. Let's go through all of them and elaborate on the various use cases that fit the different ways of embedding containers followed by examples that may serve as a template.

Embedding container workloads into a bootc image implies declaring the containers in some shape or form. Most mechanisms we present below build upon declaring such workloads in the form of so-called Quadlets. So let's first explore {podman-docs}/podman-systemd.unit.5.html[Quadlets].


Running containerized workloads in systemd is a simple yet powerful means for reliable and rock-solid deployments. Podman has an excellent integration with systemd in the form of Quadlet. Quadlet is a tool for running Podman containers in systemd in an optimal and declarative way. Workloads can be declared in the form of systemd-unit-like files extended with Podman-specific functionality.

You might be wondering about the benefits of running containerized workloads in systemd. First, systemd is the central control instance on modern Linux systems. Among other things, it manages system and user services and the dependencies among them. It has tons of capabilities such as elaborate restart policies. Hence, Podman’s integration with systemd was an important milestone to integrate traditional Linux sysadmin craftmanship with modern container technologies.

Second, Podman’s daemonless architecture integrates perfectly with systemd. The sophisticated process management of systemd allows it to monitor a container and restart it if needed. The combination of systemd and Podman allowed us to tackle new use cases where human intervention is not always possible, for instance edge computing or IoT. On top, Quadlet is a seamless extension for systemd, which makes it very approachable for sysadmins.

So let's take a closer look at Quadlet, and take the following example Quadlet `.container` file:

[source,text]
----
[Unit]
Description=A minimal container

[Container]
Image=registry.fedoraproject.org/fedora
# For demo purposes, the container just sleeps
Exec=sleep 60

[Service]
# Restart service when sleep finishes
Restart=always

[Install]
# Start by default on boot
WantedBy=multi-user.target default.target
----

As mentioned, Quadlet extends systemd-units with Podman-specific features. Quadlet `.container` files, for instance, add a `[Container]` table where we can declare container-specific options such as the image, command, and name of the container, but also which volumes and networks it should use. Quadlet is a systemd-generator that is being executed on boot or when reloading the systemd daemon. If you want to test the upper example, you can create the file in your home directory (`$HOME/.config/containers/systemd/test.container`) and run `systemctl --user daemon-reload`. Reloading the daemon will fire Quadlet and generate a systemd service named `test.service` that you can then start with `systemctl --user start test.service`.

You can think of Quadlet like Docker Compose, but for running containers in systemd. The declarative nature of Quadlet makes it a perfect candidate for installing and embedding workloads on a bootc system. Quadlet also supports running Pods and Kubernetes-compliant YAML definitions, can manage volumes, networks and images and further supports building images. For detailed documentation on Quadlet and more examples, please refer to the {podman-docs}/podman-systemd.unit.5.html[upstream documentation].

Quadlets are managed as files, which allows for a smooth and easy integration into the bootc workflow. The center of gravity for working with bootc is the Containerfile. Hence, Quadlets can live next to a Containerfile in the same Git tree and be copied onto the image during the container build to make the workloads available at runtime.

Let's use the `test.container` example mentioned above and integrate it into a `fedora-bootc` based bootc image:

.Containerfile
[source,dockerfile,subs="attributes"]
----
FROM {container-fedora-full}
RUN  mkdir -p /etc/containers/systemd
COPY test.container /etc/containers/systemd
----

The Quadlet service will be started on boot and there is nothing else to be done. This allows for a great hands-off experience when managing Linux hosts as the entire workload can be declared at once. Traditional config management and provisioning can shift left into the build process.


Running containers via Quadlets requires container images to be present on the bootc system. The various options presented below can be used in combination, depending on the use-case requirements and user needs.


Container images can be pulled on-demand whenever a Quadlet is started and the referenced image is not yet present in the local containers storage.

The big advantage of this model is the ease-of-use as we just need to install and run a Quadlet and let Podman deal with the pulling. Podman containers started by Quadlet and those created by the user or other tools can all just share the same store.

A major disadvantage of on-demand pulls is that it delays starting the workloads until the individual images have been pulled down. This may even impact boot time when Quadlets are started at boot. Moreover, disconnected environments cannot make use of on-demand pulls at all due to the lack of network connectivity.


Logically-bound images are an improvement over on-demand pulls as they allow images to be pre-pulled at install time and on updates. This way, logically-bound images must not be pulled when a Quadlet starts as those images are already present on the system.

Logically-bound images can be specified in the `/usr/lib/bootc/bound-images.d` directory in the form of symlinks. `bootc` will automatically pull the images on `bootc install`, `bootc upgrade` and `bootc switch`.  The symlinks in the directory point to Quadlet files on the system.  Currently, those Quadlets must either be `.container` or `.image` files.

An example Containerfile using logically-bound images may look as follows:

.Containerfile
[source,dockerfile,subs="attributes"]
----
FROM quay.io/myorg/myimage:latest

COPY ./my-app.container /usr/share/containers/systemd/another-app.container

RUN ln -s /usr/share/containers/systemd/my-app.container /usr/lib/bootc/bound-images.d/my-app.container
----

To access logically-bound images, `.container` Quadlets need to add the following like to the `[Container]` table:

`GlobalArgs=--storage-opt=additionalimagestore=/usr/lib/bootc/storage`

This setting allows Podman to access the so-called https://www.redhat.com/en/blog/image-stores-podman[additional image store] of bootc.  Please note that the presented solution of using `GlobalArgs` is preferable over a system-wide configuration in `storage.conf` -- unless all containers run in Quadlets.  For more details on logically-bound images, please refer to the {bootc-upstream}[upstream documentation of bootc] and the https://gitlab.com/fedora/bootc/examples/-/tree/main/logically-bound-images[Fedora examples].


Some use cases require the entire boot image to be fully self contained. That means that everything needed to execute the workloads is shipped with the bootc image, including container images of the application containers and Quadlets. Such images are also referred to as “physically-bound images”.

The underlying mechanism is very similar to the one of logically-bound images in that physically-bound images can be pre-pulled during image build time and made available at runtime. Let’s first dive into how we can achieve such physical embedding and explain later on why we recommend doing it as follows.

The instruction in a Containerfile to physically embed an image at build time may look like that:

`RUN skopeo copy --preserve-digests docker://<IMAGE> dir:/usr/lib/containers-image-cache/<DIRECTORY>`

At runtime, the image can be moved into Podman’s mutable store as follows:

`RUN skopeo copy --preserve-digests dir:/usr/lib/containers-image-cache/<DIRECTORY> containers-storage:<IMAGE>`

Since the embedded images may change with each system update, we cannot use an additional image store for this purpose as it was not designed to be swapped out. Instead we make use of the “dir” transport of the container tools which allows for storing one or more images in the same directory. The “dir” transport further allows to preserve the digest of the image which is crucial for the image to be referenced by its original digest. However, there is one caveat: we need to keep track of the name of the image.

To improve the experience of physically embedding container images, we propose two scripts that you can find in the https://gitlab.com/fedora/bootc/examples/-/tree/main/physically-bound-images[upstream Fedora bootc examples].

An example to physically embed a number of images in a Containerfile may look as follows:

.Containerfile
[source,dockerfile,subs="attributes"]
----
COPY ./embed_image.sh /usr/bin/
COPY ./copy_embedded_images.sh /usr/bin/

RUN &lt;&lt;PULL
/usr/bin/embed_image.sh registry.fedoraproject.org/fedora:latest
/usr/bin/embed_image.sh docker.io/library/busybox:latest
/usr/bin/embed_image.sh docker.io/library/alpine@sha256:ca1c944a4f8486a153024d9965aafbe24f5723c1d5c02f4964c045a16d19dc54 --all
PULL
----

To copy the images into Podman's mutable store at runtime, just run `/usr/bin/copy_embedded_images.sh`. Note that the images must be copied over before any container or service (e.g., Quadlet) depending on such an image is started. It could be moved into a systemd unit that starts before any Quadlet, for instance. For more information, please see the upstream https://gitlab.com/fedora/bootc/examples/-/tree/main/physically-bound-images[upstream Fedora bootc examples]. In the meantime, we are working on improving the user experience when using physically-embedded images.


An important aspect of embedding images is the way they are referenced. Images can be referenced by tag, by digest, or a mix of both. While digests always point to exactly one image, a tag may be updated on a registry. Choosing the right way of referencing an image can impact the quality and robustness of workloads, so we should be intentional about it. If you are interested in this topic, please see an https://developers.redhat.com/articles/2025/01/28/how-name-version-and-reference-container-images[article on how to name, version, and reference container images].
* Provisioning machines

This guide provides instructions to install {projname} to bare metal.
Three main options are available:

* Installing from a stock Anaconda ISO/PXE over the network
* Installing from a bootc-image-builder generated ISO
* Installing from the container directly with `bootc install`


Before installing {projname}, it's recommended that you have created a customized derived container image;
but this is not a hard requirement, as it is possible to enable basic system access via
e.g. injecting SSH keys with kickstart or with `bootc install` and the `-root-ssh-authorized-keys`
argument.


{projname} can be installed using https://anaconda-installer.readthedocs.io/en/latest/[Anaconda].


The `ostreecontainer` kickstart verb can be used to provision your custom container image.

<https://pykickstart.readthedocs.io/en/latest/kickstart-docs.html#ostreecontainer>

This is a full basic Kickstart example:

.basic.ks
[source]
----
# Basic setup
text
network --bootproto=dhcp --device=link --activate
# Basic partitioning
clearpart --all --initlabel --disklabel=gpt
reqpart --add-boot
part / --grow --fstype xfs

# Here's where we reference the container image to install - notice the kickstart
# has no `%packages` section!  What's being installed here is a container image.
ostreecontainer --url quay.io/centos-bootc/centos-bootc:stream9

firewall --disabled
services --enabled=sshd

# Only inject a SSH key for root
rootpw --iscrypted locked
sshkey --username root "<your key here>"
reboot
----


See the {bootc-upstream}/registries-and-offline.html[bootc documentation] on
registries, as well as the xref:container-pull-secrets.adoc[Container pull secrets] section.

The default Anaconda installation ISOs may also need a duplicate copy of some "bootstrap"
configuration in order to access the targeted registry when fetching over the network.

In general you can use the Anaconda https://pykickstart.readthedocs.io/en/latest/kickstart-docs.html#chapter-4-pre-installation-script[`%pre` command]
to perform arbitrary changes to the installation environment before the target
bootc container image is fetched.

.Configuring a pull secret
[source]
----
%pre
mkdir -p /etc/ostree
cat > /etc/ostree/auth.json << 'EOF'
{
"auths": {
"quay.io": {
"auth": "<your secret here>"
}
}
}
EOF
%end
----

.Disable TLS for an insecure registry
[source]
----
%pre
mkdir -p /etc/containers/registries.conf.d/
cat > /etc/containers/registries.conf.d/local-registry.conf << 'EOF'
[[registry]]
location="[IP_Address]:5000"
insecure=true
EOF
%end
----

Alternatively, the `%pre` can fetch data from the network using
binaries included in the installation environment, such as `curl`.

Similarly, one can use `%pre` to inject trusted certificate
authorities into the installation environment's `/etc/pki/ca-trust/source/anchors`
and via running `update-ca-trust`.

Finally, insecure registries can be configured in a similar way by
modifying the `/etc/containers` directory as documented above.


See the https://github.com/osbuild/bootc-image-builder?tab=readme-ov-file#-image-types[bootc-image-builder documentation];
the key is usage of the `anaconda-iso` type.

Conceptually, this generates a system close to the "stock" ISOs available
from Fedora/CentOS, except your container image content is embedded in
the ISO. This means that there is no need to access the network
during installation.

For example, you can copy the ISO to a USB stick, and take it into
an air-gapped/disconnected environment and perform a bare metal installation.


A key goal of the `bootc` project is having the container image be
the "source of truth" as much as possible. A "basic" installer
is built into the `bootc` project and is available as `bootc install to-disk`
or `bootc install to-filesystem`.

TIP: More information is available {bootc-upstream}/bootc-install.html[at the upstream bootc site].

In the very simplest example, assuming you have a running Linux environment
with `podman`, you can perform a bare metal installation to a block device.
Commonly, that existing Linux environment will be a "Live ISO" of some form.
At the current time, in Fedora the most suitable default "Live ISO" is
the https://docs.fedoraproject.org/en-US/fedora-coreos/live-booting/[Fedora CoreOS Live ISO].
You can inject an Ignition configuration into the Live ISO which
runs the following invocation via e.g. a systemd unit:

[source]
----
$ podman run \
--rm --privileged \
--pid=host \
-v /dev:/dev \
-v /var/lib/containers:/var/lib/containers \
--security-opt label=type:unconfined_t \
<image> \
bootc install to-disk /path/to/disk
----


In this model, you can configure a target block device and root filesystem
using whatever tools you want (e.g. LVM) and then run the same command above,
except with `to-filesystem`:

[source]
----
$ podman run \
--rm --privileged \
--pid=host \
-v /:/target \
-v /dev:/dev \
-v /var/lib/containers:/var/lib/containers \
--security-opt label=type:unconfined_t \
<image> \
bootc install to-filesystem /path/to/mounted/fs
----


The https://github.com/containers/podman-bootc-cli[podman-bootc-cli] tool streamlines
a local virtualization experience.

NOTE: This is not yet shipped in Fedora derivatives but will be soon. At
the moment for Fedora or derivatives the recommendation is to build from
source.  See the upstream web site for installation instructions.

This command will "self-install" the container into a disk image behind
the scenes, and launch an interactive SSH session.

```
$ podman-bootc run quay.io/centos-bootc/centos-bootc:stream9
```

IMPORTANT: There is no default root filesystem type configured for the Fedora
base images; you can select one in this way:

[source,subs="attributes"]
----
$ podman-bootc run --filesystem xfs {container-fedora-full}
----

For more information on configuring storage, see xref:storage.adoc[Storage].

Other useful commands include:

- `podman-bootc list`
- `podman-bootc ssh`
- `podman-bootc rm`

TIP: The `podman-bootc` command can also directly run your custom derived container images!

```
$ podman-bootc run quay.io/exampleuser/mycustom-image:latest
```

See xref:building-containers.adoc[Building derived container images].



The `podman-bootc-cli` tool streamlines logging into the generated
disk images by injecting a default SSH key for the `root` user
via https://systemd.io/CREDENTIALS/[systemd credentials] passed
to the hypervisor, and further automatically invoking `ssh` to
get an interactive shell.

This maximizes convenience for testing locally, while supporting
a container image that is intended to be deployed via e.g.
Anaconda and e.g. a kickstart-provided SSH key to bare metal.

Or, you may be creating a container image that is intended
to run via a fully https://about.gitlab.com/topics/gitops/["git-ops"]
fashion where it is not allowed to SSH in at all for production
scenarios. This can be done by simply not including any
SSH keys in your container image or disk image generation phases,
but the `podman-bootc` CLI helps inject a key for local
development/testing scenarios.


First, use the https://github.com/osbuild/bootc-image-builder[bootc-image-builder]
project to make a `.raw` or `.qcow2` disk image from your container
image.

The default base images do not have any default credentials to log in. If you
have not configured any, and you are directly passing the default base image into
the below command, you will not be able to log into the virtual machine.
See xref:authentication.adoc[Authentication] as well as xref:building-containers.adoc[Building containers]
for more information.


NOTE: Particularly when using container images that require a pull secret,
you will need to ensure the image is pulled into your container storage
before invoking the below command via e.g. `podman pull $CONTAINER_IMAGE`.
The issue of authentication and image pulling is being discussed in
https://github.com/osbuild/bootc-image-builder/pull/418[this PR].

.Run bootc-image-builder to turn a container image into a disk image
[source,subs="attributes"]
----
# Pull the bootable container image
sudo podman pull $CONTAINER_IMAGE
# Create an output directory to write the qcow2 file to
mkdir -p output
# Create an empty config.toml for bootc-image-builder
sudo podman run \
--rm \
-it \
--privileged \
--pull=newer \
--security-opt label=type:unconfined_t \
-v $(pwd)/output:/output \
-v /var/lib/containers/storage:/var/lib/containers/storage \
{bib-image} \
--local \
--type qcow2 \
$CONTAINER_IMAGE
----

[IMPORTANT]
====
There is no default root filesystem type configured for the Fedora
base images; you can specify one via the `--rootfs` option, for example:

- `--rootfs xfs`
- `--rootfs btrfs`

etc.

For supported filesystem types in the base images, see xref:storage.adoc[Storage].
====

(Note that there are more options, such as including a `config.json` which
can inject users and SSH keys)


The https://libvirt.org/[libvirt] project is supported on
several platforms but is very common on Linux environments.

Here is a very simplified example `virt-install` invocation:

[source]
----
sudo virt-install \
--name fedora-bootc \
--cpu host \
--vcpus 4 \
--memory 4096 \
--import --disk ./output/qcow2/disk.qcow2,format=qcow2 \
--os-variant fedora-eln
----

More information about `virt-install` is available in its man page,
as well as other places such as https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_virtualization/assembly_creating-virtual-machines_configuring-and-managing-virtualization#creating-virtual-machines-using-the-command-line-interface_assembly_creating-virtual-machines[Red Hat Enterprise Linux documentation].


Using "raw" qemu in some cases gives more control than higher level tools
such as `libvirt`, but introduces architecture and platform specifics.


```
qemu-system-x86_64 \
-M accel=kvm \
-cpu host \
-smp 2 \
-m 4096 \
-bios /usr/share/OVMF/OVMF_CODE.fd \
-serial stdio \
-snapshot output/qcow2/disk.qcow2
```


```
qemu-system-aarch64 \
-M accel=hvf \
-cpu host \
-smp 2 \
-m 4096 \
-bios /opt/homebrew/Cellar/qemu/8.1.3_2/share/qemu/edk2-aarch64-code.fd \
-serial stdio \
-machine virt \
-snapshot output/qcow2/disk.qcow2
```

This guide shows how to provision new {projname} instances on the Amazon Web Services (AWS) cloud platform.


It is possible to directly run the base images, without creating a customized one. This can be
useful for experimentation. However, an important use case here for {projname}
is to create custom derived container images, and then instantiate them.

In this example, you must have access to an AWS account.
The examples below use the https://aws.amazon.com/cli/[aws] command-line tool, which must be separately installed and configured beforehand.




At the current time, the {projname} project does not produce pre-built AMIs
for the base images. There are two primary paths to running in AWS:


A very useful and low-cost mechanism for provisioning a container image *without*
generating a disk image is to use the `bootc install to-existing-root`
flow, where the container image "installs itself" alongside an existing root
filesystem, setting it up for the next boot. ( {bootc-upstream}/bootc-install.html#using-bootc-install-to-existing-root[Upstream documentation] )

As a convenience, system-reinstall-bootc can be used in place of `bootc install to-existing-root`.
See {bootc-upstream}/bootc-install.html#using-system-reinstall-bootc[the system-reinstall-bootc docs]
for details.


The https://github.com/osbuild/bootc-image-builder/[bootc-image-builder] tool can
generate a disk image (such as an AMI) from a given container image.



The fundamental architecture here is:

Launch existing package based + cloud-init Fedora/CentOS AMI,
with attached cloud-init metadata that installs podman,
which fetch your target container that install itself to the target root.

Then on reboot, your system has been replaced with the container image content.

Save the following file as e.g. `run-bootc-install`:
[source,text]
----
#!/bin/bash
dnf -y install podman
podman run --rm --privileged -v /dev:/dev -v /var/lib/containers:/var/lib/containers -v /:/target \
--pid=host --security-opt label=type:unconfined_t \
<image> \
bootc install to-existing-root
----

Replace `<image>` with your container image in this text file.

NOTE: The default base image does not include `cloud-init`; so any keys fetched
from the instance metadata will not by default be re-applied on the subsequent boot.
As the `bootc` documentation notes, there is special case support for injecting a SSH key
from the initial running system.

To do this, add `--root-ssh-authorized-keys /target/root/.ssh/authorized_keys`
(or `/target/home/cloud-user/.ssh/authorized_keys`, depending on your cloud-init configuration).

The SSH `authorized_keys` data will be copied from the host system into the target
system.

.Launching an instance
[source,bash]
----
NAME='instance1'
SSHKEY='my-key'     # optional: the name of your SSH key: `aws ec2 describe-key-pairs`
IMAGE='ami-xxx'     # the AMI ID as used by e.g. https://fedoraproject.org/cloud/download or https://www.centos.org/download/aws-images/
DISK='20'           # the size of the hard disk
REGION='us-east-1'  # the target region
TYPE='m5.large'     # the instance type
SUBNET='subnet-xxx' # the subnet: `aws ec2 describe-subnets`
SECURITY_GROUPS='sg-xx' # the security group `aws ec2 describe-security-groups`
USERDATA='/path/to/run-bootc-install' # path to your user-data script
aws ec2 run-instances                     \
--region $REGION                      \
--image-id $IMAGE                     \
--instance-type $TYPE                 \
${SSHKEY:+--key-name $SSHKEY}         \
--subnet-id $SUBNET                   \
--security-group-ids $SECURITY_GROUPS \
--user-data "file://${USERDATA}"      \
--tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=${NAME}}]" \
--block-device-mappings "VirtualName=/dev/xvda,DeviceName=/dev/xvda,Ebs={VolumeSize=${DISK}}"
----



The canonical documentation is in the https://github.com/osbuild/bootc-image-builder/?tab=readme-ov-file#amazon-machine-images-amis[bootc-image-builder]
upstream site. Conceptually, the tool takes as input a bootc container image, and can create disk
images such as AMIs and also help upload them to cloud infrastructure.

Additionally, "static" configuration can be injected as part of the disk image.

Once you have generated an AMI, *subsequent* operating system changes can be made
by just building and pushing a new container image to the registry; any existing
instances will fetch updates from it automatically and synchronize to that state.
Hence, you do not need to produce a new AMI for every container image change.

A generally useful pattern is to only produce new AMIs periodically, when you
want to ensure that newly provisioned instances boot directly into the latest
container image state.


- You will need the https://github.com/vmware/govmomi/tree/main/govc[govc] tool.
- A Linux host with kernel 6.x or newer




This example container build injects both `open-vm-tools` and `cloud-init`,
so that SSH credentials can be injected per virtual machine instantiation.

[source]
----
FROM <base image>
RUN dnf install -y open-vm-tools cloud-init && dnf clean all && rm -rf /var/cache /var/log/dnf && \
systemctl enable vmtoolsd.service
----

Include the additional software or tooling you want in your image as well; see
xref:building-containers.adoc[Building containers].


This uses https://github.com/osbuild/bootc-image-builder[bootc-image-builder]:

[source,subs="attributes"]
----
$ podman run --rm -it --privileged -v /var/lib/containers/storage:/var/lib/containers/storage -v .:/output --security-opt label=type:unconfined_t \
--pull newer {bib-image} --local --rootfs xfs --type vmdk <your container image>
----

The generated VMDK file will be present in `vmdk/disk.vmdk`.


[source]
----
govc import.vmdk \
-dc="${DATACENTER}" \
-ds="${DATASTORE}" \
-pool="${DATACENTER_POOL}" \
vmdk/disk.vmdk \
${DESTINATION_FOLDER}
----


[source]
----
govc vm.create \
-dc="${DATACENTER}" \
-ds="${DATASTORE}" \
-pool="{DATACENTER_POOL}" \
-net="${NETWORK}" \
-disk.controller=pvscsi \
-on=false \
-c=${CPUS} \
-m=${MEMORY} \
-g="rhel9_64Guest" \
-firmware="${FIRMWARE}" \
"${VM_NAME}"
----


[source]
----
govc vm.disk.attach \
-dc="${DATACENTER}" \
-ds="${DATASTORE}" \
-vm="${VM_NAME}" \
-link=false \
-disk="${DESTINATION_FOLDER}/disk.vmdk"
----

This guide shows how to provision new Fedora/CentOS bootc instances on the
https://cloud.google.com/products/compute[Google Compute Engine] platform.


You'll need a Google Compute Engine account, and if following this full example, also https://opentofu.org/[OpenTofu].
To be clear, the OpenTofu usage is just an example, you can provision instances
however you like, including the https://cloud.google.com/sdk/gcloud[gcloud CLI]
interactively in the console GUI, or in a Kubernetes environment
using https://cluster-api.sigs.k8s.io/[Cluster API], etc.



In particular for Google Compute Engine, this means that the base image
does *not* integrate with https://cloud.google.com/compute/docs/oslogin[OSLogin] by
default.


At the current time, the {projname} project does not produce pre-built disk images
for the base images.


The https://github.com/osbuild/bootc-image-builder/[bootc-image-builder] tool does
not yet support generating GCP disk images (however, this would be relatively
easy to fix).


The default SSH key used by GCP when deploying a VM has a short lifetime. By default,
system-reinstall-bootc will prompt to use the short lived SSH key to access the bootc system.
Be sure to use a permanent SSH key instead or you may lose access to the system.

For details on using system-reinstall-bootc to provision a GCP system, see the
{bootc-upstream}/bootc-install.html#using-system-reinstall-bootc[system-reinstall-bootc docs.]


This is effectively a variant of the xref:provisioning-aws[AWS example]
that uses the `aws` CLI in concert with `bootc install to-existing-root`,
except instead of the CLI tool we are using https://opentofu.org/[OpenTofu]
to more fully automate provisioning.

Copy and modify the following code:

.main.tf
[,terraform]
----
provider "google" {
project = var.project
region  = var.region
zone = var.region_zone
}

resource "google_compute_instance" "bootc_test" {
name         = "bootc-test"
machine_type = "e2-standard-4"
tags = ["bootc-test"]
allow_stopping_for_update = true

boot_disk {
initialize_params {
# This instance uses the default RHEL 9 as a "launcher image"
image = "rhel-cloud/rhel-9"
}
}

# LOOK HERE
# This is really the main interesting thing going on; we're injecting a "startup script"
# via GCE instance metadata into the stock RHEL-9 guest image. This script fetches our
# target container image, and reboots into it - *taking over* the existing instance.
metadata_startup_script = <<-EOS
dnf -y install podman && podman run --rm --privileged -v /dev:/dev -v /:/target -v /var/lib/containers:/var/lib/containers --pid=host --security-opt label=type:unconfined_t ${var.bootc_image} bootc install to-existing-root && reboot
EOS

network_interface {
# A default network is created for all GCP projects
network = "default"
access_config {
}
}
----

.variables.tf
[,terraform]
----
variable "project" {
type = string
description = "Your GCP project ID"
}

variable "region" {
type = string
description = "GCP region"
default = "us-central1"
}

variable "region_zone" {
type = string
description = "GCP region and zone"
default = "us-central1-f"
}

# This is the new important variable!  It will be injected into the startup
# script; see `provision.tf`.
variable "bootc_image" {
type = string
description = "Your bootable container"
}
----

You will need to perform basic replacements in `vars.tf`, including
at a minimum your desired container image. But it is almost
certain that you will want to modify `main.tf` to include it
as part of a more substatial workload that may include network firewalls
or routers, etc.

Once you are ready, follow the https://opentofu.org/docs/intro/core-workflow/[OpenTofu workflow]
to provision and update the system.

TIP: OpenTofu is a good tool to manage cloud-level infrastructure. However,
once you want to make changes to the operating system itself, you can
use a fully container-native workflow - just push changes to the registry
and the instances will update in place.

Not every deployment platform is explicitly covered by
this project. However, the aim of this project
is to support deployment anywhere that runs Linux
as a general rule.


Many virtualized environments can be configured to boot via an ISO (or PXE);
then the xref:bare-metal.adoc[Installing on bare metal] path can
be used to install.


The {bootc-image-builder}[bootc-image-builder] tool can generate e.g. `raw`
disk images from the container image, which can often be transformed
via additional tooling into a format suitable for a specific
virtualization platform.

For more, see xref:qemu-and-libvirt.adoc[qemu and libvirt].


The {bootc-upstream}/bootc-install.html#using-bootc-install-to-existing-root[`bootc install to-existing-root`]
path supports installing "inside" an existing running Linux system, effectively
replacing its contents with the container image. This allows provisioning
systems using *existing* disk images already uploaded and managed
as part of the virtualization platform.

As a convenience, system-reinstall-bootc can be used in place of `bootc install to-existing-root`.
See {bootc-upstream}/bootc-install.html#using-system-reinstall-bootc[the system-reinstall-bootc docs]
for details.


Fedora base images from this project do not specify a default root filesystem.
CentOS Stream defaults to `xfs`.

Some provisioning tools (such as Anaconda) provide their own defaults,
while others (`bootc-image-builder`, `bootc install to-disk`) require
explicit configuration.

You can configure the default root filesystem at build time
via {bootc-upstream}man/bootc-install-config.5.html[bootc install-config],
or at deployment time via bootc-image-builder's `--rootfs`.

The base image does not contain any special hypervisor-specific agents.
The following specifically are not included for example:

- cloud-init
- vmware-guest-agent
- google-guest-agent
- qemu-guest-agent
- ignition
- afterburn

etc.


For deployment to bare metal using e.g. Anaconda or `bootc install`, none of these are necessary.


The goal of this project is to provide a generic base image; it is fully supported to install any desired cloud specific agents.
See

- xref:provisioning-aws.adoc[Provisioning AWS] for a cloud-init case


A model we aim to emphasize is having the container image define the "source of truth" for system state.
This conflicts with using e.g. `cloud-init` and having it fetch instance metadata and raises questions around changes to the instance metadata and when they apply.

Related to this, `vmware-guest-agent` includes a full "backdoor" mechanism to log into the OS.


A high level goal is also to aim for containerizing agents.



Ignition as shipped by CoreOS Container Linux derivatives such as https://fedoraproject.org/en/coreos/[Fedora CoreOS] has a lot of advantages in providing a model that works smoothly across both bare metal and virtualized scenarios.

It also has some compelling advantages over cloud-init at a technical level.

But just like cloud-init, there is also significant overlap between a container-focused model of the world and an Ignition-focused model.

Nevertheless, they don't actively conflict, and it is very likely that e.g. Fedora CoreOS will "rebase" on top of this project.

You can run any of the containers via any OCI-compliant runtime, such as `podman`, `docker`, etc.
Be sure to consult documentation for your runtime, e.g. https://podman.io/docs[Podman documentation].

The images are configured to launch systemd by default, but in order to just inspect
the system you will likely want to invoke e.g. `bash`:

```
$ podman run --rm -ti <yourimage> bash
bash-5.1# rpm -q kernel systemd podman bootc
kernel-5.14.0-432.el9.aarch64
systemd-252-32.el9.aarch64
podman-5.0.0-1.el9.aarch64
bootc-0.1.7-1.el9.aarch64
bash-5.1#
```

If systemd is launched instead, you can enter the container using e.g. `podman exec`:

```
$ podman exec -ti wizardly_kare bash
```

to get an interactive shell (replace `wizardly_kare` with the actual container name).

TIP: The default `ENTRYPOINT` is `/sbin/init`, which will launch systemd. If you forgot
to specify `bash` above, you will likely be at a login prompt which is usually not
desired. Use the default `Ctrl-p Ctrl-q` detach sequence to disconnect the terminal,
and from there you can e.g. `podman rm -f` the container.


At the current time, not all services are expected to run when executed without
any additional privileges, i.e. the defaults for `podman/docker run`.
This is a bug, albeit mostly a cosmetic one. For more information see
https://gitlab.com/fedora/bootc/tracker/-/issues/43[this tracking issue].

These services will run when the system is "deployed" to a virtual or physical machine,
or when the container is run safely with a few additional privileges and namespacing.


It makes a lot of sense to "test" your operating system container *as a container*
before handing it off to more expensive CI/CD pipelines to provision virtual
or physical machines or upgrade in place. However, before you do this you
should carefully consider all of the below.


Never use a "bare" `--privileged`. At a minimum, you should use `--privileged -v /sys:/sys:ro`.
This is documented in the systemd https://systemd.io/CONTAINER_INTERFACE/[container interface];
having a readonly `/sys` ensures that the udev in the container won't try to manage devices.

Additionally, you should ensure that the container is invoked in a user namespace.
When using an unprivileged runtime (e.g. podman as non-root) this is the default.
When running podman as root, use `--userns=auto`. For more information
see e.g. https://github.com/containers/podman/discussions/13728[this discussion].


You should never use `--net=host` with `default.target`. This will spawn e.g. `sshd` which will attempt to listen
on port 22 and such. When user namespacing is in use, this will just fail; if
you forget to set up a user namespace, this combination could cause the container
to try to interfere with the host services.


It's a good idea to use `--volume /var` when running as a container; this will
help enable scenarios like nested containerization (avoiding overlay-on-overlay
for a nested `/var/lib/containers`) etc.

It also helps decouple the OS container image from persistent state, for
the same reasons as application containers.


Some tooling may want to create block devices (e.g. LVM). This will
be properly denied when the container is spawned in a user namespace.
In general you should not attempt to do this, but focus such testing
on virtual or physical machines.

* System Configuration


There are no default interactive users other than `root` in the base image.

In the default `full` base image, OpenSSH is running; but there are no
hardcoded credentials (passwords or SSH keys).


There are multiple mechanisms for users, groups and SSH keys, depending
on the chosen installation path.


The two paths to generate disk images support users, groups and SSH keys:

- Anaconda: xref:bare-metal.adoc[Installing on bare metal]
- https://github.com/osbuild/bootc-image-builder/[bootc-image-builder] supports a `config.json`


Tools such as `cloud-init` (which can be added as part of a derived
build), (or in general anything that ultimately invokes `useradd` at runtime on the target system, these users become
"local mutable state", with entries in `/etc/passwd` and `/var/home/$user`.


This kickstart fragment will inject a SSH key for the root user:

[source]
----
rootpw --iscrypted locked
sshkey --username root "<your key here>"
----

NOTE: The need for the `rootpw` is a bug/misdesign in Anaconda that will be fixed in the future. The default
root password defaults to being locked already.


Similar to kickstart authentication, the bootc-image-builder project for generating
disk images supports a `config.json`. For more information, see https://github.com/osbuild/bootc-image-builder?tab=readme-ov-file#-build-config[the bootc-image-builder docs].

Inline example:

[source]
----
{
"blueprint": {
"customizations": {
"user": [
{
"name": "alice",
"key": "ssh-rsa AAA ... user@email.com",
"groups": [
"wheel"
]
}
]
}
}
}
----


The https://www.freedesktop.org/software/systemd/man/latest/systemd-sysusers.html[systemd-sysusers]
process also runs on each boot, adding local mutable users starting
from the definitions in the image.


The base images use https://github.com/aperezdc/nss-altfiles[nss-altfiles], with some statically-allocated users
in `/usr/lib/passwd` and `/usr/lib/group` that are part of the immutable
base. It is possible to extend this in derived builds; however,
using either systemd `DynamicUser=yes` or
https://systemd.io/USER_RECORD/[JSON user records] for users is preferred.


For more, see {bootc-upstream}building/users-and-groups.html[bootc generic user/group guidance].


TIP: The https://github.com/cgwalters/osbuild-cfg[osbuild-cfg] project is aiming
to create a fully declarative interface for a subset of operating
system configuration tasks, and includes support for SSH keys for root.


The default root home directory `/root` on bootc images is a
symbolic link to `/var/roothome`, defaulting to machine-local
state. For more on `/var` see xref:filesystem.adoc[Filesystem].

At the current time, when run as a container image the `/var/roothome`
directory will not exist. This has a few side effects; one of them
is that the default `~/.bashrc` that is injected into the current
"app" containers like `quay.io/fedora/fedora:41` will not be sourced,
and `$PATH` will hence be different.

In general, if you just happen to need to run some tools at build time,
you can run e.g.:

[source]
----
RUN mkdir -p -m 0700 /var/roothome && \
<your provisioning tooling here> && \
rm -rf /var/roothome
----

which will help keep the image clean, avoiding leaking build-time
code into the final image.


Similarly, the `/home` directory is a symbolic link to `/var/home`
and does not exist by default. For more on non-root users and
groups, see xref:authentication[Authentication, Users and Groups].

The bootc model emphasises building a custom container image,
binding together a base operating system with configuration
in a "static" manner.

However, there are no default restrictions on "dynamic"
reconfiguration.  For example, you can include default
firewall rules in the base image configuration, but
it's also possible to directly apply live firewalling
changes by invoking tools such as `nft` or `firewall-cmd`
etc. by invoking the command directly, or scripting
it across multiple machines via tooling such as
https://www.ansible.com[Ansible].


Using firewalling as an example, the default for the
https://firewalld.org/documentation/man-pages/firewall-cmd.html[firewall-cmd]
tool is that changes are *not* persistent across a reboot;
you need to explicitly use the `--permanent` flag, which will
cause the changes to be written to the `/etc` directory.

By default, the `/etc` directory is persistent,
and changes made via tools such as `firewall-cmd --permanent` can
over time lead to "state drift"; the contents of the `/etc` on the
system will differ from the one described in the container image.
In this default configuration, a best practice is
to first make the changes in the base image, queuing the
changes (but not restarting running systems), and also simultaneously
write e.g. an Ansible playbook to apply the changes
to existing systems, but just in memory.

NOTE: It is also possible to configure the `/etc` directory
to be transient as well.  For more, see xref:filesystem.adoc[Filesystem].


The `/run` directory is an https://www.freedesktop.org/wiki/Software/systemd/APIFileSystems/[API filesystem]
that is defined to be deleted when the system is restarted.  It is a best
practice to use this directory for transient files.



A common pattern is to contact a remote network server
for dynamic configuration state. This is effectively how https://kubernetes.io/[Kubernetes]
operates - the API server defines the desired state of the system,
and machines reconcile to that state.

In this model, you may either include code directly embedded in your base
image or a privileged container that contacts the remote network server for configuration,
and itself spawns further container images (for example, via the https://docs.podman.io/en/latest/_static/api.html[podman API]).


Instead of a "pull" based model, at least some workloads may
best match a "push" model, as implemented by tooling such as
https://www.ansible.com/[Ansible].



For systemd units, there is full support for dynamic transient reconfiguration
or launching new services by writing to the `/run/systemd` directory.  For
example, `systemctl edit --runtime foo.service` will allow dynamically changing
the configuration of the `foo.service` unit, without persisting the changes.


There is a `/run/NetworkManager/conf.d` directory for applying temporary
network configuration.

The `nmcli connection modify` command by default writes persistent changes;
there is a `--temporary` flag that can be used to make changes only in memory.


The default for `podman run` is to create a container that will persist
across system reboots.  The `--rm` flag can be used for transient containers.
For more on this, see xref:running-containers.adoc[Running containers] as well
as the https://podman.io/[Podman documentation].


In many cases of dynamic reconfiguration, a tool may want to execute code
on the host filesystem, perhaps copying it over SSH - for example, Ansible
does this.

But even "systems management" agents that run as a container may still
have a need to execute code.

Often some of these tools need to handle a diverse set of operating
systems.

The recommendation is to place this dynamic code in `/tmp` by default.
A benefit of this location is that it's clear that the code is only
temporary and should go away on reboot. Some systems may
attempt "hardening" by making `/tmp` be mounted `noexec`, but that's
always been trivial to bypass by writing a file and then pointing
the interpreter at it (e.g. `/bin/bash /tmp/myfile` instead of just
executing `/tmp/myfile`).

Specifically placing it in e.g. `/root` may cause it to persist
across reboots, which is commonly not desired.

The {projname} default container image includes https://github.com/rpm-software-management/dnf[dnf].


A key toplevel goal of this project is to support and encourage creating
custom derived container images, using every tool and technique that applies
to "application" containers.

Many examples use `RUN dnf install`.  There is nothing special about the use
of `dnf` in a bootc-derived container versus an application container; it
is exactly the same code.


The system is mounted read-only when "deployed" as a physical or virtual machine.

At the current time, depending on the version, `dnf install` for
example may appear to start working, but will error out with e.g.

`error: can't create transaction lock on /usr/share/rpm/.rpm.lock (Read-only file system)`

This is a known bug; see e.g. https://github.com/rpm-software-management/dnf/pull/2053/[this PR].


Using the `bootc usroverlay` command, a transient writable overlay filesystem
is created for `/usr`, and `dnf install` will write to that overlay.  This is
a useful pattern for installing debugging tools but changes will be lost on reboot.


At the current time, `rpm-ostree` is also included in the base image, though
this may change in a future version. For more, see xref:rpm-ostree.adoc[rpm-ostree].


Conceptually, it would be possible to extend the tooling that surrounds
`fedora-bootc` to also enable a model where this would work:

```
FROM quay.io/fedora/fedora:41
RUN dnf -y install kernel
```

And then pass that to e.g. Anaconda to write to disk.

In this theoretical model, there wouldn't be `bootc`
in the deployed image, and updates could be done by
`dnf` directly.

The core disadvantages of this would be:

- Losing transactional updates and default runtime immutability
- Losing support for updating non-RPM content

At the current time such a model is not within the direct
focus of this project.

However, technically this would only be a small
extension of the existing Anaconda support for deploying
a raw tarball, and if someone was interested in working
on it, it could be added to the overall Fedora/CentOS
container-related project scope.


{projname} is a container image that can be "materialized" into a disk image in 3 different ways:

- Anaconda (kickstart)
- bootc-image-builder
- `bootc install`



The `full` images include support for:

- `xfs`/`ext4`
- `btrfs` (Fedora only)
- LVM
- LUKS

But you can of course add other storage packages directly to the host system.


You can use https://docs.fedoraproject.org/en-US/fedora/f36/install-guide/advanced/Kickstart_Installations/[Kickstart]
to configure storage with Anaconda.


You can use the https://github.com/osbuild/bootc-image-builder[bootc-image-builder] tool to create a disk image.
The available configuration for partitioning and layout is fairly fixed.

Note that the default filesystem type does come from the container image's `bootc install` configuration.


There is a default systemd unit included `bootc-generic-growpart.service` which will
run on each boot, and attempt to expand the partition and filesystem backing
the root device.
For more information, see xref:storage-bootc-generic-growpart.adoc[bootc-generic-growpart.service].



See {bootc-upstream}/bootc-install.html[bootc install] for more information.
The `bootc install to-disk` can be used to directly write a "simple" flat storage configuration.

More advanced installations (especially to bare metal) can also
be done via {bootc-upstream}/man/bootc-install-to-filesystem.html[bootc install to-filesystem];
you can set up the target block storage and filesystem however
you desire via external tools, then have your custom container
image "self-install" to it.


{projname} uses the bootc upstream project which currently uses ostree as a backend.

For more on the filesystem layout, see {bootc-upstream}/filesystem.html[bootc filesystem].


The {projname} project enables `composefs` for storage by default. The
`/opt` and `/usr/local` paths are plain directories, and *not*
symbolic links into `/var`. This means it is easier to install
3rd-party content in derived container images that write into `/opt`
for example.


{projname} uses GRUB by default (except on s390x). A menu entry is created for each version of {projname} currently available on a system. This menu entry references an `ostree` deployment which consists of a Linux kernel, an initramfs and a hash linking to an `ostree` commit (passed via the `ostree=` kernel argument). During bootup, `ostree` will read this kernel argument to determine which deployment to use as the root filesystem. Each update or change to the system (package installation, addition of kernel arguments) creates a new deployment. This enables rolling back to a previous deployment if the update causes problems.



The default "full" images include `NetworkManager`.
Unless otherwise configured, {projname} will attempt DHCP on every interface with a cable plugged in.

However, if you need to use static addressing or more complex networking (vlans, bonds, bridges, teams, etc.), you can do so in a number of ways which are summarized below.
Regardless of the way you choose to configure networking it all ends up as configuration for NetworkManager, which takes the form of NetworkManager keyfiles.
More information on the keyfile format can be found https://networkmanager.dev/docs/api/latest/nm-settings-keyfile.html[here].
More information on the subsection options for keyfiles can be found https://networkmanager.dev/docs/api/latest/ref-settings.html[here].


Complex networking configuration often also requires *per-machine* state.
It is of course possible to generate machine-specific container images that have e.g. static IP addressing included.
Another pattern is to include code to *generate* network configuration from inside the image by inspecting the host's MAC address.


It is supported to use Anaconda https://pykickstart.readthedocs.io/en/latest/kickstart-docs.html#network[kickstart support to configure networking] (Wi-Fi including) for bare metal installations.

Note that these will generally be stored in `/etc/NetworkManager/system-connections/`, and be inherently per-machine state.


On the first boot of a machine a user can provide kernel arguments that define networking configuration.
These kernel arguments are mostly defined in the https://man7.org/linux/man-pages/man7/dracut.cmdline.7.html[dracut.cmdline man page].
There are a few different ways to apply these kernel arguments on first boot.

When using `bootc install`, it is also possible to set per-machine kernel arguments via `--karg`.


`nmcli`, the NetworkManager command line tool, provides an "offline" mode that doesn't talk with the NetworkManager daemon and just writes the keyfile content to standard output. Run the tool for each connection profile you want to create:

[source]
----
$ nmcli --offline connection add \
type ethernet ifname enp1s0 \
ipv4.method manual ipv4.addresses 192.0.0.1/24 \
ipv6.method disabled

[connection]
id=ethernet-enp1s0
uuid=ff242096-f803-425f-9a77-4c3ec92686bd
type=ethernet
interface-name=enp1s0

[ethernet]

[ipv4]
address1=192.0.0.1/24
method=manual

[ipv6]
addr-gen-mode=default
method=disabled

[proxy]
----

See the https://networkmanager.dev/docs/api/latest/nm-settings-nmcli.html[settings man page] for a list of the properties that can be specified via `nmcli`. Bash autocompletion is available.


NetworkManager ships a tool, https://networkmanager.dev/docs/api/latest/nm-initrd-generator.html[nm-initrd-generator], that can generate keyfiles from dracut kernel argument syntax.
This might be a good way to either convert from kernel arguments to keyfiles or to just quickly generate some keyfiles giving a small amount of input and then tweak some more detailed settings.

Here's an example of generating keyfiles for a bond via `nm-initrd-generator`:

[source,bash,subs="attributes"]
----
$ podman run --rm -ti {container-c9s} /usr/libexec/nm-initrd-generator -s -- "ip=bond0:dhcp" "bond=bond0:ens2,ens3:mode=active-backup,miimon=100" "nameserver=8.8.8.8"

*** Connection 'bond0' ***

[connection]
id=bond0
uuid=643c17b5-b364-4137-b273-33f450a45476
type=bond
interface-name=bond0
multi-connect=1
permissions=

[ethernet]
mac-address-blacklist=

[bond]
miimon=100
mode=active-backup

[ipv4]
dns=8.8.8.8;
dns-search=
may-fail=false
method=auto

[ipv6]
addr-gen-mode=eui64
dns-search=
method=auto

[proxy]

*** Connection 'ens3' ***

[connection]
id=ens3
uuid=b42cc917-fd87-47df-9ac2-34622ecddd8c
type=ethernet
interface-name=ens3
master=643c17b5-b364-4137-b273-33f450a45476
multi-connect=1
permissions=
slave-type=bond

[ethernet]
mac-address-blacklist=

*** Connection 'ens2' ***

[connection]
id=ens2
uuid=e111bb4e-3ee3-4612-afc2-1d2dfff97671
type=ethernet
interface-name=ens2
master=643c17b5-b364-4137-b273-33f450a45476
multi-connect=1
permissions=
slave-type=bond

[ethernet]
mac-address-blacklist=
----

This run generates three keyfiles.
One for `bond0`, one for `ens3`, and one for `ens2`.
You can take the generated output, add more settings or tweak existing settings, and then commit the files into a container image.



.Template
[source, bash]
----
ip=${ip}::${gateway}:${netmask}:${hostname}:${interface}:none:${nameserver}
----

.Rendered
[source, bash]
----
ip=10.10.10.10::10.10.10.1:255.255.255.0:myhostname:ens2:none:8.8.8.8
----


It's recommended to store NetworkManager configuration embedded in container images in `/usr/lib/NetworkManager/system-connections/` because this forms part of the immutable image state.

You can also write configuration to `/etc/NetworkManager/system-connections/` as part of the container image; the default OSTree "3 way merge" will apply with any machine-specific configuration.

The keyfiles must have root-only access permissions (`600`), otherwise NetworkManager will ignore them.


By default, NetworkManager will attempt to autoconfigure (DHCP/SLAAC) on every interface with a cable plugged in.
In some network environments this may not be desirable.
It's possible to change this behavior of NetworkManager with a configuration file dropin to e.g. `/usr/lib/NetworkManager/conf.d/noauto.conf`.

.Disable NetworkManager autoconfiguration of ethernet devices
[source,text]
----
[main]
# Do not do automatic (DHCP/SLAAC) configuration on ethernet devices
# with no other matching connections.
no-auto-default=*
----


{projname} ships with https://podman.io[podman] installed to run application containers.

Per the {bootc-upstream}/building/guidance.html#configuration[configuration guidance]
prefer using `/usr/share/containers/systemd` for including "static" container images that are launched
dynamically.


The following configures the systemd `caddy.service` to run https://www.busybox.net[busybox].

This is one of several examples in {git-examples}[Fedora/CentOS bootc examples].

{podman-docs}/podman-systemd.unit.5.html[Podman systemd] is functionality included in podman that allows starting containers via systemd using a systemd generator.

[source,text]
----
$ cat /usr/share/containers/systemd/caddy.image
[Image]
Name=docker.io/library/caddy
$ cat /usr/share/containers/systemd/caddy.container
[Unit]
Description=Run a demo webserver

[Container]
# This image happens to be multiarch and somewhat maintained
Image=docker.io/library/caddy
PublishPort=80:80
AutoUpdate=registry

[Install]
WantedBy=default.target
----


Note that that the example above uses an implicit floating `:latest`
tag. When configured this way, by default the specified image will be pulled
once at install time and not updated thereafter. This is rarely
desirable; however, the choice of how to update the application
containers is one that needs to be designed to match the specific
workload.


In all models described below, the "application" containers
are pushed and managed separately from the base bootc image.
They are fetched dynamically over the network - or in other
words, they are "physically" distinct from the base bootc image.

See xref:embedding-containers.adoc[Embedding containers] for
a model where the application workloads are physically embedded
in the base bootc image.


Instead of referencing a container by a generic floating tag
such as `:latest`, instead you can include dedicated versioned
tags or `sha256` digest inside the referenced container image.

The example `caddy` container has tags of the form `:2.6`, `:2.7`
etc. If you specify one of those via e.g.

```
Image=docker.io/library/caddy:2.7
```

Then you can change them in your "base bootc" container image definition,
and have them updated transactionally at the same time as
the host system.

The state of the system (including these workload containers) is then describeable via the single
base bootc container image.


In many cases however, you will want the ability to have at least some
application containers "float" versions distinct from the host system.

The {podman-docs}/podman-auto-update.1.html[podman-auto-update.timer]
unit can be enabled to automatically upgrade workload container images that
are explicitly configured to opt-in to automatic updates.


Updating container images dynamically, distinct from the base OS image is
one case of xref:dynamic-reconfiguration.adoc[Dynamic reconfiguration].

A common scenario is needing to configure the host system with a
"pull secret" necessary to fetch container images - which includes
the host updates itself!

There is a section in the upstream bootc documentation for
{bootc-upstream}/building/secrets.html#secrets-eg-container-pull-secrets[secrets in general]
that applies here.

The following content applies to the *built image*. When using an external
installer such as Anaconda (xref:bare-metal.adoc[Bare metal]) or https://github.com/osbuild/bootc-image-builder[bootc-image-builder], those systems will each need
to be configured with any applicable pull secrets.


See https://github.com/ostreedev/ostree-rs-ext/blob/9a4743a657ffe0435018d9720c6df80a486ca0f1/man/ostree-container-auth.md[ostree-container-auth.md].

The recommendation for host `bootc` updates is to write configuration to `/etc/ostree/auth.json`
(which is shared with `rpm-ostree`). As of relatively recently, `/usr/lib/ostree/auth.json`
is also supported.


See https://github.com/containers/image/blob/main/docs/containers-auth.json.5.md[containers-auth.json].


A general conceptual problem with all of the available containers-auth locations
that are accepted by `podman` today is that the two locations are underneath:

- `/run`: This vanishes on reboot, which is not usually desired
- `/root`: Part of root's home directory, which is local *mutable* state by default

There is discussion about adding a system-wide location for the container stack,
but this has not yet happened. More in https://github.com/containers/image/pull/1746[this pull request].


A common pattern will be using a single default global pull secret
for both bootc and podman.

The following container build demonstrates one approach to achieve this.

[subs="attributes"]
--
This reference example is maintained in {git-examples}/container-auth[container-auth].
--

.Containerfile
[source]
----
COPY link-podman-credentials.conf /usr/lib/tmpfiles.d/link-podman-credentials.conf
RUN --mount=type=secret,id=creds,required=true cp /run/secrets/creds /usr/lib/container-auth.json && \
chmod 0600 /usr/lib/container-auth.json && \
ln -sr /usr/lib/container-auth.json /etc/ostree/auth.json
----

.link-podman-credentials.conf
[source]
----
# Make /run/containers/0/auth.json (a transient runtime file)
# a symlink to our /usr/lib/container-auth.json (a persistent file)
# which is also symlinked from /etc/ostree/auth.json.
d /run/containers/0 0755 root root -
L /run/user/0/containers/auth.json - - - - ../../../../usr/lib/container-auth.json
----


https://en.wikipedia.org/wiki/Federal_Information_Processing_Standards[FIPS] includes
standards for cryptographic operations and can be configured as required.

First, you must add the configuration from {git-examples}/fips[fips] to your container build.

If you are using bootc-image-builder or `bootc install to-disk`, there are currently no
further steps required for system installation; however, see below.


When performing an xref:bare-metal.adoc[Anaconda installation] you must
*additionally* set `fips=1` on the kernel commandline for the installation
environment.

This is necessary because the Anaconda installer may itself perform
cryptographic operations such as setting up LUKS encrypted volumes.


Bootc systems gather their initial state from the Container it was deployed from, any configuration
under `/etc` will be automatically updated, deleted or replaced by any deployment done using bootc.

However, any file modified in any way after a deployment will be 100% the responsibility of the
system administrator and will no longer be touched by bootc even if those files are updated in
the containers the system is receiving updates from.

This is important to highlight when dealing with certificates, denylists/allowlists or crypto policies.
If on the fly changes need to happen without a deployment which will require a reboot, the system
administrator needs to be aware that manual updates will be required every time a package touches the
sensitive configuration.

As an example a system administrator trying to prevent changes to crypto policies at runtime and require
all changes to come from the image can do the following:

[source,dockerfile,subs="attributes"]
----
FROM quay.io/centos-bootc/centos-bootc:stream10
RUN &lt;&lt;EORUN
set -xeuo pipefail
update-crypto-policies --set FUTURE
mv /etc/crypto-policies /usr/lib/crypto-policies
ln -sr /usr/lib/crypto-policies /etc/crypto-policies
EORUN

----

This will prevent admins from changing /etc/crypto-policies by running update-crypto-policies manually.
They should see an output similar to:

[source]
----
$ update-crypto-policies --set LEGACY
Setting system policy to LEGACY
Error saving config for bind
Keeping original configuration
Error saving config for gnutls
…
Keeping original configuration
…
Error updating current policy dump
Note: System-wide crypto policies are applied on application start-up.
It is recommended to restart the system for the change of policies
to fully take place.
----

Similarly a system administrator could mask `update-ca-trust` to prevent running it during runtime.

On the other hand, if the system administrator needs to control the ca-trust, crypto-policies at
runtime they can run `update-ca-trust` or `update-crypto-policies` after manually modifying the configs
and again after update (once deployment is completed and the system is rebooted) to restore their respective configurations
in `/etc` to an expected state. This could be done with systemd drop-in or manually.

We understand this behavior might not be optimal if you need to apply an update without a deployment and rebooting,
we are exploring alternatives for future bootc version on this https://github.com/bootc-dev/bootc/issues/1251[Github issue].


To set a custom hostname for your system, write to the `/etc/hostname`
file, which acts as machine-specific state. This can be done
via Anaconda, or with a privileged container.

Once booted, you can also verify that the desired hostname has been set using `hostnamectl`.

If you are deploying to an environment requiring internet access via a proxy, you will want to configure services so that they can access resources as intended.

This is best done by defining a single file with required environment variables in your configuration, and to reference this via systemd drop-in unit files for all such services.


This common file has to be subsequently referenced explicitly by each service that requires internet access.

[source,yaml]
----
# /etc/example-proxy.env
https_proxy="http://example.com:8080"
all_proxy="http://example.com:8080"
http_proxy="http://example.com:8080"
HTTP_PROXY="http://example.com:8080"
HTTPS_PROXY="http://example.com:8080"
no_proxy="*.example.com,127.0.0.1,0.0.0.0,localhost"
----


The `bootc` and `podman` tools will commonly need proxy configuration. At the
current time, `bootc` does not always run as a systemd unit.

[source,yaml,subs="attributes"]
----
# /usr/lib/systemd/system/bootc-fetch-apply-updates.service.d/99-proxy.conf
[Service]
EnvironmentFile=/etc/example-proxy.env
----


Using the
{podman-docs}/podman-systemd.unit.5.html[Podman systemd] configuration,
similarly add `EnvironmentFile=/etc/example-proxy.env`.

You may also consider setting the https://github.com/containers/common/blob/main/docs/containers.conf.5.md[containers.conf] configuration for proxy and environment settings of podman and containers.

There are multiple ways to configure kernel arguments.


In most cases you should use `/usr/lib/bootc/kargs.d/` method of configuring
kernel arguments.

For details, please refer to the {bootc-upstream}/building/kernel-arguments.html[upstream documentation].

Via this method, kernel arguments can be applied at install time as well
as updated "day 2" via `bootc upgrade/switch`.

The remainder below covers special cases; by using Anaconda or
bootc-image-builder config, kernel arguments can be defined per-install/per-machine
or to be embedded in a disk image.

Kernel arguments defined below are carried forward across `bootc upgrade`,
but are otherwise unmanaged state - bootc will not change them.


The Anaconda https://pykickstart.readthedocs.io/en/latest/kickstart-docs.html#bootloader[bootloader kickstart verb] can be used to configure kernel arguments.

Note that like everything else written via Anaconda,
once set these arguments become "unmanaged" machine-specific
state.


The `bootc-image-builder` configuration file supports
`customizations.kernel.append`. See
https://github.com/osbuild/bootc-image-builder?tab=readme-ov-file#kernel-arguments-kernel-mapping[the upstream documentation].

Note that like everything else set via the bootc-image-builder
install config, once set these arguments become "unmanaged" machine-specific
state.


When invoking `bootc install` directly, there is also support for passing
`--karg`. These kernel arguments also become unmanaged state.

Finally, there is also support for kernel arguments in {bootc-upstream}man/bootc-install-config.5.html[bootc install config].
However, this logic predates the "day 2 updatable" kernel argument support in
`/usr/lib/bootc/kargs.d` and the use cases for install-time only kernel arguments
defined in the container image are rare.


If you are using a custom kernel, you can embed custom arguments into the kernel
binary.


The canonical kernel arguments are maintained in `/boot/loader/entries`. While
the `/boot` filesystem is expected to be mounted read-only by default, it is
supported to edit these files directly:

[source,bash]
----
$ unshare -m /bin/bash -c 'mount -o remount,rw /boot; bash'
$ $editor /boot/loader/entries/...
----

Note that doing so will currently change the *existing* bootloader entry.

The current base images do include `rpm-ostree` which has
`rpm-ostree kargs`; however, it is likely that `rpm-ostree`
will be dropped from the default base image.

This image uses https://github.com/dracutdevs/dracut[dracut]
to build an https://docs.kernel.org/admin-guide/initrd.html[initial RAM disk]
or "initrd" for short.

It is important to understand that the initrd is generated
at *build time* and included in the container image in
`/usr/lib/modules/$kver/initramfs.img`



Dracut supports "drop-in" config files; per the configuration
guidance prefer using the location in `/usr`, which is
`/usr/lib/dracut/dracut.conf.d`.

A configuration drop-in similar to the following can be
placed into e.g. `/usr/lib/dracut/dracut.conf.d/50-custom-added-modules.conf`:

[source]
----
add_dracutmodules+=" somemodule "
----


An invocation similar to the following will regenerate the initrd as part of a container
build:

[source,dockerfile]
----
RUN set -xe; kver=$(ls /usr/lib/modules); env DRACUT_NO_XATTR=1 dracut -vf /usr/lib/modules/$kver/initramfs.img "$kver"
----

Note that we must explicitly pass dracut the kernel version
to target; the default will attempt to pull the *running*
kernel version which is almost always the incorrect thing to do.

The Linux kernel offers a plethora of knobs under `/proc/sys` to control the availability of different features and tune performance parameters.

Values under `/proc/sys` can be changed directly at runtime, but such changes will not be persisted across reboots.
Persistent settings should be written under `/etc/sysctl.d/` during provisioning, in order to be applied on each boot.

This shows how to disable _SysRq_ keys:

.Example: configuring kernel tunable to disable SysRq keys
[source]
----
# /usr/lib/sysctl.d/90-sysrq.conf
kernel.sysrq = 0
----

Further details can be found in the systemd man pages https://www.freedesktop.org/software/systemd/man/sysctl.d.html[sysctl.d(5)] and https://www.freedesktop.org/software/systemd/man/systemd-sysctl.service.html[systemd-sysctl.service(8)].

To set your system keyboard layout (keymap), write to `/etc/vconsole.conf`:

[source,text]
----
# /etc/vconsole.conf
KEYMAP=de
----

Once booted, you can also verify that the desired keymap has been set using `localectl`.

.NOTE: This content is specific to Fedora and does not currently apply to CentOS.

Fedora nodes are counted by the Fedora infrastructure via the Count Me feature. This system is explicitly designed to make sure that no personally identifiable information is sent from counted systems. It also ensures that the Fedora infrastructure does not collect any personal data. The nickname for this counting mechanism is "Count Me", from the option name. Implementation details of this feature are available in https://fedoraproject.org/wiki/Changes/DNF_Better_Counting[DNF Better Counting change request for Fedora 32]. In short, the Count Me mechanism works by telling Fedora servers how old your system is (with a very large approximation).

CentOS bootc does not include Count Me infrastructure.


This system is explicitly designed to make sure that no personally identifiable information is sent from counted systems. It also ensures that the Fedora infrastructure does not collect any personal data. The nickname for this counting mechanism is "Count Me", from the option name. Implementation details of this feature are available in https://fedoraproject.org/wiki/Changes/DNF_Better_Counting[DNF Better Counting change request for Fedora 32]. In short, the Count Me mechanism works by telling Fedora servers how old your system is (with a very large approximation).

On {projname} in the `full` image, note that 3 different tools are included:

- `dnf`
- `bootc`
- `rpm-ostree` (currently)


If you use `dnf` as part of a container build, it will inherently run through the `countme` process unless you set `countme=0` in `/etc/yum.repos.d/fedora/*.repo`.


Because the base image includes `rpm-ostree`, it will run by default https://coreos.github.io/rpm-ostree/countme/[rpm-ostree as a stand-alone method]. The new implementation has the same privacy preserving properties as the original DNF implementation.

Additionally, if you invoke `dnf` at runtime on an installed system, it will also be counted unless
disabled in the `/etc/yum.repos.d` files.


The `dnf-makecache.timer` systemd unit would also by default invoke counting, except it is disabled
by default if `/run/ostree` is present, which suppresses the cache invocation by default.


At the current time there is no default interaction between the Count Me infrastructure and `bootc`.


You can disable the `countme` repo flags and the unit timer as part of a container build:

[source,dockerfile]
----
RUN sed -i -e s,countme=1,countme=0, /etc/yum.repos.d/*.repo && systemctl mask --now rpm-ostree-countme.timer
----

* System Architecture

This section briefly re-summarizes some of the content that is in the
upstream {bootc-upstream}/filesystem.html[bootc filesystem documentation].
Please consult that for more.


A first important difference between {projname} and other ostree-using
variants in Fedora derivatives is that {projname} enables the use of https://github.com/containers/composefs/[composefs]
for the root filesystem by default.

However, this use of composefs is in an "unsigned" mode; when targeting
a filesystem with fs-verity enabled, then fs-verity will be turned on.
However, no signature verification is enforced for default installations.


With just the two exceptions below, all directories are writable
as part of a container build; but once deployed (onto a physical
or virtual machine) `bootc` defaults to presenting the image content
as read-only.

This is similar to a semantic similar to that accessible
with `podman run --read-only` for application container images.


The `/etc` directory is persistent, mutable machine local state by default.
While it appears as a mount point, it is always part of the machine's
local root filesystem.

If for example you inject static IP addresses via Anaconda kickstarts,
they will persist here across upgrades.

A 3-way merge is applied across upgrades, with each "deployment"
having its own copy of `/etc`.

NOTE: It is not supported to attempt to make `/etc` a distinct
physical partition.  However, it can be made explicitly "transient";
for more see {git-examples}/transient-etc[transient-etc example].


When you change the contents of `/etc` in a derived container images, any
added or removed files will be applied on upgrades.  For example,
if you add a new file to `/etc/NetworkManager/conf.d/` in your
container to configure networking, this will apply on upgrades.

However, according to the 3-way merge logic, any machine-local modified files will "win"
by default.  This can be a common issue with modifications to `/etc/passwd`
in derived containers.  For more on users and groups, see {bootc-upstream}/building/users-and-groups.html[bootc users and groups].


The `/var` mount point is also persistent and mutable machine local state,
but there is only one physical copy of it.  This is the
storage location for the following state:

- Application container image state (`/var/lib/containers`) used by
`podman`
- system logs in `/var/log`
- User and root home directories (`/var/home` and `/var/roothome` respectively)
- General host-bound application state such as the default `/var/lib/postgresql` database path


It is supported to make the toplevel `/var` a mount point; however it's
more generally recommended to make sub-paths of `/var` such as
`/var/lib/custom-database` a mount point instead, or to target a
subdirectory of `/mnt` for example.

[IMPORTANT]
====
In a container build, you can write to `/var`.  However,
this will have a semantic similar to a Dockerfile `VOLUME` instruction;
the content from the container image is *only copied at initial install time*.
Any subsequent system updates will not by default see new changes.

It's recommended instead to use e.g. https://www.freedesktop.org/software/systemd/man/latest/tmpfiles.d.html[systemd tmpfiles.d]
as a way to ensure that newly added state "reconciles" across upgrades
as desired.
====


The `transient-ro` option allows privileged users to create dynamic top-level mountpoints
at runtime while keeping the filesystem read-only by default. This is particularly useful for
applications that need to bind mount host paths that may be platform-specific or dynamic.


This feature addresses scenarios where:

- Applications need to bind mount host directories that match the host's absolute paths
- Platform-specific mountpoints are required (e.g., `/Users` on macOS)
- Dynamic mountpoints need to be created after deployment but before application startup
- The filesystem should remain read-only for regular processes


To enable this feature, add the following to `/usr/lib/ostree/prepare-root.conf`:

[source,ini]
----
[root]
transient-ro = true
----

NOTE: When making changes to filesystem configuration, the initramfs also needs to be regenerated.
For more information, see xref:initramfs.adoc[The initial RAM disk (initrd)].

[IMPORTANT]
====
Due to a limitation in util-linux, the `LIBMOUNT_FORCE_MOUNT2=always` environment variable
must be set when performing mount operations with this feature. This is an issue
(https://github.com/util-linux/util-linux/issues/2283[util-linux issue #2283]) that affects
the mount namespace functionality required by `transient-ro`.

For a complete end-to-end example demonstrating this feature, see the
{git-examples}/transient-root-ro[transient-root-ro example] in the examples repository.
====


When `transient-ro=true` is set:

1. The overlayfs upper directory is mounted read-only by default
2. Privileged processes can mount it writable only in a new mount namespace, and perform arbitrary changes there, such as creating new toplevel mount points
3. These mountpoints persist for the current boot but do not survive reboots or upgrades
4. Regular processes continue to see a read-only filesystem


A common use case is with `podman machine` on macOS, where the VM needs to bind mount
host paths like `/Users/username` into the VM. With `transient-ro`, the system can:

1. Create the `/Users` directory dynamically at runtime
2. Bind mount the host's `/Users` directory to the VM's `/Users`
3. Keep the rest of the filesystem read-only for security


- Only privileged users (root) can create these mountpoints
- The mountpoints are transient and don't persist across reboots
- The filesystem remains read-only for non-privileged processes
- This feature should be used judiciously in production environments

For more information about this feature, see the upstream ostree documentation and the
related discussion in the bootc project.

A fundamental goal of this project is to make it easy
to create custom bootable Linux systems by building
container images.

However, not every use case needs or wants a specific
custom image; they can often use a "generic" image with
only a little bit of injected dynamic configuration.

One can use the term "base image" for an image such
as this one which is intended primarily to be used as
a derivation point, instead of being run directly.

The term we will use for the latter case to clearly distinguish
is "final image", again in contrast to "base image".

These two cases can also be further subdivided on an axis
of "generic" vs "specific" (to a use case).

A "generic" image is one that does not come with any
pre-set up configuration such as users, SSH keys, network
configuration, or applications.

A "specific" image in contrast is one explicitly designed
for a use case such as:

- Run a Postgres instance in a specific cloud
- Control software for a drone
- Be a graphical kiosk at a shopping mall

etc.


Our sister project https://docs.fedoraproject.org/en-US/fedora-coreos/[Fedora CoreOS]
is an excellent example of a "final generic image". It
is "final" in the sense that while it *does* have an associated
container image, it is not currently the focus of the project
to derive from that image.

It is also generic - there's again no pre-set users or applications.
It focuses on Ignition as a generic customization mechanism
that writes to `/etc` and `/usr/local` and other directories
which act as machine-local mutable state.


The `/etc` directory is intended for configuration, and
many operations that one might include in a container build
are also viable to inject as machine-local state into
that directory (via kickstart, cloud-init, Ignition, etc).

For example, an organization may create a "final"
image derived from this project, but for one specific
site want to enable or disable one or more systemd units,
or inject an extra container image.

Or, the organization may want to produce a "final" image
but allow their own users to decide more dynamically
whether or not to enable FIPS mode.

Many tasks like this boil down to writing files into `/etc`,
which defaults to machine-local persistent, mutable state
partially to support this design.

In other words: It is supported today to use this project
to produce "generic" images, and will continue to be
into the future. However, the primary focus is on providing
tools to build custom derived images.
* OS updates


{projname} provides atomic updates and rollbacks via {bootc-upstream}[bootc] deployments
for the host system.

By default, the OS performs continual auto-updates via a stock copy of the upstream
`bootc-fetch-apply-updates.timer` and corresponding `bootc-fetch-apply-updates.service`.

For more, see <{bootc-upstream}man/bootc-fetch-apply-updates.service.5.html>


Additionally for referenced application containers, the
{podman-docs}/podman-auto-update.1.html[podman-auto-update.timer]
unit can be enabled to automatically upgrade workload container images that
are explicitly configured to opt-in to automatic updates. The containers can
also be rolled back when properly configured. Please refer to the following
https://www.redhat.com/sysadmin/podman-auto-updates-rollbacks[article] for details.



The main focus of {projname} is on {bootc-upstream}[bootc]. However,
the "full" base image currently includes https://github.com/coreos/rpm-ostree/[rpm-ostree]
which shares a lot of the same underlying code from the https://github.com/ostreedev/ostree-rs-ext/[ostree]
project.


It is supported to install and use rpm-ostree on a {projname} system,
because the bootc and rpm-ostree projects share significant underlying code.


At the current time, operations such as `RUN rpm-ostree install foo` when
executed as part of a container build are not substantially different
from what happens with `RUN dnf install foo` - the two codebases
ultimately share a lot via https://github.com/rpm-software-management/libdnf[libdnf].

There's not generally a strong reason to use `rpm-ostree` inside a container
build.



The `rpm-ostree upgrade`, `rebase`, `deploy` verbs etc. will all gracefully
interact with a `bootc` system - at the current time they operate on shared state.

You can interchange `rpm-ostree upgrade` with `bootc upgrade`, etc; they
currently perform the same operations in general.


However, any local state mutations such as package layering, removals, or
enabling e.g. `rpm-ostree initramfs --enable` will cause `bootc upgrade`
to error out.

Hence if you choose to use such features, then you will need to switch over
to interacting with rpm-ostree going forward for updates.

In the more medium term, there are plans to extend `dnf` to also support
client-side functionality similar to this:
https://gitlab.com/fedora/bootc/tracker/-/issues/4[client side layering tracker].

This would build on more generic support for {bootc-upstream}/booting-local-builds.html[booting local builds]
for persistent layering.


For environments without a direct connection to a centralized container
registry, we encourage mirroring an on-premise registry if possible or manually
moving container images using `skopeo copy`.
See https://www.redhat.com/sysadmin/manage-container-registries[this blog]
for example.

For systems that require manual updates via USB drives, this procedure
describes how to use `skopeo` and `bootc switch`.

Copy image to USB Drive:

`skopeo copy docker://[registry]/[path to image] dir://run/media/$USER/$DRIVE/$DIR`

You may also use the https://github.com/containers/skopeo/blob/main/docs/skopeo-sync.1.md[skopeo sync] command to synchronize entire registries or namespaces.

**NOTE** Using the dir transport will create a number of files,
and it's recommended to place the image in it's own directory.
If the image is local the containers-storage transport will transfer
the image from a system directly to the drive:

`skopeo copy containers-storage:[image]:[tag] dir://run/media/$USER/$DRIVE/$DIR`

From the client system, insert the USB drive and mount it:

`mount /dev/$DRIVE /mnt`

`bootc switch` will direct the system to look at this mount point for future
updates, and is only necessary to run one time if you wish to continue
consuming updates from USB devices. note that if the mount point changes,
simply run this command to point to the alternate location. We recommend
using the same location each time to simplfy this.

`bootc switch --transport dir /mnt/$DIR`

Finally `bootc upgrade` will check for updates and reboot the system (when `--apply` is used).

`bootc upgrade --apply`


IMPORTANT: Updating the bootloader is not currently automatic.
The https://github.com/coreos/bootupd/[bootupd] project is included in {projname} and may be used for manual updates.

This is usually only relevant on bare metal scenarios, or virtualized
hypervisors that support Secure Boot. An example reason to update the
bootloader is for https://eclypsium.com/2020/07/29/theres-a-hole-in-the-boot/[the BootHole vulnerability].

Inspect the system status:

[source,bash]
----
# bootupctl status
Component EFI
Installed: grub2-efi-x64-1:2.04-31.fc33.x86_64,shim-x64-15-8.x86_64
Update: At latest version
#
----

If an update is available, use `bootupctl update` to apply it; the
change will take effect for the next reboot.

[source,bash]
----
# bootupctl update
...
Updated: grub2-efi-x64-1:2.04-31.fc33.x86_64,shim-x64-15-8.x86_64
#
----

Enable `bootloader-update.service` to automate bootupd updates

[source,text]
----
# systemctl enable bootloader-update.service
----


It is possible that future {projname} versions may default
to automating bootloader updates similar to the above.
* Management tooling
# Generic management advice

The upstream bootc project provides a low-level generic tool
that can be driven by multiple higher level management tooling.

The Fedora/CentOS bootc project provides a generic operating
system build.

By default, there is a simplistic xref:auto-updates.adoc[automatic update timer]
enabled. At any nontrivial scale, it is intended that this is replaced
with something more sophisticated that has knowledge of things like
rollouts, checking for application status, etc.

## Included client tooling

Note that the CentOS image includes https://github.com/RedHatInsights/rhc[rhc]
for use with Red Hat Insights.

## Ansible integration

The https://docs.ansible.com/projects/ansible/latest/collections/community/general/bootc_manage_module.html[bootc_manage module] in the Ansible `community.general` collection provides integration for managing bootc systems via Ansible playbooks.
# Fleet Management with Flight Control

Quoting from their documentation:

https://github.com/flightctl/flightctl[Flight Control] aims to provide simple, scalable, and secure
management of edge devices and applications. Users declare the operating system version, host configuration,
and set of applications they want to run on an individual device or a whole fleet of devices, and
Flight Control will roll out the target configuration to devices where a device agent will automatically
apply them and report progress and health status back up.

* Troubleshooting

When an update is complete, the previous OS deployment remains on disk. If an update causes issues, you can use it as a fallback.
By default, this is a manual operation, but you can automate it.


To temporarily boot the previous OS deployment, hold down `Shift` during the OS boot process. When the bootloader menu appears, select the relevant OS entry in the menu.


To cause the previous OS deployment to be the next boot:

[source,bash]
----
# Mark the previous OS deployment as the default, and immediately reboots into it
bootc rollback
----

Please note that the default `bootc-fetch-apply-updates.timer` will keep looking for updates and upgrade to any new available OS deployment, other than the one you just reverted.

If you prefer, you can temporarily turn off auto-updates. Later on, you can re-enable them in order to let the machine catch up with the usual flow of updates:

[source,bash]
----
systemctl disable --now bootc-fetch-apply-updates.timer

[...]

# At a later point, re-enable it to track updates to your container image.
systemctl enable --now bootc-fetch-apply-updates.timer
----

When you perform a rollback (e.g., with bootc rollback), any changes made to files in the `/etc` directory won’t carry over to the rolled-back deployment.
The /etc files will revert to their state from that previous deployment instead.

This is because `bootc rollback` just reorders the existing deployments. It doesn't create new deployments. The /etc merges happen when new deployments are created.

If you want to save a modified /etc file for use after the rollback:
You can copy it to a directory under `/var`, like /var/home/User (for a specific user) or /var/root/ (for the root user).
These directories aren’t affected by the rollback as it is user content.

Going back to the original state from either through a temporary rollback or another `bootc rollback`, the `/etc` directory will restore to its state from that original deployment.

Another option if one is sure the situation you are rolling back for is not the config files i.e content in /etc/ and you want to go to an older deployment you can `bootc switch`
to that older image, this will perform the /etc merge and deploy the previous version of the software.

If you've lost the private key of an SSH key pair used to log into a Fedora system, and do not have any password logins set up to use at the console, you can gain access back to the machine with the following steps:

. When booting the system, intercept the GRUB menu and edit the entry to append `init=/bin/bash``
. Wait for the system to boot into a shell prompt

Execute the following commands:

- `mount -t selinuxfs selinuxfs /sys/fs/selinux`
- `/sbin/load_policy`
- `passwd root`
- `sync`
- `/sbin/reboot -ff`

Or alternatively, you can manually attempt to update e.g. `~/.ssh/authorized_keys`.

Note that Fedora by default does not allow SSH login via password authentication.

The {projname} image does include some classic systems inspection
tools such as `lsof`, but the recommended approach is to leverage
containers with the https://containertoolbx.org/[toolbox] utility
included in the image.

NOTE: Due to an oversight, `toolbox` may not yet be in all images,
but it should be soon.


Toolbx is a utility that allows you to create privileged containers meant to
debug and troubleshoot your instance. It is a wrapper around podman which
starts long running containers with default mounts and namespaces to facilitate
debugging the host system.

These containers can then be used to install tools that you may need for
troubleshooting, keeping things distinct from the host root filesystem.


You can create a new toolbox by running the command below.

[source,sh]
----
toolbox create
----

You can then list all the running toolboxes running on the host. This should
show you your newly created toolbox.

[source,sh]
----
toolbox list
----

As pointed out by the output of the `toolbox create` command, you can enter the
following command to enter your toolbox.

[source,sh]
----
toolbox enter
----

Now that you're in the container, you can use the included `dnf` package
manager to install packages. For example, let's install `strace` to look at
read syscall done by the host's `toolbox` utility.

[source,sh]
----
sudo dnf install strace
# Some hosts directories are mounted at /run/host
strace -eread /run/host/usr/bin/toolbox list
----

Once done with your container, you can exit the container and then remove it
from the host using `toolbox rm` if desired.


* Related projects

This project is generally datacenter/server/edge focused.
However, there exist other projects which are using
similar technologies and are focused on Linux desktops.

In the future, it is likely that some of these projects
will "rebase" on {projname} as a common basis technology.


See https://fedoraproject.org/wiki/SIGs/AtomicDesktops[Fedora Atomic Desktops].

Notable differences/overlaps:

- {projname} and Fedora Atomic Desktops both use Anaconda.
- Fedora Atomic Desktops do not currently default to using containers for updates, but the legacy OSTree model
- Fedora Atomic Desktops do not use `composefs`, and do not use `bootupd`, and have a slightly different filesystem layout


The https://universal-blue.org/[Universal Blue] project is heavily focused on container-native
Linux desktops.

There is a lot of similarity to the Atomic Desktops, except that the default update
mechanism is containers, not OSTree.


By far the biggest difference between the
two is that {projname} emphasizes users *deriving*
from our container base image, making arbitrary,
potentially large changes. Up to and including
custom images.

Fedora CoreOS (much like Fedora Atomic Desktops)
generally emphasizes users *configuring* an
image pre-built by Fedora, running workloads
as containers in general, but also supporting
changing config files and kernel arguments.

A good example of the division line is
https://docs.fedoraproject.org/en-US/fedora-coreos/os-extensions/[Fedora CoreOS system extensions]
aka "package layering".

If you find the need to make such changes
to the host, then you may find {projname}
a better target.


- Fedora derivatives
- systemd, containers, ostree


[cols="1,1"]
|===
| Fedora/CentOS bootc | Fedora CoreOS
| Datacenter/server/edge (but can be extended)
| Datacenter/server/edge
| bootc
| ignition, rpm-ostree
| Includes python
| No python
| No cloud agents
| afterburn
| Anaconda, bootc-image-builder, bootc install
| Pre-built disk images, coreos-installer
| podman
| podman, docker
| user: root
| user: core
| composefs
| legacy ostree
| Versioned with Fedora base
| https://docs.fedoraproject.org/en-US/fedora-coreos/stream-metadata/[Custom streams]
| Automatic updates, but for *your* image
| Automatic updates from Fedora
| Targets Fedora and CentOS
| Targets Fedora only
|===
* Projects documentation
** {bootc-upstream}[bootc]
