= Getting Started with Fedora CoreOS

== Introduction

=== Streams

There are three Fedora CoreOS (FCOS) update streams available: `stable`, `testing`, and `next`. In general, you will want to use `stable`, but it is recommended to run some machines on `testing` and `next` as well and provide feedback.

Each stream has a canonical URL representing its current state in JSON format, known as "stream metadata". For example, the stream metadata URL for `stable` is: https://builds.coreos.fedoraproject.org/streams/stable.json

For automating Fedora CoreOS installations, it is expected that you will interact with stream metadata. While Fedora CoreOS does automatic in-place updates, it is generally recommended to start provisioning new machines from the latest images.

For more information on using stream metadata, see xref:stream-metadata.adoc[Stream Metadata].
For more about the available streams, see xref:update-streams.adoc[Update Streams].

=== Provisioning Philosophy

Fedora CoreOS does not have a separate install disk. Instead, every instance starts from a generic disk image which is customized on first boot via https://github.com/coreos/ignition[Ignition].

Each platform has specific logic to retrieve and apply the first boot configuration. For cloud deployments, Ignition gathers the configuration via user-data mechanisms. In the case of bare metal, Ignition can fetch its configuration from the disk or from a remote source.

For more information on configuration, refer to the documentation for xref:producing-ign.adoc[Producing an Ignition File].

== Quickstart

=== Booting on a cloud VM (AWS example)


A more complete example that allows customization is described in xref:provisioning-aws.adoc[].

=== Booting on a local hypervisor (libvirt example)


=== Exploring the OS

Once the VM has finished booting, its IP addresses will appear on the console. By design, there are no hard-coded default credentials.

If you set up an xref:authentication.adoc[SSH key] for the default `core` user, you can SSH into the VM and explore the OS:

[source, bash]
----
ssh core@<ip address>
----

== Getting in touch

We recommend that all users subscribe to the low-volume https://lists.fedoraproject.org/archives/list/coreos-status@lists.fedoraproject.org/[coreos-status mailing list] for operational notices related to Fedora CoreOS.

Bugs can be reported to the https://github.com/coreos/fedora-coreos-tracker[Fedora CoreOS Tracker].

For live questions, feel free to reach out in the https://chat.fedoraproject.org/#/room/#coreos:fedoraproject.org[#coreos:fedoraproject.org room on Matrix].

For doubts and longer discussions related to Fedora CoreOS, a https://discussion.fedoraproject.org/tag/coreos[forum] and a https://lists.fedoraproject.org/archives/list/coreos@lists.fedoraproject.org/[mailing list] are available.
* Provisioning Machines
= Stream metadata

Metadata about Fedora CoreOS is available in a custom JSON format, called "stream metadata". For maintaining automation, it is expected that you will interact with this stream metadata.

The format is stable, and intended to be relatively self-documenting. There is not yet a JSON schema.
However, in most web browsers, navigating to the URL will render the JSON in an easy-to-read form.

== Canonical URL

The URL for the `stable` stream is: https://builds.coreos.fedoraproject.org/streams/stable.json
You can similarly replace `stable` here with other available xref:update-streams.adoc[Update Streams].

== Using coreos-installer to download

The `coreos-installer` tool has built-in support for fetching artifacts:

[source, bash]
----
STREAM="stable"
coreos-installer download --decompress -s $STREAM -p openstack -f qcow2.xz
----

== Using coreos/stream-metadata-go

There is an official https://github.com/coreos/stream-metadata-go[coreos/stream-metadata-go] library for
software written in the Go programming language. The `README.md` file in that repository contains a link to example code.

== Example: Script ec2 CLI

Fetch the latest `x86_64` AMI in `us-west-1` and use it to launch an instance:

[source, bash]
----
$ AMI=$(curl -sSL https://builds.coreos.fedoraproject.org/streams/stable.json | jq -r '.architectures.x86_64.images.aws.regions["us-west-1"].image')
$ echo "${AMI}"
ami-021238084bf8c95ff
$ aws ec2 run-instances --region us-west-1 --image-id "${AMI}" ...
----
= Installing CoreOS on Bare Metal

This guide provides instructions to install Fedora CoreOS to bare metal. Three options are available:

* Installing from live ISO
* Installing from PXE
* Installing from the container

== Prerequisite

Before installing FCOS, you must have an Ignition configuration file and host it somewhere (e.g. using `python3 -m http.server`). If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: If you have servers with different types and/or number of hard drives, you must create a separate customized Ignition config for each machine (or machine class). A good model is to have the common parts of the configuration factored out into a separate Ignition configuration which can be merged into (via HTTP or inline) the per-machine custom config.

== Installing from live ISO

To install FCOS onto bare metal using the live ISO interactively, follow these steps:

- Download the latest ISO image from the https://fedoraproject.org/coreos/download/?stream=stable#baremetal[download page] or with podman (see https://coreos.github.io/coreos-installer/cmd/download/[documentation] for options):
[source, bash]
----
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release download -s stable -p metal -f iso
----

Note this is just using `coreos-installer` as a tool to download the ISO.

NOTE: You can boot the live ISO in either legacy BIOS or UEFI mode, regardless of what mode the OS will use once installed.

- Burn the ISO to disk. On Linux and macOS, you can use `dd`. On Windows, you can use https://rufus.ie/[Rufus] in "DD Image" mode.
- Boot it on the target system. The ISO is capable of bringing up a fully functioning FCOS system purely from memory (i.e. without using any disk storage). Once booted, you will have access to a bash command prompt.
- You can now run `coreos-installer`:
[source, bash]
----
sudo coreos-installer install /dev/sda \
--ignition-url https://example.com/example.ign
----

Once the installation is complete, you can reboot the system using `sudo reboot`. After rebooting, the first boot process begins. It is at this time that Ignition ingests the configuration file and provisions the system as specified.

For more advanced ISO installs, including automation, see below. For more about the live ISO image, see the xref:live-reference.adoc[live image reference].

TIP: Check out `coreos-installer install --help` for more options on how to install Fedora CoreOS.

== Installing from the network

NOTE: Booting the live PXE image requires at least 2 GiB of RAM with the `coreos.live.rootfs_url` kernel argument, and 4 GiB otherwise. You can install in either legacy boot (BIOS) mode or in UEFI mode, regardless of what mode the OS will use once installed.

=== Installing from PXE

To install from PXE, follow these steps:

- Download an FCOS PXE kernel, initramfs, and rootfs image:
[source, bash]
----
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release download -f pxe
----

- Follow this example `pxelinux.cfg` for booting the installer images with PXELINUX:

[source,subs="attributes"]
----
DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
KERNEL fedora-coreos-{stable-version}-live-kernel-x86_64
APPEND initrd=fedora-coreos-{stable-version}-live-initramfs.x86_64.img,fedora-coreos-{stable-version}-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://192.168.1.101:8000/config.ign
IPAPPEND 2
----

For more details on how to use this information, see this https://dustymabe.com/2019/01/04/easy-pxe-boot-testing-with-only-http-using-ipxe-and-libvirt/[blog post] for testing a PXE installation via a local VM and `libvirt`. For other supported kernel command-line options, see the https://coreos.github.io/coreos-installer/getting-started/#kernel-command-line-options-for-coreos-installer-running-as-a-service[coreos-installer docs], but note that `coreos-installer pxe customize` (see below) is more flexible. For more about the live PXE image, see the xref:live-reference.adoc[live image reference].

=== Installing from iPXE

An iPXE-capable machine needs to be provided with a relevant Boot Script to fetch and load FCOS artifacts.

The example below shows how to load those directly from Fedora infrastructure. For performance and reliability reasons it is recommended to mirror them on the local infrastructure, and then tweak the `BASEURL` as needed.

[source,subs="attributes"]
----
#!ipxe

set STREAM stable
set VERSION {stable-version}
set INSTALLDEV /dev/sda
set CONFIGURL https://example.com/config.ign

set BASEURL https://builds.coreos.fedoraproject.org/prod/streams/$\{STREAM}/builds/$\{VERSION}/x86_64

kernel $\{BASEURL}/fedora-coreos-$\{VERSION}-live-kernel.x86_64 initrd=main coreos.live.rootfs_url=$\{BASEURL}/fedora-coreos-$\{VERSION}-live-rootfs.x86_64.img coreos.inst.install_dev=$\{INSTALLDEV} coreos.inst.ignition_url=$\{CONFIGURL}
initrd --name main $\{BASEURL}/fedora-coreos-$\{VERSION}-live-initramfs.x86_64.img

boot
----

For other supported kernel command-line options, see the https://coreos.github.io/coreos-installer/getting-started/#kernel-command-line-options-for-coreos-installer-running-as-a-service[coreos-installer docs], but note that `coreos-installer pxe customize` (see below) is more flexible. For more about the live PXE image, see the xref:live-reference.adoc[live image reference].

== Installing from the container

You can use the `coreos-installer` https://quay.io/repository/coreos/coreos-installer[container] from an existing system to install to an attached block device. For example (substitute `docker` for `podman` if needed):

[source, bash]
----
sudo podman run --pull=always --privileged --rm \
-v /dev:/dev -v /run/udev:/run/udev -v .:/data -w /data \
quay.io/coreos/coreos-installer:release \
install /dev/vdb -i config.ign
----

In this example, `coreos-installer` will download the latest stable FCOS metal image and install it onto `/dev/vdb`. It will then inject the Ignition file `config.ign` in the current directory into the image. Use `--help` to see all the available options.

== Downloading and mirroring the metal image

Sometimes, it's necessary to download the metal image ahead of time and then have it passed locally to `coreos-installer` for installation. You can download the metal image directly from the https://fedoraproject.org/coreos/download/?stream=stable#baremetal[FCOS download page], or you can use `coreos-installer download`.

TIP: When installing via the live ISO or PXE, there is no need to download the metal image. It is already part of those environments.

There are two metal images: one for 512b-sector disks (labeled "Raw" on the download page), and one for 4k-sector native disks (labeled "Raw (4K Native)"). Unless you know to be targeting a 4k native disk, use the 512b one, which is the most common. See https://en.wikipedia.org/wiki/Advanced_Format#4K_native[this page] for more information.

To download the 4kn native metal image with `coreos-installer download`, use the `--format 4k.raw.xz` switch.

NOTE: The metal image uses a hybrid partition layout which supports both BIOS and UEFI booting.

When you're finally ready to install FCOS, you can point it at your downloaded image using `coreos-installer install --image-url <LOCAL_MIRROR>` or `coreos-installer --image-file <PATH>`.

== Customizing installation

The `coreos-installer iso customize` and `coreos-installer pxe customize` commands can be used to create customized ISO and PXE images with site-specific configuration, including the ability to perform unattended installations of Fedora CoreOS.

NOTE: When booting an image created with `coreos-installer pxe customize`, the PXE or iPXE kernel command line must include the arguments `ignition.firstboot ignition.platform.id=metal`. If running in a virtual machine, replace `metal` with the https://coreos.github.io/ignition/supported-platforms/[platform ID] for your platform, such as `qemu` or `vmware`.

For example:

[source,bash,subs="attributes"]
----
# Create customized.iso which:
# - Automatically installs to /dev/sda
# - Provisions the installed system with config.ign
# - Configures the installed GRUB and kernel to use a primary graphical
#   and secondary serial console
# - Uses network configuration from static-ip.nmconnection
# - Trusts HTTPS certificates signed by ca.pem
# - Runs post.sh after installing
coreos-installer iso customize \
--dest-device /dev/sda \
--dest-ignition config.ign \
--dest-console ttyS0,115200n8 \
--dest-console tty0 \
--network-keyfile static-ip.nmconnection \
--ignition-ca ca.pem \
--post-install post.sh \
-o custom.iso fedora-coreos-{stable-version}-live.x86_64.iso
# Same, but create a customized PXE initramfs image
coreos-installer pxe customize \
--dest-device /dev/sda \
--dest-ignition config.ign \
--dest-console ttyS0,115200n8 \
--dest-console tty0 \
--network-keyfile static-ip.nmconnection \
--ignition-ca ca.pem \
--post-install post.sh \
-o custom-initramfs.img fedora-coreos-{stable-version}-live-initramfs.x86_64.img
----

For details on available customizations, see the https://coreos.github.io/coreos-installer/customizing-install/#customize-options[coreos-installer documentation].

=== ISO installation on diverse hardware

Commonly bare metal systems will have a diversity of hardware - some systems may have NVMe drives `/dev/nvme*`, whereas others have `/dev/sd*` for example. You will almost certainly have to template the value of `/dev/sda` above.

A useful approach is to script generating a per-machine `.iso`. If you have a hardware database (whether a text file in git or relational database) then it will work to generate a per-machine `target-dell.ign` and `target-hp.ign` for example, and specify that to `--dest-ignition` alongside the appropriate `--dest-device` to generate `fedora-coreos-install-dell.iso` and `fedora-coreos-install-hp.iso`.

Alternatively, instead of generating per-machine ISOs, you can have a `--pre-install` script run a privileged container which inspects the target system and writes an appropriate https://coreos.github.io/coreos-installer/customizing-install/#config-file-format[installer config] to `/etc/coreos/installer.d`.

=== Installing on iSCSI

To install CoreOS on an iSCSI boot device, follow the same
steps as described above to get the live environment, and add the iSCSI-related kernel arguments.


- Mount the iSCSI target from the live environment:
[source, bash]
----
sudo iscsiadm -m discovery -t st -p 10.0.0.1
sudo iscsiadm -m node -T iqn.2023-10.coreos.target.vm:coreos -l
----

- Append the necessary kargs when running `coreos-installer`:

.Installing to an iSCSI target with iBFT

On a completely diskless machine, the iscsi target and initiator values can be passed through iBFT.
These could be supplied with an iPXE boot script for example:

[source, bash]
----
#!ipxe
set initiator-iqn iqn.2023-11.coreos.diskless:testsetup
sanboot iscsi:10.0.0.1::::iqn.2023-10.coreos.target.vm:coreos
----

[source, bash]
----
sudo coreos-installer install \
/dev/disk/by-path/ip-10.0.0.1\:3260-iscsi-iqn.2023-10.coreos.target.vm\:coreos-lun-0 \
--append-karg rd.iscsi.firmware=1 --append-karg ip=ibft \
--console ttyS0 \
--ignition-url https://example.com/example.ign
----


.Installing to an iSCSI target with manual configuration
[source, bash]
----
sudo coreos-installer install \
/dev/disk/by-path/ip-10.0.0.1\:3260-iscsi-iqn.2023-10.coreos.target.vm\:coreos-lun-0 \
--append-karg rd.iscsi.initiator=iqn.2024-02.com.yourorg.name:lun0 \
--append-karg netroot=iscsi:iqn.2023-10.coreos.target.vm:coreos \
--console ttyS0 \
--ignition-url https://example.com/example.ign
----


All this can also be set using `coreos-installer iso customize` or `coreos-installer pxe customize`. (See "Customizing installation" section above).

For example using iBFT:

[source,bash,subs="attributes"]
----
# Create customized.iso which:
# - Automatically mounts iSCSI target using mount-iscsi.sh
# - Provisions the installed system with config.ign
# - Configures the installed system to use iBFT
coreos-installer iso customize \
--pre-install mount-iscsi.sh \
--dest-device /dev/disk/by-path/ip-10.0.0.1\:3260-iscsi-iqn.2023-10.coreos.target.vm\:coreos-lun-0 \
--dest-ignition config.ign \
--dest-karg-append rd.iscsi.firmware=1 \
--dest-karg-append ip=ibft \
-o custom.iso fedora-coreos-{stable-version}-live.x86_64.iso
----

The  `--pre-install` flag is used to run a script with `iscsiadm` commands, `--dest-device` targets the mounted disk and then `--dest-karg-append` add the necessary kargs.

== Reinstalling Fedora CoreOS

You can use any of the methods described above to reinstall Fedora CoreOS on the same machine via the live environment.

=== Data persistence

The installer does not completely scrub the target disk. Thanks to Ignition's https://coreos.github.io/ignition/operator-notes#filesystem-reuse-semantics[filesystem-reuse semantics], if you apply a config with the exact same partition and filesystem settings as the first install, all previously stored data in additional partitions will still be available. This can be useful for persisting e.g. `/var/lib/containers` or `/var/home` between reinstalls. There are some restrictions, however. See the xref:live-booting.adoc#_using_persistent_state[Using persistent state] section for more information. You will also want to ensure the root filesystem is a fixed number at least 8 GiB as described in xref:storage.adoc[Configuring Storage] so that data partitions are not accidentally overwritten.

=== Destination drive

Fedora CoreOS requires that only a single filesystem labeled `boot` exists. If multiple such filesystems are found on the first boot, provisioning will error out as a fail-safe. If reinstalling Fedora CoreOS on a different disk than the previous installation, make sure to wipe the previous disk using e.g. `wipefs`. For example, if reinstalling to `/dev/sdb`, but `/dev/sda` still contains the previous installation of FCOS, use `wipefs -a /dev/sda` in the installation environment.
= Running Fedora CoreOS directly from RAM
:page-aliases: live-booting-ipxe.adoc

The Fedora CoreOS live environment is a fully functional copy of Fedora CoreOS. It can provision itself via Ignition, execute containers, and so on. The live environment can be used to xref:bare-metal.adoc[install Fedora CoreOS to disk], and can also be used to run Fedora CoreOS directly from RAM.

This guide shows how to boot a transient Fedora CoreOS (FCOS) system via ISO, PXE, or iPXE. For more about the live ISO and PXE images, see the xref:live-reference.adoc[live image reference].

== Prerequisites

Before booting FCOS, you must have an Ignition configuration file. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

If you're booting from PXE or iPXE, you must host the Ignition config on a reachable HTTP(S) or TFTP server. Booting the live PXE image requires at least 2 GiB of RAM with the `coreos.live.rootfs_url` kernel argument, and 4 GiB otherwise.

== Booting via ISO

The live ISO image can be booted from a physical DVD disc, from a USB stick, or from a virtual CD drive via lights-out management (LOM) firmware.

To boot from the ISO image, follow these steps:

- Download the ISO image:
+
[source,bash]
----
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release download -f iso
----

- Use `coreos-installer iso customize` to customize the ISO for your needs. In this example we assume an Ignition config exists in a file `config.ign`. We also add the optional `coreos.liveiso.fromram` kernel argument to the live boot.
+
NOTE: The `coreos.liveiso.fromram` is optional and is used in cases where you want to have no references to the booted media (ISO) once the system is up and running. This enables use cases like removing the media after boot or rewriting the disk the booted media is on, but does require more memory.
+
[source,bash,subs="attributes"]
----
KERNEL_ARG='--live-karg-append=coreos.liveiso.fromram'
IGNITION_ARG='--live-ignition=./config.ign'
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release iso customize $KERNEL_ARG $IGNITION_ARG \
-o customized.iso fedora-coreos-{stable-version}-live.x86_64.iso
----

- Burn the ISO to disk. On Linux and macOS, you can use `dd`. On Windows, you can use https://rufus.ie/[Rufus] in "DD Image" mode.
- Boot it on the target system.

== Booting via PXE

To boot from PXE, follow these steps:

- Download an FCOS PXE kernel, initramfs, and rootfs image:
[source, bash]
----
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release download -f pxe
----

- Follow this example `pxelinux.cfg` for booting the installer images with PXELINUX and running Ignition:

[source,subs="attributes"]
----
DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
KERNEL fedora-coreos-{stable-version}-live-kernel.x86_64
APPEND initrd=fedora-coreos-{stable-version}-live-initramfs.x86_64.img,fedora-coreos-{stable-version}-live-rootfs.x86_64.img ignition.firstboot ignition.platform.id=metal ignition.config.url=http://192.168.1.101/config.ign
IPAPPEND 2
----

== Booting via iPXE

An iPXE-capable machine needs to be provided with a relevant Boot Script to fetch and load FCOS artifacts.

The example below shows how to load those directly from Fedora infrastructure. For performance and reliability reasons it is recommended to mirror them on the local infrastructure, and then tweak the `BASEURL` as needed.

[source,subs="attributes"]
----
#!ipxe

set STREAM stable
set VERSION {stable-version}
set CONFIGURL https://example.com/config.ign

set BASEURL https://builds.coreos.fedoraproject.org/prod/streams/$\{STREAM}/builds/$\{VERSION}/x86_64

kernel $\{BASEURL}/fedora-coreos-$\{VERSION}-live-kernel.x86_64 initrd=main coreos.live.rootfs_url=$\{BASEURL}/fedora-coreos-$\{VERSION}-live-rootfs.x86_64.img ignition.firstboot ignition.platform.id=metal ignition.config.url=$\{CONFIGURL}
initrd --name main $\{BASEURL}/fedora-coreos-$\{VERSION}-live-initramfs.x86_64.img

boot
----

== Using persistent state

By default, the Fedora CoreOS live environment does not store any state on disk, and is reprovisioned from scratch on every boot. However, you may choose to store some persistent state (such as container images) in a filesystem that persists across reboots. For example, here's a Butane config that configures a persistent `/var`:

[source,yaml,subs="attributes"]
----
variant: fcos
version: 1.1.0
storage:
disks:
- device: /dev/sda
wipe_table: true
partitions:
- label: var
filesystems:
- device: /dev/disk/by-partlabel/var
label: var
format: xfs
wipe_filesystem: false
path: /var
with_mount_unit: true
----

When booting a live environment, the Ignition config runs on every boot, so it should avoid wiping LUKS volumes and filesystems that you want to reuse. Instead, configure such structures so that they're created on the first boot and preserved on subsequent boots.

In particular, note the following guidelines:

- Avoid setting `wipe_filesystem` or `wipe_volume` for https://coreos.github.io/ignition/operator-notes/#filesystem-reuse-semantics[filesystems] or LUKS volumes that you intend to reuse. (You can safely set `wipe_table` or `wipe_partition_entry` when reusing a disk, since those don't modify the contents of a https://coreos.github.io/ignition/operator-notes/#partition-reuse-semantics[partition].)
- When reusing LUKS volumes, you must configure a `key_file`. Ignition cannot reuse Clevis-backed LUKS volumes.
- Avoid writing persistent data to RAID volumes, since Ignition cannot reuse those.
- When writing files in persistent storage, set `overwrite` to `true` to avoid Ignition failures when reusing a filesystem that already has the file. Avoid using the `append` directive for persistent files, since the append operation will occur on every boot.

== Update process

Since the traditional FCOS upgrade process requires a disk, the live environment is not able to auto-update in place. For this reason, Zincati is not running there.

For PXE-booted systems, it is recommended that images referenced in the PXE configuration are regularly refreshed. Once infrastructure and configurations are updated, the live PXE instance only needs to be rebooted in order to start running the new FCOS version.

For ISO-booted systems, the ISO image used to boot the live environment should be periodically refreshed, and the instance rebooted to update the running OS.
= Provisioning Fedora CoreOS on Alibaba Cloud (Aliyun)

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Alibaba Cloud. Fedora currently does not publish Fedora CoreOS images within Alibaba Cloud, so you must download an Alibaba Cloud image from Fedora and upload it to one of your Object Storage Service (OSS) buckets.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an Alibaba Cloud account and https://www.alibabacloud.com/help/doc-detail/31884.htm?spm=a2c63.p38356.879954.10.3d1264baRYHfmB#task-njz-hf4-tdb[activated Object Storage Service (OSS)].
The examples below use the https://www.alibabacloud.com/help/product/29991.htm[Alibaba Cloud CLI] and https://stedolan.github.io/jq/[jq] as a command-line JSON processor.

== Downloading an Alibaba Cloud image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.
Once you have picked the relevant stream, download, verify, and decompress the latest Alibaba Cloud image:

[source, bash]
----
STREAM="stable"
coreos-installer download --decompress -s $STREAM -p aliyun -f qcow2.xz
----

Alternatively, you can manually download an Alibaba Cloud image from the https://fedoraproject.org/coreos/download/?stream=stable#cloud_images[download page]. Verify the download, following the instructions on that page, and decompress it.

== Uploading the image to Alibaba Cloud

. Create any bucket that doesn't already exist in your Alibaba Cloud account with a globally unique name or reuse an existing bucket:
+
.Example creating Alibaba Cloud OSS (Object Storage Service) bucket
[source, bash]
----
REGION="ap-southeast-1"
BUCKET_NAME="my-bucket"
BUCKET_URL="oss://${BUCKET_NAME}"
aliyun oss mb "${BUCKET_URL}" --region="${REGION}" --acl=private
----

. Upload an FCOS image:
+
.Example uploading FCOS to an Alibaba Cloud OSS bucket
[source, bash]
----
DOWNLOADED_IMAGE="./image.qcow2"
IMAGE_NAME="my-fcos-image"
IMAGE_BLOB="${IMAGE_NAME}.qcow2"
aliyun oss cp "${DOWNLOADED_IMAGE}" "${BUCKET_URL}/${IMAGE_BLOB}" \
--region="${REGION}" --acl=private
----

. Import uploaded FCOS image:
+
.Example importing FCOS to Alibaba Cloud ECS
[source, bash]
----
TASK_ID=$(aliyun ecs ImportImage \
--region="${REGION}" \
--DiskDeviceMapping.1.OSSBucket="${BUCKET_NAME}" \
--DiskDeviceMapping.1.OSSObject="${IMAGE_BLOB}" \
--ImageName="${IMAGE_NAME}" \
| jq --raw-output .TaskId)
----

. Wait until the image was successfully imported
+
.Example waiting with a timeout equal to one hour
[source, bash]
----
aliyun ecs DescribeTasks --region="${REGION}" --TaskIds="${TASK_ID}" \
--waiter expr='TaskSet.Task[0].TaskStatus' to=Finished timeout=3600
----

. Determine id of imported FCOS image:
+
.Example determining id of the imported FCOS image
[source, bash]
----
IMAGE_ID=$(aliyun ecs DescribeImages --region="${REGION}" --ImageName="${IMAGE_NAME}" \
| jq --raw-output .Images.Image[0].ImageId)
----

. Delete uploaded blob
+
.Example deleting uploaded blob
[source, bash]
----
aliyun oss rm "${BUCKET_URL}/${IMAGE_BLOB}" --region "${REGION}"
----

== Creating a VSwitch

There exists no default VPCs or VSwitches in Alibaba Cloud. Hence, for creating any instances a VSwitch must exist. Pick some existing or create one with the following steps.

. Create a new VPC:
+
.Example creating a new VPC
[source, bash]
----
VPC_CIDR="172.16.0.0/12"
VPC_NAME="fcos-test"
VPC_ID=$(aliyun vpc CreateVpc --region="${REGION}" \
--CidrBlock="${VPC_CIDR}" --VpcName="${VPC_NAME}" \
| jq --raw-output .VpcId)
----

. Pick some availability zone for creating a VSwitch:
+
.Example pick some availability zone
[source,bash]
----
ZONE_ID=$(aliyun ecs DescribeZones --region="${REGION}" \
| jq --raw-output .Zones.Zone[0].ZoneId)
----

. Create a new VSwitch:
+
.Example creating a new VSwitch
[source, bash]
----
VSWITCH_CIDR="172.16.0.0/16"
VSWITCH_NAME="${VPC_NAME}"
VSWITCH_ID=$(aliyun vpc CreateVSwitch \
--region="${REGION}" \
--CidrBlock="${VSWITCH_CIDR}" \
--VpcId="${VPC_ID}" \
--VSwitchName="${VSWITCH_NAME}" \
--ZoneId="${ZONE_ID}" \
| jq --raw-output .VSwitchId)
----

== Launching an ECS instance

. Upload an SSH public key to Alibaba Cloud ECS
+
.Example uploading an SSH public key
[source, bash]
----
KEY_PAIR_NAME="fcos-key"
PUBLIC_KEY_PATH="<Please fill the path to your public key>"
PUBLIC_KEY_BODY=$(cat "${PUBLIC_KEY_PATH}")
aliyun ecs ImportKeyPair --region="${REGION}" \
--KeyPairName="${KEY_PAIR_NAME}" --PublicKeyBody="${PUBLIC_KEY_BODY}"
----

. Creating an ECS instance
+
.Example creating ECS instance
[source, bash]
----
INSTANCE_NAME="my-fcos-vm"
INSTANCE_TYPE="ecs.t6-c1m1.large"
INSTANCE_ID=$(aliyun ecs CreateInstance \
--region="${REGION}" \
--KeyPairName="${KEY_PAIR_NAME}" \
--ImageId="${IMAGE_ID}" \
--InstanceName="${INSTANCE_NAME}" \
--InstanceType="${INSTANCE_TYPE}" \
--InternetChargeType=PayByTraffic \
--InternetMaxBandwidthIn=5 \
--InternetMaxBandwidthOut=5 \
--VSwitchId="${VSWITCH_ID}" \
| jq --raw-output .InstanceId)
----

NOTE: If the command fails with the error message "The specified parameter SystemDisk.Category is not valid.", resolve this issue by adding the parameter `--SystemDisk.Category=cloud_essd` to the command.

. Allocate a public IPv4 address for the previously created instance
+
.Example allocating a public IP address
[source, bash]
----
PUBLIC_IP=$(aliyun ecs AllocatePublicIpAddress \
--region="${REGION}" --InstanceId="${INSTANCE_ID}" \
| jq --raw-output .IpAddress)
----

. Start the instance
+
.Example starting an instance
[source, bash]
----
aliyun ecs StartInstance --region="${REGION}" --InstanceId="${INSTANCE_ID}"
----

. Wait until the instance is running
+
.Example waiting and determining the public IP address
[source, bash]
----
aliyun ecs DescribeInstanceStatus --InstanceId.1="$INSTANCE_ID" --region="${REGION}" \
--waiter expr='InstanceStatuses.InstanceStatus[0].Status' to=Running timeout=600
----

. Connect to the new instance via SSH
+
.Example connecting
[source, bash]
----
ssh core@"${PUBLIC_IP}"
----

You can start a customized instance with your Ignition file by adding the parameter `--UserData=$(cat <Path to your Ignition config> | base64 -w0)` to the `aliyun ecs CreateInstance` command that creates a new instance.
= Provisioning Fedora CoreOS on Amazon Web Services

This guide shows how to provision new Fedora CoreOS (FCOS) instances on the Amazon Web Services (AWS) cloud platform.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an AWS account. The examples below use the https://aws.amazon.com/cli/[aws] command-line tool, which must be separately installed and configured beforehand.

== Launching a VM instance

=== Minimal Example


=== Customized Example

In order to launch a customized FCOS instance, a valid Ignition configuration must be passed as its https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html#instancedata-add-user-data[user data] at creation time. You can use the same command from the xref:#_minimal_example[] but add `--user-data file://path/to/config.ign` argument:

NOTE: The SSH key for the `core` user is supplied via Afterburn in this example as well.

.Launching and customizing a new instance
[source,bash]
----
NAME='instance1'
SSHKEY='my-key'     # the name of your SSH key: `aws ec2 describe-key-pairs`
IMAGE='ami-xxx'     # the AMI ID found on the download page
DISK='20'           # the size of the hard disk
REGION='us-east-1'  # the target region
TYPE='m5.large'     # the instance type
SUBNET='subnet-xxx' # the subnet: `aws ec2 describe-subnets`
SECURITY_GROUPS='sg-xx' # the security group `aws ec2 describe-security-groups`
USERDATA='/path/to/config.ign' # path to your Ignition config
aws ec2 run-instances                     \
--region $REGION                      \
--image-id $IMAGE                     \
--instance-type $TYPE                 \
--key-name $SSHKEY                    \
--subnet-id $SUBNET                   \
--security-group-ids $SECURITY_GROUPS \
--user-data "file://${USERDATA}"      \
--tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=${NAME}}]" \
--block-device-mappings "VirtualName=/dev/xvda,DeviceName=/dev/xvda,Ebs={VolumeSize=${DISK}}"
----

NOTE: By design, cloud-init configuration and startup scripts are not supported on FCOS. Instead, it is recommended to encode any startup logic as systemd service units in the Ignition configuration.

TIP: You can find out the instance's assigned IP by running `aws ec2 describe-instances`

You now should be able to SSH into the instance using the associated IP address.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----

== Remote Ignition configuration

As user-data is limited to 16 KB, you may need to use an external source for your Ignition configuration.
A common solution is to upload the config to a S3 bucket, as the following steps show:

.Create a new s3 bucket
[source, bash]
----
NAME='instance1'
aws s3 mb s3://$NAME-infra
----

.Upload the Ignition file
[source, bash]
----
NAME='instance1'
CONFIG='/path/to/config.ign' # path to your Ignition config
aws s3 cp $CONFIG s3://$NAME-infra/bootstrap.ign
----

You can verify the file has been correctly uploaded:

.List files in the bucket
[source, bash]
----
NAME='instance1'
aws s3 ls s3://$NAME-infra/
----

Then create a minimal Ignition config as follows:

.Retrieving a remote Ignition file from a s3 bucket
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
replace:
source: s3://instance1-infra/bootstrap.ign
----

.Format the remote Ignition file to json format
[source, bash]
----
butane -p config.bu -o config.ign
----

You need to create a role that includes `s3:GetObject` permission, and attach it to the instance profile. See https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html#roles-creatingrole-service-cli[role creation document] for more information.

.Create the instance profile
[source,bash]
----
cat <<EOF >trustpolicyforec2.json
{
"Version": "2012-10-17",
"Statement": {
"Effect": "Allow",
"Principal": {"Service": "ec2.amazonaws.com"},
"Action": "sts:AssumeRole"
}
}
EOF

# Create the role and attach the trust policy that allows EC2 to assume this role.
ROLE_NAME="my-role"
aws iam create-role --role-name ${ROLE_NAME} --assume-role-policy-document file://trustpolicyforec2.json

# Attach the AWS managed policy named AmazonS3ReadOnlyAccess to the role
aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess --role-name ${ROLE_NAME}

# Create the instance profile required by EC2 to contain the role
PROFILE="my-instance-profile"
aws iam create-instance-profile --instance-profile-name ${PROFILE}

# Finally, add the role to the instance profile
aws iam add-role-to-instance-profile --instance-profile-name ${PROFILE} --role-name ${ROLE_NAME}
----

To launch the instance, need to attach the created profile. From the command-line, use `--iam-instance-profile`.

.Launching and customizing a new instance with remote Ignition file from a S3 bucket
[source,bash]
----
NAME='instance1'
SSHKEY='my-key'          # the name of your SSH key: `aws ec2 describe-key-pairs`
IMAGE='ami-xxx'          # the AMI ID found on the download page
DISK='20'                # the size of the hard disk
REGION='us-east-1'       # the target region
TYPE='m5.large'          # the instance type
SUBNET='subnet-xxx'      # the subnet: `aws ec2 describe-subnets`
SECURITY_GROUPS='sg-xxx' # the security group `aws ec2 describe-security-groups`
USERDATA='/path/to/config.ign' # path to your Ignition config
PROFILE='xxx-profile'    # the name of an IAM instance profile `aws iam list-instance-profiles`
aws ec2 run-instances                     \
--region $REGION                      \
--image-id $IMAGE                     \
--instance-type $TYPE                 \
--key-name $SSHKEY                    \
--subnet-id $SUBNET                   \
--security-group-ids $SECURITY_GROUPS \
--user-data "file://${USERDATA}"      \
--iam-instance-profile Name=${PROFILE}     \
--tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=${NAME}}]" \
--block-device-mappings "VirtualName=/dev/xvda,DeviceName=/dev/xvda,Ebs={VolumeSize=${DISK}}"
----

Once the first boot is completed, make sure to delete the configuration as it may contain sensitive data.
See xref:#_configuration_cleanup[].

== Configuration cleanup

If you need to have secrets in your Ignition configuration you should store it into a S3 bucket and have a minimal configuration in user-data.
Once the instance has completed the first boot, clear the S3 bucket as any process or container running on the instance could access it.
See the https://coreos.github.io/ignition/operator-notes/#secrets[Ignition documentation] for more advice on secret management.

.Deleting the Ignition configuration from the s3 bucket
[source,bash]
----
NAME='instance1'
aws s3 rm s3://$NAME-infra/bootstrap.ign
----

Optionally, you can delete the whole bucket:

.Deleting the s3 bucket
[source,bash]
----
NAME='instance1'
aws s3 rb s3://$NAME-infra
----
NOTE: The instance's user data cannot be modified without stopping the instance.
= Provisioning Fedora CoreOS on Azure

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Azure. Fedora currently does not publish Fedora CoreOS images within Azure, so you must download an Azure image from Fedora and upload it to your Azure subscription.

NOTE: FCOS does not support legacy https://learn.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation[Azure Service Manager] virtual machines.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an Azure subscription. The examples below use the https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest[Azure CLI].

== Downloading an Azure image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.
Once you have picked the relevant stream, download, verify, and decompress the latest Azure image:

[source, bash]
----
STREAM="stable"
coreos-installer download --decompress -s $STREAM -p azure -f vhd.xz
----

Alternatively, you can manually download an Azure image from the https://fedoraproject.org/coreos/download/?stream=stable#cloud_images[download page]. Verify the download, following the instructions on that page, and decompress it.

== Uploading the image to Azure

. Create any resources that don't already exist in your Azure account:
+
.Example creating Azure resources
[source, bash]
----
az_region="westus2"
az_resource_group="my-group"
az_storage_account="mystorageacct"
az_container="my-container"
# Create resource group
az group create -l "${az_region}" -n "${az_resource_group}"
# Create storage account for uploading FCOS image
az storage account create -g "${az_resource_group}" -n "${az_storage_account}"
# Retrieve connection string for storage account
cs=$(az storage account show-connection-string -n "${az_storage_account}" -g "${az_resource_group}" | jq -r .connectionString)
# Create storage container for uploading FCOS image
az storage container create --connection-string "${cs}" -n "${az_container}"
----

. Create an FCOS image:
+
.Example creating Azure image
[source, bash]
----
downloaded_image_file="./image.vhd"
az_image_name="my-fcos-image"
az_image_blob="${az_image_name}.vhd"
# Upload image blob
az storage blob upload --connection-string "${cs}" -c "${az_container}" -f "${downloaded_image_file}" -n "${az_image_blob}"
# Create the image
az image create -n "${az_image_name}" -g "${az_resource_group}" --source "https://${az_storage_account}.blob.core.windows.net/${az_container}/${az_image_blob}" --location "${az_region}" --os-type Linux
# Delete the uploaded blob
az storage blob delete --connection-string "$cs" -c "${az_container}" -n "${az_image_blob}"
----

== Launching a VM instance using custom-data

. Launch a VM.
Your Ignition configuration can be passed to the VM as custom data, or you can skip passing custom data if you just want SSH access.
Your SSH public key from `~/.ssh` will automatically be added to the VM.
This provides an easy way to test out FCOS without first creating an Ignition config.
+
.Example launching Azure image
[source, bash]
----
az_vm_name="my-fcos-vm"
ignition_path="./config.ign"
az vm create -n "${az_vm_name}" -g "${az_resource_group}" --image "${az_image_name}" --admin-username core --custom-data "$(cat ${ignition_path})"
----

. You now should be able to SSH into the instance using the associated IP address.
+
.Example connecting
[source, bash]
----
ssh core@<ip address>
----

== Launching a VM instance using custom-data and a private Azure blob

. Define variables for the VM name and Ignition configs:
+
.Define your variables
[source, bash]
----
az_vm_name=my-fcos-vm
ignition_path="./config.ign"
az_blob_ignition_path=./privateConfig.ign
az_blob_ignition_file_name=privateConfig.ign
----
+
.Upload your ign file to Azure blob storage
[source, bash]
----
az storage blob upload --connection-string "${cs}" -c "${az_blob_ignition_file_name}" -f  "${az_blob_ignition_path}" -n "${ignition_file_name}"
----
+
. Create your remote ignition config to reference this new blob.
Read about that here xref:remote-ign.adoc[Using a remote Ignition config]
+
NOTE: The source field should have a value similar to `https://${az_storage_account}.blob.core.windows.net/${az_image_blob}/${az_blob_ignition_file_name}`
+
. Create an identity and give it proper access to your storage account:
+
[source, bash]
----
az identity create --name "${az_vm_name}-identity" --resource-group "${az_resource_group}"
identity_principal_id=$(az identity show --name "${az_vm_name}-identity" --resource-group "${az_resource_group}" --query principalId -o tsv)
identity_id=$(az identity show --name "${az_vm_name}-identity" --resource-group "${az_resource_group}" --query id -o tsv)

az role assignment create --assignee "${identity_principal_id}" --role "Storage Blob Data Contributor" --scope /subscriptions/${subscription_id}/resourceGroups/${az_resource_group}/providers/Microsoft.Storage/storageAccounts/${az_storage_account}
----
+
.Create the VM passing the new identity
[source, bash]
----
az vm create -n "${az_vm_name}" -g "${az_resource_group}" --image "${az_image_name}" --admin-username core --custom-data "$(cat ${ignition_path})" --assign-identity "${identity_id}"
----

== Launching a Confidential VM instance

WARNING: Support for Confidential Computing is a work in progress in Fedora CoreOS.
See the https://github.com/coreos/fedora-coreos-tracker/issues/1719[issue #1719].

NOTE: For an overview about confidential VMs on Azure see https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-vm-overview[confidential VM overview].

To launch a confidential VM, we need to create an image that supports confidential computing in an https://learn.microsoft.com/en-us/azure/virtual-machines/azure-compute-gallery[Azure Compute Gallery].

.Example creating a gallery image that supports confidential computing
[source, bash]
----
# Create an image gallery
gallery_name="mygallery"
az sig create --resource-group "${az_resource_group}" --gallery-name "${gallery_name}"

# Create a gallery image definition
gallery_image_definition="mygallery-def"
az sig image-definition create \
--resource-group "${az_resource_group}" \
--gallery-name "${gallery_name}" \
--gallery-image-definition "${gallery_image_definition}" \
--publisher azure \
--offer example \
--sku standard \
--features SecurityType=ConfidentialVmSupported \
--os-type Linux \
--hyper-v-generation V2

# Get the source VHD URI of OS disk
os_vhd_storage_account=$(az storage account list -g ${az_resource_group} | jq -r .[].id)

# Create a new image version
gallery_image_version="1.0.0"
az sig image-version create \
--resource-group "${az_resource_group}" \
--gallery-name "${gallery_name}" \
--gallery-image-definition "${gallery_image_definition}" \
--gallery-image-version "${gallery_image_version}" \
--os-vhd-storage-account "${os_vhd_storage_account}" \
--os-vhd-uri https://${az_storage_account}.blob.core.windows.net/${az_container}/${az_image_blob}
----

To launch a confidential FCOS instance, you need to specify the confidential compute type and use a https://learn.microsoft.com/en-us/azure/confidential-computing/virtual-machine-options[machine type] that supports confidential computing.

From the command-line, use `--security-type ConfidentialVM` and `--size`.

.Example launching a Confidential VM instance
[source, bash]
----
vm_name="my-fcos-cvm"
ignition_path="./config.ign"

# Specify a size that supports confidential computing (using AMD SEV-SNP for example)
vm_size="Standard_DC2as_v5"

# Get gallery image id
gallery_image_id=$(az sig image-version show --gallery-image-definition "${gallery_image_definition}" --gallery-image-version "${gallery_image_version}" --gallery-name "${gallery_name}" --resource-group $az_resource_group | jq -r .id)

# Create a VM with confidential computing enabled using the gallery image and an ignition config as custom-data
az vm create \
--name "${vm_name}" \
--resource-group $az_resource_group \
--size "${vm_size}" \
--image "${gallery_image_id}" \
--admin-username core \
--generate-ssh-keys \
--custom-data "$(cat ${ignition_path})" \
--enable-vtpm true \
--public-ip-sku Standard \
--security-type ConfidentialVM \
--os-disk-security-encryption-type VMGuestStateOnly \
--enable-secure-boot true
----

NOTE: We pass parameter `--enable-secure-boot true` to enable Secure Boot. Use `false` to disable secure boot.

NOTE: To get the full console log, append the parameter `--boot-diagnostics-storage ${az_storage_account}`.

.Example Confidential VM Boot Verification
[source, bash]
----
ssh core@<ip address>
# Confirm the VM is using `AMD SEV-SNP` confidential type
sudo systemd-detect-virt --cvm
sev-snp

# Confirm the VM is using `Intel TDX` confidential type
sudo systemd-detect-virt --cvm
tdx
----

Note: Another way to confirm is looking at "Group B" and see that it ends with 2 (`HV_ISOLATION_TYPE_SNP`), or ends with 3 (`HV_ISOLATION_TYPE_TDX`).

.Example Confidential VM Boot Verification by checking dmesg log
[source, bash]
----
# `AMD SEV-SNP` confidential type
dmesg | grep "Hyper-V: Isolation Config"
[    0.000000] Hyper-V: Isolation Config: Group A 0x1, Group B 0xba2

# `Intel TDX` confidential type
dmesg | grep "Hyper-V: Isolation Config"
[    0.000000] Hyper-V: Isolation Config: Group A 0x1, Group B 0xbe3
----
= Provisioning Fedora CoreOS on DigitalOcean

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on DigitalOcean. Fedora CoreOS images are currently not published directly on DigitalOcean, so you must download a Fedora CoreOS DigitalOcean image and upload it to your DigitalOcean account as a https://www.digitalocean.com/docs/images/custom-images/[custom image].

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to a DigitalOcean account. The examples below use the https://github.com/digitalocean/doctl[doctl] command-line tool and https://stedolan.github.io/jq/[jq] as a command-line JSON processor.

== Creating a DigitalOcean custom image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.

. Once you have picked the relevant stream, find the corresponding DigitalOcean image on the https://fedoraproject.org/coreos/download/?stream=stable#cloud_images[download page] and copy the URL of the Download link.

. Create the custom image:
+
.Example uploading FCOS to a DigitalOcean custom image
[source, bash]
----
doctl compute image create my-fcos-image --region sfo2 --image-url <download-url>
# Wait for image creation to finish
while ! doctl compute image list-user --output json | jq -c '.[] | select(.name=="my-fcos-image")' | grep available; do sleep 5; done
----

The above command uploads the image and waits until it is ready to be used. This process can take a long time, in our testing we have seen it take up to 15 minutes. Wait time is dependent on upload speeds and platform load.

=== Launching a droplet

. If you don't already have an SSH key uploaded to DigitalOcean, upload one:
+
.Example uploading an SSH key to DigitalOcean
[source, bash]
----
doctl compute ssh-key create my-key --public-key "$(cat ~/.ssh/id_rsa.pub)"
----

. Launch a droplet. Your Ignition configuration can be passed to the VM as its https://docs.digitalocean.com/products/droplets/how-to/provide-user-data/#about-user-data[user data], or you can skip passing user data if you just want SSH access. This provides an easy way to test out FCOS without first creating an Ignition config.
+
When creating a FCOS DigitalOcean droplet, you must specify an SSH key for the droplet, even if you plan to inject SSH keys via Ignition.
+
.Example launching FCOS on DigitalOcean using an Ignition configuration file
[source, bash]
----
image_id=$(doctl compute image list-user | grep my-fcos-image | cut -f1 -d ' ')
key_id=$(doctl compute ssh-key list | grep my-key | cut -f1 -d ' ')
doctl compute droplet create my-fcos-droplet --image "${image_id}" --region sfo2 --size s-2vcpu-2gb --user-data-file <ignition-config-path> --ssh-keys "${key_id}" --wait
----
+
NOTE: While the DigitalOcean documentation mentions `cloud-init` and scripts, FCOS does not support cloud-init or the ability to run scripts from user-data. It accepts only Ignition configuration files.

. You now should be able to SSH into the instance using the associated IP address.
+
.Example connecting
[source, bash]
----
ssh core@<ip address>
----
= Provisioning Fedora CoreOS on Exoscale

This guide shows how to provision new Fedora CoreOS (FCOS) instances on https://exoscale.com[Exoscale] Cloud Hosting.

== Prerequisites

Before provisioning an FCOS machine, it is recommended to have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an Exoscale account. https://portal.exoscale.com/register[Register] if you don't have one.

== Uploading an FCOS image as a custom Template

NOTE: Exoscale offers official FCOS templates, but they are currently out of date. For now, we recommend creating your own template. Track progress on fixing this in https://github.com/coreos/fedora-coreos-tracker/issues/1166[#1166].

Exoscale provides https://community.exoscale.com/documentation/compute/custom-templates[Custom Templates] to be able to upload any cloud image. To create a Custom Template you first need to download and decompress the image.

.Download and decompress the QCOW2 image with https://github.com/coreos/coreos-installer[coreos-installer]
[source, bash]
----
STREAM="stable"
coreos-installer download -d -s $STREAM -p exoscale -f qcow2.xz
----

Alternatively, QCOW2 images can be downloaded from the https://fedoraproject.org/coreos/download/?stream=stable#cloud_images[download page] and manually decompressed.

Next you can https://community.exoscale.com/documentation/compute/custom-templates/#register-a-custom-template[Register a Custom Template]. This can be done from the https://portal.exoscale.com/compute/templates/add[Web Portal] or the https://community.exoscale.com/documentation/tools/exoscale-command-line-interface/[Exoscale CLI]. Either option requires the uncompressed image to be uploaded somewhere public and for the URL and an MD5 checksum to be provided during registration. One option is to use the object storage provided by Exoscale to host the image.

.Upload to Object Storage and create Custom Template
[source, bash]
----
# Set the version and calculate the checksum
FCOS_VERSION='...'
FILE="fedora-coreos-${FCOS_VERSION}-exoscale.x86_64.qcow2"
CHECKSUM=$(md5sum "${FILE}" | cut -d " " -f 1)

# Upload to object storage
BUCKET='newbucket'
exo storage mb "sos://${BUCKET}"
exo storage upload --acl public-read "${FILE}" "sos://${BUCKET}/image-import/"

# Create the template using given URL and CHECKSUM
URL=$(exo storage show "sos://${BUCKET}/image-import/${FILE}" --output-template "{{.URL}}")
TEMPLATE="fedora-coreos-${FCOS_VERSION}"
exo compute instance-template register --boot-mode=uefi $TEMPLATE $URL $CHECKSUM
----

You can then view the template using `exo compute instance-template show --visibility=private $TEMPLATE`.

== Launching a VM instance

You can provision a FCOS instance using the Exoscale https://portal.exoscale.com/compute/instances/add[Web Portal] or via the https://community.exoscale.com/documentation/tools/exoscale-command-line-interface/[Exoscale CLI].

NOTE: You will need to use at least version https://github.com/exoscale/cli/releases/tag/v1.54.0[v1.54.0] of the Exoscale CLI.

WARNING: Do not use the `--cloud-init-compress` argument to the CLI. It causes the Ignition config to be passed compressed to the instance and https://github.com/coreos/fedora-coreos-tracker/issues/1160[Ignition doesn't tolerate that].

.Add your ssh-key
[source, bash]
----
exo compute ssh-key register key-name /path/to/key
----

.Launching a new instance with Exoscale CLI
[source, bash]
----
NAME='worker'
TYPE='standard.medium'
DISK='10' # in GiB
SSHKEY='key-name'
TEMPLATE=$TEMPLATE # template name set above
exo compute instance create "${NAME}" \
--disk-size $DISK \
--ssh-key "${SSHKEY}" \
--template $TEMPLATE \
--template-visibility private \
--cloud-init "path/to/ignition-file.ign"
----

NOTE: If just SSH access is desired and no further customization is required, you don't need to pass any Ignition file and can omit the `--cloud-init` argument.

TIP: You can find out the instance's assigned IP by running `exo compute instance show "${NAME}"`

You now should be able to SSH into the instance using the associated IP address.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----
= Provisioning Fedora CoreOS on Google Cloud Platform

This guide shows how to provision new Fedora CoreOS (FCOS) instances on Google Cloud Platform (GCP).

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to a GCP account. The examples below use the https://cloud.google.com/sdk/gcloud[gcloud] command-line tool, which must be separately installed and configured beforehand.

== Selecting an image family

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.

FCOS images are published under the `fedora-coreos-cloud` project and further organized into image families, tracking the corresponding stream:

* `fedora-coreos-stable`
* `fedora-coreos-testing`
* `fedora-coreos-next`

Before proceeding, check the details of each xref:update-streams.adoc[update stream] and pick the one most suited for your use case.

You can inspect the current state of an image family as follows:

.Inspecting an image family
[source, bash]
----
STREAM='stable'
gcloud compute images describe-from-family \
--project "fedora-coreos-cloud" "fedora-coreos-${STREAM}"
----

== Launching a VM instance

New GCP instances can be directly created and booted from public FCOS images.

If you just want SSH access and no further customization, you don't need to pass any custom instance metadata. Depending on your GCP project configuration, relevant SSH public keys will be automatically added to the VM. This provides an easy way to test out FCOS without first creating an Ignition config.

NOTE: Currently, we don't support logging in using SSH through the GCP web console, using the `gcloud compute ssh` CLI method or OS Login. See https://github.com/coreos/fedora-coreos-tracker/issues/648[fedora-coreos-tracker#648] for more information.

.Launching a new instance
[source, bash]
----
STREAM='stable'
NAME='fcos-node01'
ZONE='us-central1-a'
gcloud compute instances create              \
--image-project "fedora-coreos-cloud"    \
--image-family "fedora-coreos-${STREAM}" \
--zone "${ZONE}" "${NAME}"
----

TIP: You can find out the instance's assigned IP by running `gcloud compute instances list`

You now should be able to SSH into the instance using the associated IP address.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----


In order to launch a customized FCOS instance, a valid Ignition configuration must be passed as metadata under the
`user-data` key at creation time. In the web console, this is available under the Management section.
From the command-line, use `--metadata-from-file`:

.Launching and customizing a new instance
[source, bash]
----
STREAM='stable'
NAME='fcos-node01'
ZONE='us-central1-a'
CONFIG='example.ign'
gcloud compute instances create                \
--image-project "fedora-coreos-cloud"      \
--image-family "fedora-coreos-${STREAM}"   \
--metadata-from-file "user-data=${CONFIG}" \
--zone "${ZONE}" "${NAME}"
----

NOTE: By design, https://cloud.google.com/compute/docs/startupscript[startup scripts] are not supported on FCOS. Instead, it is recommended to encode any startup logic as systemd service units in the Ignition configuration.
Again, note you need to use the `user-data` key for Ignition; it will also not work to paste Ignition into this field in the web console.


== Launch a Confidential VM

WARNING: Support for Confidential Computing is a work in progress in Fedora CoreOS. See the https://github.com/coreos/fedora-coreos-tracker/issues/1719[issue #1719].

NOTE: For an overview about confidential VMs on GCP see https://cloud.google.com/confidential-computing/confidential-vm/docs/confidential-vm-overview[confidential VM overview].

To launch a confidential FCOS instance, you need to specify the confidential compute type and use a https://cloud.google.com/confidential-computing/confidential-vm/docs/supported-configurations[machine type] that supports confidential compute.

From the command-line, use `--confidential-compute-type` and `--machine-type`.

.Launching a confidential instance using confidential type `AMD SEV_SNP`
[source, bash]
----
STREAM='stable'
NAME='fcos-cvm-node01'
ZONE='us-central1-a'
CONFIG='example.ign'
MACHINE_TYPE='n2d-standard-2'
gcloud compute instances create                \
--image-project "fedora-coreos-cloud"      \
--image-family "fedora-coreos-${STREAM}"   \
--metadata-from-file "user-data=${CONFIG}" \
--confidential-compute-type "SEV_SNP"      \
--machine-type "${MACHINE_TYPE}"           \
--maintenance-policy terminate             \
--zone "${ZONE}" "${NAME}"
----

.Launching a confidential instance using confidential type `Intel TDX`
[source, bash]
----
STREAM='stable'
NAME='fcos-cvm-node01'
ZONE='us-central1-a'
CONFIG='example.ign'
MACHINE_TYPE='c3-standard-4'
gcloud compute instances create                \
--image-project "fedora-coreos-cloud"      \
--image-family "fedora-coreos-${STREAM}"   \
--metadata-from-file "user-data=${CONFIG}" \
--confidential-compute-type "TDX"      \
--machine-type "${MACHINE_TYPE}"           \
--maintenance-policy terminate             \
--zone "${ZONE}" "${NAME}"
----

.Example Confidential VM Boot Verification
[source, bash]
----
ssh core@<ip address>
# Confirm the VM is using `AMD SEV-SNP` confidential type
sudo systemd-detect-virt --cvm
sev-snp

# Confirm the VM is using `Intel TDX` confidential type
sudo systemd-detect-virt --cvm
tdx
---
= Provisioning Fedora CoreOS on Hetzner

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Hetzner.
Fedora CoreOS is currently not available as an option in the operating system selection on Hetzner.
Thus you must first download the Fedora CoreOS disk image for Hetzner, then create a snapshot from it in your Hetzner account using the https://github.com/apricote/hcloud-upload-image[hcloud-upload-image] tool, and finally create your servers from this snapshot.

IMPORTANT: Support for Fedora CoreOS on Hetzner is considered emerging, in that it does not yet offer an optimized user experience and relies on tools not officially supported by Hetzner.
See https://github.com/coreos/fedora-coreos-tracker/issues/1324[issue #1324] for more details.

IMPORTANT: The https://github.com/apricote/hcloud-upload-image[hcloud-upload-image] tool is not an official Hetzner Cloud product and Hetzner Cloud does not provide support for it.
Alternatively, you can also use the official https://github.com/hetznercloud/packer-plugin-hcloud[packer-plugin-hcloud] to install the image via `coreos-installer`.

IMPORTANT: In order to create a snapshot, the https://github.com/apricote/hcloud-upload-image[hcloud-upload-image] tool will provision a small server and boot it in rescue mode.
As this server is short lived, the cost should be very limited.
The resulting snapshots are charged per GB per month.
See https://docs.hetzner.com/cloud/servers/backups-snapshots/overview/[Backups/Snapshots] in the Hetzner Cloud documentation.
You may delete this snapshot once the server has been provisioned.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations.
If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS.
If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support] and only configure SSH keys.

You also need to have access to a Hetzner account.
The examples below use the https://github.com/hetznercloud/cli[hcloud] command-line tool, the https://github.com/apricote/hcloud-upload-image[hcloud-upload-image] tool and https://stedolan.github.io/jq/[jq] as a command-line JSON processor.

== Downloading a Hetzner image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream. Once you have picked the relevant stream, download and verify the latest Hetzner image:

[source, bash]
----
ARCH="x86_64"    # or "aarch64"
STREAM="stable"  # or "testing", "next"
coreos-installer download -s "$STREAM" -p hetzner -a "$ARCH" -f raw.xz
----

NOTE: Both x86_64 and aarch64 architectures are supported on Hetzner.

Alternatively, you can manually download an Hetzner image from the https://fedoraproject.org/coreos/download/?stream=stable[download page].
Verify the download, following the instructions on that page.

== Creating a snapshot

. Use the `hcloud-upload-image` to create a snapshot from this image:
+
[source, bash]
----
IMAGE_NAME="fedora-coreos-41.20250213.0-hetzner.x86_64.raw.xz"
export HCLOUD_TOKEN="<your token>"
STREAM="stable"     # or "testing", "next"
HETZNER_ARCH="x86"  # or "arm"

hcloud-upload-image upload \
--architecture "$HETZNER_ARCH" \
--compression xz \
--image-path "$IMAGE_NAME" \
--labels os=fedora-coreos,channel="$STREAM" \
--description "Fedora CoreOS ($STREAM, $ARCH)"
----
+
NOTE: The `hcloud-upload-image` tool uses different names for architectures (`x86_64` -> `x86`, `aarch64` -> `arm`).
+
. Wait for the process to complete and validate that you have a snapshot:
+
[source, bash]
----
hcloud image list --type=snapshot --selector=os=fedora-coreos
----

== Launching a server

. If you don't already have an SSH key uploaded to Hetzner, you may upload one:
+
.Example uploading an SSH key to Hetzner
[source, bash]
----
SSH_PUBKEY="ssh-ed25519 ..."
SSH_KEY_NAME="fedora-coreos-hetzner"
hcloud ssh-key create --name "$SSH_KEY_NAME" --public-key "$SSH_PUBKEY"
----
+
. Launch a server. Your Ignition configuration can be passed to the VM as its user data, or you can skip passing user data if you just want SSH access.
This provides an easy way to test out FCOS without first creating an Ignition config.
+
.Example launching FCOS on Hetzner using an Ignition configuration file and SSH key
[source, bash]
----
IMAGE_ID="$(hcloud image list \
--type=snapshot \
--selector=os=fedora-coreos \
--output json \
| jq -r '.[0].id')"
SSH_KEY_NAME="fedora-coreos-hetzner"  # See: hcloud ssh-key list
DATACENTER="fsn1-dc14"                # See: hcloud datacenter list
TYPE="cx22"                           # See: hcloud server-type list
NAME="fedora-coreos-test"
IGNITION_CONFIG="./config.ign"
hcloud server create \
--name "$NAME" \
--type "$TYPE" \
--datacenter "$DATACENTER" \
--image "$IMAGE_ID" \
--ssh-key "$SSH_KEY_NAME" \
--user-data-from-file "$IGNITION_CONFIG"
----
+
NOTE: While the Hetzner documentation and website mentions `cloud-init` and "cloud config", FCOS does not support cloud-init.
It accepts only Ignition configuration files.

. You now should be able to SSH into the instance using the associated IP address.
+
.Example connecting
[source, bash]
----
ssh core@"$(hcloud server ip "$NAME")"
----
= Provisioning Fedora CoreOS on Microsoft Hyper-V

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Microsoft Hyper-V.

== Prerequisites

You must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

You will also need a small utility from https://github.com/containers/libhvee[libhvee] called `kvpctl`. It attaches your Ignition config to your virtual machine. Precompiled binaries can be found on the project's https://github.com/containers/libhvee/releases[releases page].

=== Downloading the disk image

Hyper-V disk images can be manually downloaded from the https://fedoraproject.org/coreos/download/[download page]. Be sure to decompress the image after downloading.

== Booting a new VM on Microsoft Hyper-V

=== Creating a virtual switch

You must first create a virtual switch so your virtual machine has a network to connect to. To do this, launch Hyper-V Manager and select your server from the list:

image::hyperv-select-server.png[Hyper-V server list]

Then click _Virtual Switch Manager..._ in the _Actions_ panel:

image::hyperv-actions.png[Hyper-V Manager Actions panel]

Follow the prompts under _New virtual network switch_ to create the virtual switch of the type you want:

image::hyperv-switch-create.png[New Virtual Network Switch tab]

=== Creating a virtual machine

In the Actions panel of Hyper-V Manager, click _New_, then _Virtual Machine..._:

image::hyperv-new.png[Hyper-V Manager]

This will launch the _New Virtual Machine Wizard_. When completing the wizard, note the following settings:

. If you select a Generation 2 virtual machine, see <<Configuring Secure Boot>>.
. When prompted to configure networking, select the virtual switch you created earlier.
. When prompted to connect a virtual hard disk, select _Use an existing virtual disk_ and specify the disk image you downloaded earlier:

image::hyperv-disk.png[Hyper-V Virtual Machine Disk Wizard]

=== Setting the Ignition config

Before starting your virtual machine for the first time, you must attach your Ignition config containing the customizations you want to apply to Fedora CoreOS.

On Hyper-V, the Ignition config is presented to the hypervisor in parts. Ignition reads the parts and reassembles them into a single config. You can use the `kvpctl add-ign` subcommand to create these parts and attach them to the virtual machine. The syntax for the command is as follows:

[source, powershell]
----
.\kvpctl.exe <name_of_vm> add-ign <path_to_ign_file>
----

For example:

[source, console]
----
> .\kvpctl.exe myvm add-ign C:\Users\joe\myvm.ign
added key:  ignition.config.0
added key:  ignition.config.1
added key:  ignition.config.2
added key:  ignition.config.3
added key:  ignition.config.4
added key:  ignition.config.5
added key:  ignition.config.6
----

=== Starting the VM

Once you've attached the Ignition config to the virtual machine, right-click the virtual machine in Hyper-V Manager and select _Start_.

=== Viewing the key-value pairs assigned to your virtual machine

You can view the key-value pairs assigned to your machine with the `kvpctl get` subcommand. You can only get key-value pairs when the virtual machine is running.

[source, powershell]
----
.\kvpctl.exe <name_of_vm> get
----

For example:

[source, console]
----
> .\kvpctl.exe myvm get
ignition.config.3 = th":"/etc/containers/registries.conf..."
ignition.config.4 = ,"contents":{"source":"data:,makeste..."
ignition.config.5 = nabled":false,"mask":true,"name":"do..."
ignition.config.6 = service\n\n[Service]\nExecStart=/usr..."
ignition.config.0 = {"ignition":{"config":{"replace":{"v..."
ignition.config.1 = default.target.wants","user":{"name"..."
ignition.config.2 = "user":{"name":"root"},"contents":{"..."
----

=== Configuring Secure Boot

If you configure a Generation 2 virtual machine, Fedora CoreOS will not successfully boot until you change the Secure Boot template to _Microsoft UEFI Certificate Authority_. You can do this in the _Security_ tab of the virtual machine's Settings dialog:

image::hyperv-secure-boot.png[Virtual machine Secure Boot settings]
= Provisioning Fedora CoreOS on IBM Cloud

This guide shows how to provision new Fedora CoreOS (FCOS) instances in IBM Cloud for either the `x86_64` or `s390x` architectures.

NOTE: FCOS does not support https://cloud.ibm.com/docs/cloud-infrastructure?topic=cloud-infrastructure-compare-infrastructure[IBM Cloud Classic Infrastructure].

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an https://cloud.ibm.com/login[IBM Cloud account]. The examples below use the https://cloud.ibm.com/docs/cli?topic=cli-getting-started[`ibmcloud`] command-line tool, which must be separately installed and configured beforehand.
Follow the directions at https://cloud.ibm.com/docs/cli?topic=cli-install-ibmcloud-cli to install the ibmcloud CLI. You'll need both the `cloud-object-storage` and `infrastructure-service` plugins installed. This can be done with:

* `ibmcloud plugin install cloud-object-storage`
* `ibmcloud plugin install infrastructure-service`

After you've logged in using `ibmcloud login` you can set a target region:

.Target a specific region
[source, bash]
----
REGION='us-east' # run `ibmcloud regions` to view options
ibmcloud target -r $REGION
----

.Target a specific resource group
[source, bash]
----
RESOURCE_GROUP='my-resource-group'
ibmcloud resource group-create $RESOURCE_GROUP # Create the resource group if it doesn't exist
ibmcloud target -g $RESOURCE_GROUP
----

There are also several other pieces that need to be in place, like a VPC, SSH keys, networks, permissions, etc. Unfortunately, this guide is not a comprehensive IBM Cloud guide. If you are new to IBM Cloud please familiarize yourself using https://cloud.ibm.com/docs/vpc?topic=vpc-getting-started[the documentation for IBM Cloud VPC networks] first.

=== Creating an Image

The following sets of commands will show you how to download the most recent image for a stream, upload it to cloud storage, and then create the cloud image in IBM Cloud. It is worth noting that Fedora CoreOS comes in three streams, with different update schedules per stream. These steps show the `stable` stream as an example, but can be used for other streams too.


.Fetch the latest image suitable for your target stream (or https://fedoraproject.org/coreos/download/[download and verify] it from the web).
[source, bash]
----
STREAM='stable'
ARCH='x86_64' # or 's390x'
coreos-installer download -s $STREAM -a $ARCH -p ibmcloud -f qcow2.xz --decompress
----

.Create a Service Account for uploading and an Authorization Policy to allow creating images from the uploaded objects.
[source, bash]
----
BUCKET='my-unique-bucket'
ibmcloud resource service-instance-create "${BUCKET}-service-instance" cloud-object-storage standard global

SERVICE_INSTANCE_ID='25df0db0-89a4-4cb8-900f-ed8b44259f80' # from just created service account
ibmcloud iam authorization-policy-create is --source-resource-type image cloud-object-storage Reader --target-service-instance-id $SERVICE_INSTANCE_ID
----

.Upload the fetched image file to IBM Cloud Object Storage.
[source, bash]
----
FCOS_VERSION='...'
FILE="fedora-coreos-${FCOS_VERSION}-ibmcloud.${ARCH}.qcow2"
ibmcloud cos create-bucket --bucket $BUCKET --ibm-service-instance-id $SERVICE_INSTANCE_ID
ibmcloud cos upload --bucket=$BUCKET --key="${FILE}" --file="${FILE}"
----

.Create the image from the storage object.
[source, bash]
----
IMAGE=${FILE:0:-6}     # pull off .qcow2
IMAGE=${IMAGE//[._]/-} # replace . and _ with -
[ $ARCH == 'x86_64' ] && OSNAME='fedora-coreos-stable-amd64'
[ $ARCH == 's390x' ] && OSNAME='red-8-s390x-byol'
ibmcloud is image-create "${IMAGE}" --file "cos://${REGION}/${BUCKET}/${FILE}" --os-name $OSNAME
----

NOTE: For `s390x` we use `--os-name=red-8-s390x-byol` (a RHEL 8 profile) here because there is not currently a `fedora-coreos-stable-s390x` profile to use.

You'll have to wait for the image creation process to finish and go from `pending` to `available` before you can use the image. Monitor with the following command:

.Monitor image creation progress by viewing the images in your account
[source, bash]
----
ibmcloud is images --visibility private --status pending,available
----

== Launching a VM instance

Now that you have an image created in your account you can launch a VM instance. You'll have to specify several pieces of information in the command. Embedded in the example below are tips for how to grab that information before launching an instance.

You'll also need the Ignition config you created earlier. Here it is represented in the example command as `@example.ign`, which indicates a file in the current directory named `example.ign`. The @ is required before the path to the Ignition file.

.Launching a VM instance
[source, bash]
----
INSTANCE_NAME='instance1'
ZONE="${REGION}-1" # view more with `ibmcloud is zones`
PROFILE='bx2-2x8' # view more with `ibmcloud is instance-profiles`
VPC='r014-c9c65cc4-cfd3-44de-ad54-865aac182ea1'    # `ibmcloud is vpcs`
IMAGE='r014-1823b4cf-9c63-499e-8a27-b771be714ad8'  # `ibmcloud is images --visibility private`
SUBNET='0777-bf99cbf4-bc82-4c46-895a-5b7304201182' # `ibmcloud is subnets`
SSHKEY='r014-b44c37d0-5c21-4c2b-aba2-438a5b0a228d' # `ibmcloud is keys`
ibmcloud is instance-create $INSTANCE_NAME $VPC $ZONE $PROFILE $SUBNET \
--allow-ip-spoofing=true --image $IMAGE --keys $SSHKEY --user-data @example.ign
----

TIP: If needed you may have to first create a subnet with a command like `ibmcloud is subnet-create my-subnet $VPC --ipv4-address-count 256 --zone $ZONE`.

WARNING: Make sure you choose an appropriate instance type based on your architecture. For example, you may want to use `bz2-2x8` instead of `bx2-2x8` above if you are targeting `s390x`.

Next, if you'd like to SSH into the instance from outside IBM Cloud, you can assign a public IP to the instance:

.Create and Assign a Floating IP
[source, bash]
----
FIP_NAME='floating-ip-1'
ibmcloud is floating-ip-reserve $FIP_NAME --zone=$ZONE
VNIC=$(ibmcloud is instance $INSTANCE_NAME --output json |
jq --raw-output .primary_network_attachment.virtual_network_interface.id)
ibmcloud is virtual-network-interface-floating-ip-add $VNIC $FIP_NAME
----

And you now should be able to SSH into the instance using the IP address associated with the floating IP.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----
= Provisioning Fedora CoreOS on KubeVirt

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on any KubeVirt-enabled Kubernetes cluster.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

You also need to have access to a Kubernetes environment with https://kubevirt.io/user-guide/operations/installation/[KubeVirt] installed.

== Referencing the KubeVirt Image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.

The image for each stream can directly be referenced from the official registry:

- `quay.io/fedora/fedora-coreos-kubevirt:stable`
- `quay.io/fedora/fedora-coreos-kubevirt:testing`
- `quay.io/fedora/fedora-coreos-kubevirt:next`

== Creating an Ignition config secret

There are various ways to expose userdata to Kubevirt VMs that are covered in the https://kubevirt.io/user-guide/virtual_machines/startup_scripts/#startup-scripts[KubeVirt user guide]. In this example we'll use the Ignition config stored in local file `example.ign` to create a secret named `ignition-payload`. We'll then use this secret when defining our virtual machine in the examples below.

.Creating the secret
[source, bash]
----
kubectl create secret generic ignition-payload --from-file=userdata=example.ign
----

NOTE: If the user prefers, they can use `oc` instead of `kubectl` in the commands throughout this guide.


== Launching a virtual machine

Given the `quay.io/fedora/fedora-coreos-kubevirt` images you can create a VM definition and combine that with the Ignition secret reference to launch a virtual machine.

.Launching a VM instance referencing the secret
[source, bash]
----
STREAM="stable" # or "testing" or "next"
cat <<END > vm.yaml
---
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
name: my-fcos
spec:
runStrategy: Always
template:
spec:
domain:
devices:
disks:
- name: containerdisk
disk:
bus: virtio
- name: cloudinitdisk
disk:
bus: virtio
rng: {}
resources:
requests:
memory: 2048M
volumes:
- name: containerdisk
containerDisk:
image: quay.io/fedora/fedora-coreos-kubevirt:${STREAM}
imagePullPolicy: Always
- name: cloudinitdisk
cloudInitConfigDrive:
secretRef:
name: ignition-payload
END
kubectl create -f vm.yaml
----

Now you should be able to SSH into the instance. If you didn't change the defaults, the
username is `core`.

.Accessing the VM instance using https://kubevirt.io/user-guide/operations/virtctl_client_tool/[`virtctl`] via ssh
[source, bash]
----
virtctl ssh core@my-fcos
----

== Launching a virtual machine with persistent storage

The above example will give you a VM that will lose any changes made to it if it is stopped and started again. You can instruct the cluster to import a containerdisk into a Physical Volume when provisioning in order for the virtual machine to have persistence of the OS disk across stop/start operations.

The positive to this approach is that the machine behaves much more like a traditional virtual machine. The drawback is that the cluster needs to offer Block PV storage and not all clusters may do that.

NOTE: You may have to specify a `storageClassName` parameter in the `spec.dataVolumeTemplates.spec.storage` section of the config if your cluster doesn't offer a default. See the https://kubevirt.io/api-reference/v1.0.0/definitions.html#_v1beta1_storagespec[API docs].

.Launching a VM with persistent storage
[source, bash]
----
STREAM="stable" # or "testing" or "next"
DISK=10
cat <<END > vm.yaml
---
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
name: my-fcos
spec:
runStrategy: Always
dataVolumeTemplates:
- metadata:
name: fcos-os-disk-volume
spec:
source:
registry:
url:
docker://quay.io/fedora/fedora-coreos-kubevirt:${STREAM}
storage:
volumeMode: Block
resources:
requests:
storage: ${DISK}Gi
accessModes:
- ReadWriteOnce
template:
spec:
domain:
devices:
disks:
- name: fcos-os-disk
disk:
bus: virtio
- name: cloudinitdisk
disk:
bus: virtio
rng: {}
resources:
requests:
memory: 2048M
volumes:
- name: fcos-os-disk
dataVolume:
name: fcos-os-disk-volume
- name: cloudinitdisk
cloudInitConfigDrive:
secretRef:
name: ignition-payload
END
kubectl create -f vm.yaml
----

NOTE: The data volume import into the PVC from the container registry may take some time. You can monitor the import by watching the logs of the `importer-fcos-os-disk-volume` pod.

After the machine is up you can connect to it using `virtctl` as shown in the previous example.

== Mirroring the image for use in private registries

If a private registry in air-gapped installations is used, the image can be mirrored to that registry using https://github.com/containers/skopeo[`skopeo`].

.Mirroring a stable stream FCOS image
[source, bash]
----
skopeo copy docker://quay.io/fedora/fedora-coreos-kubevirt:stable docker://myregistry.io/myorg/fedora-coreos-kubevirt:stable
----
= Provisioning Fedora CoreOS on libvirt

This guide shows how to provision new Fedora CoreOS (FCOS) instances on a https://libvirt.org/[libvirt] platform, using the https://www.qemu.org/[QEMU] hypervisor.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

You also need to have access to a host machine with `libvirt`. The examples below use the `virt-install` command-line tool, which must be separately installed beforehand.

TIP: If running on a host with SELinux enabled (use the `sestatus` command to check SELinux status), make sure your OS image and Ignition file are labeled as `svirt_home_t`. You can do this by placing them under `~/.local/share/libvirt/images/` or running `chcon -t svirt_home_t /path/to/file`.

== Launching a VM instance

= Provisioning Fedora CoreOS on AppleHV

This guide shows how to provision new Fedora CoreOS (FCOS) instances on macOS using https://github.com/crc-org/vfkit[vfkit].


== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

You will also need to build or acquire https://github.com/crc-org/vfkit[vfkit].  Prebuilt binaries are also available from its https://github.com/crc-org/vfkit/releases/tag/v0.6.0[releases] page.  Vfkit, like QEMU, has many options which are outside the scope of this provisioning example.  Please consider reading their https://github.com/crc-org/vfkit/tree/main/doc[documentation].

==== Installing via Homebrew

vfkit is available as a https://brew.sh[Homebrew] package:

[source,bash]
----
brew install vfkit
----

==== Installing via MacPorts

vfkit is available as a https://www.macports.org/[MacPorts] package:

[source,bash]
----
sudo port install vfkit
----

== Booting a new VM on macOS

This section shows how to boot a new VM with vfkit. Vfkit is known to work with both Intel and Apple Silicon based Macs.

=== Fetching the AppleHV image

Fetch the latest image suitable for your target stream (or https://fedoraproject.org/coreos/download/[download and verify] it from the web).  Remember to download the appropriate image based on the architecture of your Mac.  Once downloaded, you will also need to decompress the image.

=== Setting up a new VM

Vfkit is not a stateful virtual machine framework. You simply need to run the vfkit binary to start a virtual machine. The following command line will launch a VM with:

* 2 virtual CPUs
* 2 GB of memory
* a network device that will receive an IP address from vfkit
* a GUI console with keyboard and mouse support

.Launching FCOS with Vfkit
[source, bash]
----
IGNITION_CONFIG="/path/to/example.ign"
IMAGE="/path/to/image.raw"

vfkit --cpus 2 --memory 2048 \
--bootloader efi,variable-store=efi-variable-store,create \
--device virtio-blk,path=${IMAGE} \
--device virtio-net,nat \
--ignition ${IGNITION_CONFIG} \
--device virtio-input,keyboard \
--device virtio-input,pointing \
--device virtio-gpu,width=800,height=600 \
--gui
----

Note: The AppleHV hypervisor does not allow you to see early boot and kernel messages.  While you will see a GRUB boot menu, you will not see anything until later in the boot.

=== Exploring the OS

image::vfkit.png[Vfkit GUI]

When FCOS is completed booting, you will see the IP address of the VM displayed in the GUI window.  Vfkit will lease an address in the `192.168.64.0/24` network.  At this point, you can either choose to login or SSH to the VM.  Unlike some other virtualization providers, you can SSH to the virtual machine from the host.

[source, bash]
----
ssh core@192.168.64.5
----
= Provisioning Fedora CoreOS on Nutanix AHV

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Nutanix AHV. Fedora currently does not publish Fedora CoreOS images within Nutanix, so you need to upload a Nutanix image to your Nutanix Prism Central subscription.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

You also need to have access to a Nutanix Prism Central subscription. The examples below use the `curl` command to access Nutanix Prism Central APIs.

== Uploading an image to Nutanix AHV

Fedora CoreOS is designed to be updated automatically, with different schedules per stream. Once you have picked the relevant stream, use the Nutanix Prism Central API to upload the latest image to Nutanix:

[source, bash]
----
STREAM=stable
IMAGE_NAME=<name of image to create>
API_HOST=<Prism Central hostname>
API_USERNAME=<username>
API_PASSWORD=<password>

URL=$(curl https://builds.coreos.fedoraproject.org/streams/${STREAM}.json | \
jq -r .architectures.x86_64.artifacts.nutanix.formats.qcow2.disk.location)
ENCODED_CREDS="$(echo -n "${API_USERNAME}:${API_PASSWORD}" | base64)"

curl -X POST --header "Content-Type: application/json" \
--header "Accept: application/json" \
--header "Authorization: Basic ${ENCODED_CREDS}" \
"https://${API_HOST}:9440/api/nutanix/v3/images" \
-d @- << EOF
{
"spec": {
"name": "${IMAGE_NAME}",
"resources": {
"image_type": "ISO_IMAGE",
"source_uri": "${URL}",
"architecture": "X86_64",
"source_options": {
"allow_insecure_connection": false
}
},
"description": "string"
},
"api_version": "3.1.0",
"metadata": {
"use_categories_mapping": false,
"kind": "image",
"spec_version": 0,
"categories_mapping": {},
"should_force_translate": true,
"entity_version": "string",
"categories": {},
"name": "string"
}
}
EOF
----

== Launching a VM instance

You can provision an FCOS instance using the Nutanix Prism Central web portal or via the Prism Central API with `curl`. Ignition configuration can be passed to the VM as a "cloud-init custom script". For example, to launch a VM using the API:

[source, bash]
----
API_HOST=<Prism Central hostname>
API_USERNAME=<username>
API_PASSWORD=<password>
CLUSTER_REFERENCE_NAME=<name of cluster to use>
CLUSTER_REFERENCE_UUID=<uuid of cluster to use>
SUBNET_REFERENCE_NAME=<name of subnet to use>
SUBNET_REFERENCE_UUID=<uuid of subnet to use>
VM_NAME=<name of VM to create>
IGNITION_CONFIG=config.ign
IMAGE_NAME=<name of image>

ENCODED_CONFIG="$(cat ${IGNITION_CONFIG} | base64 -w 0)"
ENCODED_CREDS="$(echo -n "${API_USERNAME}:${API_PASSWORD}" | base64)"
IMAGE_ID=$(curl -X POST --header "Content-Type: application/json" \
--header "Accept: application/json" \
--header "Authorization: Basic ${ENCODED_CREDS}" \
"https://${API_HOST}:9440/api/nutanix/v3/images/list"
-d '{ "kind": "image","filter": "", "length": 30, "offset": 0}' | \
jq -r '.entities[] | select(.spec.name == "${IMAGE_NAME}") | .metadata.uuid')


curl -X POST --header "Content-Type: application/json" \
--header "Accept: application/json" \
--header "Authorization: Basic ${ENCODED_CREDS}" \
"https://${API_HOST}:9440/api/nutanix/v3/vms" \
-d @- << EOF
{
"spec": {
"name": "${VM_NAME}",
"resources": {
"power_state": "ON",
"num_vcpus_per_socket": 1,
"num_sockets": 1,
"memory_size_mib": 16384,
"disk_list": [
{
"disk_size_mib": 32768,
"device_properties": {
"device_type": "DISK",
"disk_address": {
"device_index": 0,
"adapter_type": "SCSI"
}
},
"data_source_reference": {
"kind": "image",
"uuid": "${IMAGE_ID}"
}
}
],
"nic_list": [
{
"nic_type": "NORMAL_NIC",
"is_connected": true,
"ip_endpoint_list": [
{
"ip_type": "DHCP"
}
],
"subnet_reference": {
"kind": "subnet",
"name": "${SUBNET_REFERENCE_NAME}",
"uuid": "${SUBNET_REFERENCE_UUID}"
}
}
],
"guest_tools": {
"nutanix_guest_tools": {
"state": "ENABLED",
"iso_mount_state": "MOUNTED"
}
},
"guest_customization": {
"cloud_init": {
"user_data": "${ENCODED_CONFIG}"
},
"is_overridable": false
}
},
"cluster_reference": {
"kind": "cluster",
"name": "${CLUSTER_REFERENCE_NAME}",
"uuid": "${CLUSTER_REFERENCE_UUID}"
}
},
"api_version": "3.1.0",
"metadata": {
"kind": "vm"
}
}
EOF
----

You now should be able to SSH into the instance using the associated IP address.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----
= Provisioning Fedora CoreOS on OpenStack

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on an
OpenStack cloud environment, either private, or public (like https://vexxhost.com/[VEXXHOST]).

The steps below were tested against the OpenStack Victoria release.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an OpenStack environment and a functioning
https://docs.openstack.org/python-designateclient/latest/user/shell-v2.html[`openstack` CLI].
Typically, you'll https://docs.openstack.org/python-openstackclient/latest/configuration/index.html[configure the client]
by using a `clouds.yaml` file or via environment variables. If you're starting from scratch, this
environment may need networks, SSH key pairs, security groups, etc.. set up. Please consult the
https://docs.openstack.org/[OpenStack Documentation] to learn more.

== Downloading an OpenStack Image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.
Once you have picked the relevant stream, download, verify, and decompress the latest
OpenStack image:

NOTE: For more information on FCOS stream offerings see xref:update-streams.adoc[Update Streams].

[source, bash]
----
STREAM="stable"
coreos-installer download --decompress -s $STREAM -p openstack -f qcow2.xz
----

Alternatively, you can manually download an OpenStack image from the
https://fedoraproject.org/coreos/download/?stream=stable#cloud_images[download page].
Verify the download, following the instructions on that page, and decompress it.

== Uploading the Image to OpenStack

.Create the FCOS image in OpenStack
[source, bash]
----
FILE=fedora-coreos-XX.XXXXXXXX.X.X-openstack.x86_64.qcow2
IMAGE=${FILE:0:-6} # pull off .qcow2
openstack image create --disk-format=qcow2 --min-disk=10 --min-ram=2 --progress --file="${FILE}" "${IMAGE}"
----

NOTE: If you're uploading an `aarch64` disk image then add `--property architecture=aarch64`.

.Monitor image creation progress by listing the image
[source, bash]
----
openstack image list --name="${IMAGE}"
----

Once the image is listed as `active`, it's ready to be used.

== Launching a VM instance

Now that you have an image created in your account you can launch a VM
instance. Youll have to specify several pieces of information in the
command, such as instance flavor, network information, SSH key, etc...

You'll also need the Ignition config you created earlier. Here it is
represented in the example command as `./example.ign`, which indicates
a file in the current directory named `example.ign`.

.Launching a VM instance
[source, bash]
----
OPENSTACK_NETWORK="private"
OPENSTACK_KEYPAIR="mykeypair" # optional
OPENSTACK_FLAVOR="v1-standard-2"
INSTANCE_NAME="myinstance" # choose a name
openstack server create                \
--key-name="${OPENSTACK_KEYPAIR}" \
--network=$OPENSTACK_NETWORK      \
--flavor=$OPENSTACK_FLAVOR        \
--image="${IMAGE}"                \
--user-data ./example.ign         \
"${INSTANCE_NAME}"
----

NOTE: Specifying `--key-name` is optional if you provide an SSH key in your Ignition config.

TIP: Monitor progress of the instance creation with `openstack server show "${INSTANCE_NAME}"`.
You can also use the `--wait` parameter when calling `openstack server create` to block
until the instance is active.

Next, if the instance's network isn't externally facing and you'd like to SSH
into it from outside the OpenStack environment, you will have to assign a public
IP to the instance:

.Create and Assign a Floating IP
[source, bash]
----
OPENSTACK_NETWORK=public
openstack floating ip create $OPENSTACK_NETWORK

FLOATING_IP=1.1.1.1  # from just created floating IP
openstack server add floating ip "${INSTANCE_NAME}" $FLOATING_IP
----

You now should be able to SSH into the instance using the floating IP address.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----
= Provisioning Fedora CoreOS on Oracle Cloud Infrastructure (OCI)

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Oracle Cloud Infrastructure.
Fedora CoreOS images are currently not published directly on Oracle Cloud Infrastructure.
Thus you must first download the Fedora CoreOS image for Oracle Cloud Infrastructure, then upload it to your account as a https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/importingcustomimagelinux.htm[custom image].

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations.
If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS.
If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to an Oracle Cloud Infrastructure account.
The examples below use the https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm[oci] command-line tool and https://stedolan.github.io/jq/[jq] as a command-line JSON processor.

IMPORTANT: This guide currently only covers Virtual Machine shapes and not Bare Metal ones.
See https://github.com/coreos/fedora-coreos-tracker/issues/414#issuecomment-1795808614[issue #414] for details.

== Downloading an Oracle Cloud Infrastructure image

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.
Once you have picked the relevant stream, download, verify and decompress the latest Oracle Cloud Infrastructure image:

[source, bash]
----
ARCH="x86_64"    # or "aarch64"
STREAM="stable"  # or "testing", "next"
coreos-installer download -s $STREAM -a $ARCH -p oraclecloud -f qcow2.xz --decompress
----

NOTE: Both x86_64 and aarch64 architectures are supported on Oracle Cloud Infrastructure.

Alternatively, you can manually download an Oracle Cloud Infrastructure image from the https://fedoraproject.org/coreos/download/?stream=stable#cloud_images[download page].

== Uploading the image to Oracle Cloud Infrastructure

Identify the ID of your root compartment.

.Listing the compartments in your tenancy.
[source, bash]
----
oci iam compartment list
ROOT_COMPARTMENT_ID=<root_compartment_id>
----

NOTE: The root compartment OCID is the same as your tenancy OCID. You can find
this information in your CLI configuration at `~/.oci/config` or in the
https://cloud.oracle.com/tenancy[Cloud Console].

If you only have one tenant/root compartment you can use the following command to get that information more easily.

.Set root compartment id based on the first compartment in list.
[source, bash]
----
ROOT_COMPARTMENT_ID=$(oci iam compartment list |
jq --raw-output '.data[0]."compartment-id"')
----

.Create a new compartment for testing.
[source, bash]
----
COMPARTMENT_ID=$(oci iam compartment create                \
--name fedora-coreos-test                 \
--compartment-id $ROOT_COMPARTMENT_ID     \
--description "Fedora CoreOS compartment" \
| jq -r '.data.id')
----

.Create a storage bucket for the image upload.
[source, bash]
----
BUCKET_NAME="fedora-coreos"
oci os bucket create --compartment-id $COMPARTMENT_ID --name $BUCKET_NAME
----

.Upload the image to the storage bucket.
[source, bash]
----
FCOS_VERSION='...'
IMAGE_NAME="fedora-coreos-${FCOS_VERSION}-oraclecloud.${ARCH}.qcow2"
FILE_PATH="./${IMAGE_NAME}"
oci os object put --bucket-name $BUCKET_NAME --file $FILE_PATH
----

.View the object in the bucket.
[source, bash]
----
oci os object list -bn $BUCKET_NAME
----

.Import the image as a custom image and remember its ID.
[source, bash]
----
NAMESPACE=$(oci os ns get | jq -r '.data')
IMAGE_ID=$(oci compute image import from-object               \
--compartment-id $COMPARTMENT_ID                   \
--namespace $NAMESPACE                             \
--bucket-name $BUCKET_NAME                         \
--name $IMAGE_NAME                                 \
--display-name "Fedora CoreOS $FCOS_VERSION $ARCH" \
--launch-mode PARAVIRTUALIZED                      \
--source-image-type QCOW2                          \
--operating-system "Linux"                         \
| jq -r '.data.id')
----

Wait until the import is completed.

.Loop until the image becomes available.
[source, bash]
----
while true; do
state=$(oci compute image get --image-id $IMAGE_ID |
jq -r '.data."lifecycle-state"')
echo "$(date): $state"
[ "$state" == "AVAILABLE" ] && break || sleep 30
done
----

The image needs to be configured so the platform knows what it is
capable of. Here we'll pull the default version `1.2` capability set
and configure some additional ones. Note that some of these are
architecture specific, but don't hurt because they also have to be
opted in at runtime anyway.

.Configure additional image capabilities.
[source, bash]
----
GLOBAL_CAP_ID=$(
oci compute global-image-capability-schema list --all | jq -r '.data[0].id')
GLOBAL_CAP_VERSION_NAME=$(
oci compute global-image-capability-schema-version list --all \
--global-image-capability-schema-id $GLOBAL_CAP_ID            \
--display-name 1.2 | jq -r '.data[0].name')

oci compute image-capability-schema create \
--global-image-capability-schema-version-name $GLOBAL_CAP_VERSION_NAME \
--compartment-id $COMPARTMENT_ID --image-id $IMAGE_ID --schema-data '{
"Compute.AMD_SecureEncryptedVirtualization": {
"default-value": true,
"descriptor-type": "boolean",
"source": "IMAGE"
},
"Compute.SecureBoot": {
"default-value": true,
"descriptor-type": "boolean",
"source": "IMAGE"
},
"Storage.Iscsi.MultipathDeviceSupported": {
"default-value": true,
"descriptor-type": "boolean",
"source": "IMAGE"
}
}'
----


Now we can mark the image as compatible with appropriate VM https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm[shapes].

.Remove default compatible shapes.
[source, bash]
----
oci compute image-shape-compatibility-entry \
list --image-id $IMAGE_ID | jq -r '.data[].shape' |
while read shape; do
[[ "$shape" =~ Generic ]] && continue # Can't remove Generic shapes
echo "Removing $shape from $IMAGE_ID"
oci compute image-shape-compatibility-entry remove \
--force --image-id $IMAGE_ID --shape-name "${shape}"
done
----

.Mark as compatible with appropriate VM shapes.
[source, bash]
----
shapes_info=$(oci compute shape list --compartment-id $COMPARTMENT_ID | jq -r '.data[]')

# Limit to VM shapes only
# https://github.com/coreos/fedora-coreos-tracker/issues/414#issuecomment-1795808614
vm_shapes_info=$(jq -r 'select(.shape | select(startswith("VM")))' <<< "$shapes_info")

# Determine x86_64 and aarch64 shapes
amd64_shape_ids=$(jq -r 'select(."processor-description"  |
contains("AMD", "Intel")) |
.shape' <<< "$vm_shapes_info")
arm64_shape_ids=$(jq -r 'select(."processor-description" |
contains("Ampere"))      |
.shape' <<< "$vm_shapes_info")

# Apply the appropriate shapes to the IMAGE
[ "$ARCH" == "x86_64" ] && shape_ids="$amd64_shape_ids"
[ "$ARCH" == "aarch64" ] && shape_ids="$arm64_shape_ids"
for shape in $shape_ids; do
oci compute image-shape-compatibility-entry add \
--image-id $IMAGE_ID --shape-name "${shape}"
done
----

.List all the compatible shapes for an image.
[source, bash]
----
oci compute image-shape-compatibility-entry list --image-id $IMAGE_ID | jq -r '.data[].shape'
----

== Launching an instance

.Create a Virtual Cloud Network.
[source, bash]
----
NETWORK_ID=$(oci network vcn create        \
--compartment-id $COMPARTMENT_ID       \
--display-name "fedora-coreos-network" \
--cidr-blocks '["10.0.0.0/16"]'        \
--dns-label "myfcos"                   \
--wait-for-state AVAILABLE | jq -r '.data.id')
----

.Create a Subnet.
[source, bash]
----
SUBNET_ID=$(oci network subnet create     \
--compartment-id $COMPARTMENT_ID      \
--display-name "fedora-coreos-subnet" \
--cidr-block "10.0.0.0/24"            \
--vcn-id $NETWORK_ID                  \
--dns-label "subnet1"                 \
--wait-for-state AVAILABLE | jq -r '.data.id')
----

.Create an Internet Gateway.
[source, bash]
----
GATEWAY_ID=$(oci network internet-gateway create \
--compartment-id $COMPARTMENT_ID             \
--display-name "fedora-coreos-gateway"       \
--vcn-id $NETWORK_ID                         \
--is-enabled true | jq -r '.data.id')
----

.Add a Rule to the Route Table.
[source, bash]
----
ROUTE_TABLE_ID=$(oci network route-table list \
--compartment-id $COMPARTMENT_ID          \
--vcn-id $NETWORK_ID | jq -r '.data[0].id')

oci network route-table update \
--rt-id $ROUTE_TABLE_ID    \
--force --route-rules      \
'[{"cidrBlock":"0.0.0.0/0","networkEntityId":"'"${GATEWAY_ID}"'"}]'
----

You can now choose an availability domain or just set it to be the
first one in the region.

.Pick an availability domain.
[source, bash]
----
AVAILABILITY_DOMAIN=$(oci iam availability-domain list | jq -r '.data[0].name')
----

NOTE: View all possible domains with `oci iam availability-domain list`.

Now we can launch an instance. If you just want SSH access you can
skip providing an Ignition configuration to the instance.

.Launching an instance without an Ignition configuration.
[source, bash]
----
NAME=fedora-coreos
SHAPE=VM.Standard.E5.Flex # or VM.Standard.A1.Flex for aarch64
SSHKEYS="/path/to/authorized_keys" # path to authorized_keys file

INSTANCE_ID=$(oci compute instance launch               \
--compartment-id $COMPARTMENT_ID                    \
--availability-domain $AVAILABILITY_DOMAIN          \
--display-name $NAME                                \
--image-id $IMAGE_ID                                \
--shape $SHAPE                                      \
--shape-config '{"ocpus": '2', "memoryInGBs": '4'}' \
--subnet-id $SUBNET_ID                              \
--assign-public-ip true                             \
--ssh-authorized-keys-file $SSHKEYS                 \
--wait-for-state TERMINATED                         \
--wait-for-state RUNNING | jq -r '.data.id')
----

NOTE: The free tier eligible `VM.Standard.E2.1.Micro` shape has less
than the recommended amount of memory for Fedora CoreOS to run.
For a free tier eligible instance it is recommended to use the ARM
based `VM.Standard.A1.Flex` shape.

.Launching an instance with customizations and an Ignition configuration.
[source, bash]
----
NAME=fedora-coreos
SHAPE=VM.Standard.E5.Flex      # or VM.Standard.A1.Flex for aarch64
DISK=50                        # size of boot volume in GBs
OCPUS=2                        # number of allocated OCPUs
MEMORY=4                       # size of memory in GBs
INSTANCE_HOSTNAME=mycoreos     # hostname for the instance
USERDATA="/path/to/config.ign" # path to your Ignition config
# that sets a ssh key

INSTANCE_ID=$(oci compute instance launch               \
--compartment-id $COMPARTMENT_ID                    \
--availability-domain $AVAILABILITY_DOMAIN          \
--display-name $NAME                                \
--image-id $IMAGE_ID                                \
--shape $SHAPE                                      \
--shape-config                                      \
'{"ocpus": '${OCPUS}', "memoryInGBs": '${MEMORY}'}' \
--subnet-id $SUBNET_ID                              \
--assign-public-ip true                             \
--hostname-label $INSTANCE_HOSTNAME                 \
--boot-volume-size-in-gbs $DISK                     \
--user-data-file $USERDATA                          \
--wait-for-state TERMINATED                         \
--wait-for-state RUNNING | jq -r '.data.id')
----

NOTE: While the Oracle Cloud Infrastructure documentation mentions `cloud-init`,
Fedora CoreOS does not support cloud-init. It accepts only Ignition configuration
files. When using the https://cloud.oracle.com[Cloud Console], an Ignition
configuration can be placed into "Cloud-init script" field.

NOTE: To enable SecureBoot you can pass additional config via
`--platform-config '{"type": "AMD_VM", "isSecureBootEnabled": true}'` or
`--platform-config '{"type": "INTEL_VM", "isSecureBootEnabled": true}'` or
depending on the processor type of your instance. Enabling Secureboot
isn't currently possible for ARM instances.

.Get the public IP address of the instance.
[source, bash]
----
PUBLIC_IP=$(oci compute instance list-vnics --instance-id $INSTANCE_ID |
jq -r '.data[0]."public-ip"')
echo "The instance public IPV4 is: $PUBLIC_IP"
----

You now should be able to SSH into the instance using the associated IP address.

.SSH into the running instance.
[source, bash]
----
ssh "core@${PUBLIC_IP}"
----
= Provisioning Fedora CoreOS on Proxmox VE

== Prerequisites

Before provisioning an FCOS machine on Proxmox VE, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

You also need to have access to a https://www.proxmox.com[Proxmox Virtual Environment] (VE) cluster or standalone server with administrative privileges to create VMs and storage configurations. All of the commands will be run as the `root` user on the Proxmox VE host.

== Setting up Proxmox VE Storage

Before provisioning Fedora CoreOS VMs, you need to create a dedicated storage location for FCOS images and Ignition configuration snippets.

=== Creating the Storage Directory

First, create a directory structure for Fedora CoreOS resources:

[source,bash]
----
mkdir -p /var/coreos
----

=== Adding Storage to Proxmox VE

Add the new directory as a Proxmox VE storage location that can hold both VM images and configuration snippets:

[source,bash]
----
pvesm add dir coreos --path /var/coreos --content images,snippets
----

This command creates a new storage location named `coreos` that Proxmox VE can use for storing disk images and configuration snippets (including Ignition files). For more details on Proxmox VE storage configuration, see the https://pve.proxmox.com/wiki/Storage[Proxmox VE Storage documentation].

== Downloading Fedora CoreOS Images

Fedora CoreOS provides pre-built images specifically optimized for Proxmox VE. You can download these images using either the `coreos-installer` binary or via a container.

=== Fetching the QCOW2 image

Fetch the latest image suitable for your target stream (or https://fedoraproject.org/coreos/download/[download and verify] it from the web).

[source,bash]
----
STREAM="stable"
# as an installed binary:
coreos-installer download -s $STREAM -p proxmoxve -f qcow2.xz --decompress -C /var/coreos
# or as a container:
podman run --pull=always --rm -v "/var/coreos/images:/data" -w /data \
quay.io/coreos/coreos-installer:release download -s $STREAM -p proxmoxve -f qcow2.xz --decompress
----

Both methods will download the latest Fedora CoreOS image for the specified stream (`stable`, `testing`, or `next`) in QCOW2 format, optimized for Proxmox VE. It will store the image in the `/var/coreos/images` directory.

== Preparing Ignition Configuration

Upload your Ignition configuration file to the snippets directory:

[source,bash]
----
# Upload your ignition to /var/coreos/config.ign
scp /path/to/your/config.ign root@proxmoxve-host:/var/coreos/snippets/config.ign
----

== Setting up a new VM

The following example demonstrates how to create and configure a Fedora CoreOS VM using Proxmox VE command-line tools.

=== Setting Configuration Variables

Define the VM parameters:

[source,bash]
----
VM_ID=101
NAME=fedora-coreos
QCOW=fedora-coreos-{stable-version}-proxmoxve.x86_64.qcow2
IGN=config.ign
STORAGE=local-lvm
CPU=2
MEMORY=2048
DISK_SIZE=90G
----

Adjust these variables according to your requirements:

* `VM_ID`: Unique VM identifier in Proxmox VE
* `NAME`: The name for the VM
* `QCOW`: Filename of the downloaded FCOS image
* `IGN`: Filename of your Ignition configuration
* `STORAGE`: Target storage pool for the VM disk
* `CPU`: Number of CPU cores to allocate
* `MEMORY`: RAM allocation in MB
* `DISK_SIZE`: Additional disk space to allocate

=== Creating and Configuring the VM

Create the VM with the specified configuration:

[source,bash]
----
# Create the initial VM configuration
qm create ${VM_ID} --name ${NAME} --cores ${CPU} --memory ${MEMORY} \
--net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci

# Import the FCOS image as the primary disk
qm set ${VM_ID} --scsi0 "${STORAGE}:0,import-from=/var/coreos/images/${QCOW}"

# Resize the disk to provide additional space
qm resize ${VM_ID} scsi0 +${DISK_SIZE}

# Add cloud-init drive for configuration delivery
qm set ${VM_ID} --ide2 ${STORAGE}:cloudinit

# Set boot order to use the imported disk
qm set ${VM_ID} --boot order=scsi0

# Configure serial console for better compatibility
qm set ${VM_ID} --serial0 socket --vga serial0

# Configure Ignition file delivery via cloud-init
qm set ${VM_ID} --cicustom vendor=coreos:snippets/${IGN}

# Disable automatic upgrades during provisioning
qm set ${VM_ID} --ciupgrade 0
----

== Network Configuration

=== DHCP Configuration

The basic VM creation above uses DHCP for network configuration, which is suitable for most environments where dynamic IP assignment is acceptable.

=== Static IP Configuration

For environments requiring static IP addresses, configure the network settings:

[source,bash]
----
# For static IP address
IP="192.168.1.100"
IP_CIDR="${IP}/24"
GATEWAY="192.168.1.1"
qm set ${VM_ID} --ipconfig0 ip=${IP_CIDR},gw=${GATEWAY}
----

Replace the IP addresses with values appropriate for your network configuration.

== Booting and Accessing the VM

=== Starting the VM

Start the VM and access its console:

[source,bash]
----
# Start and wait for the VM to start
qm start ${VM_ID}
----

=== Exploring the OS

You can log into the VM from the host with the following command:

[source,bash]
----
# Access the VM console from the host
qm terminal ${VM_ID}
----

If you set up an xref:authentication.adoc[SSH key] for the default `core` user, you can SSH into the VM via the IP address:

[source, bash]
----
ssh core@<ip address>
----

== Clean up

For testing purposes, you can easily clean up the VM:

[source,bash]
----
# Stop the VM
qm stop ${VM_ID}

# Remove the VM and its associated storage
qm destroy ${VM_ID}
----

== Links

* Consider using Proxmox VE's built-in https://pve.proxmox.com/wiki/Backup_and_Restore[backup functionality] for important VMs
* Monitor VM performance and resource usage through https://pve.proxmox.com/wiki/Proxmox_VE_Administration_Guide[Proxmox VE monitoring tools]
= Provisioning Fedora CoreOS on QEMU

This guide shows how to provision new Fedora CoreOS (FCOS) instances on a bare https://www.qemu.org/[QEMU] hypervisor.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

You also need to have access to a host machine with https://www.linux-kvm.org/page/Main_Page[KVM] support. The examples below use the `qemu-kvm` command-line tool, which must be separately installed beforehand.

TIP: If running with SELinux enabled, make sure your OS image and Ignition file are labeled as `svirt_home_t`, for example by placing them under `~/.local/share/libvirt/images/`.

== Booting a new VM on QEMU

This section shows how to boot a new VM on QEMU. Based on the platform, the Ignition file is passed to the VM, which sets the `opt/com.coreos/config` key in the QEMU firmware configuration device.

You can use `-snapshot` to make `qemu-kvm` allocate temporary storage for the VM, or `qemu-img create` to first create a layered qcow2.

=== Fetching the QCOW2 image

Fetch the latest image suitable for your target stream (or https://fedoraproject.org/coreos/download/[download and verify] it from the web).

[source, bash]
----
STREAM="stable"
coreos-installer download -s $STREAM -p qemu -f qcow2.xz --decompress -C ~/.local/share/libvirt/images/
----

=== Setting up a new VM

Launch the new VM using `qemu-kvm`.

In snapshot mode, all changes that are performed live after boot are discarded once the machine is powered off.
If you need to persist your changes, it is recommended to set up a dedicated persistent disk first.

.Launching FCOS with QEMU (temporary storage)
[source, bash]
----
IGNITION_CONFIG="/path/to/example.ign"
IMAGE="/path/to/image.qcow2"
# for x86/aarch64:
IGNITION_DEVICE_ARG="-fw_cfg name=opt/com.coreos/config,file=${IGNITION_CONFIG}"

# for s390x/ppc64le:
IGNITION_DEVICE_ARG="-drive file=${IGNITION_CONFIG},if=none,format=raw,readonly=on,id=ignition -device virtio-blk,serial=ignition,drive=ignition"

qemu-kvm -m 2048 -cpu host -nographic -snapshot \
-drive "if=virtio,file=${IMAGE}" ${IGNITION_DEVICE_ARG} \
-nic user,model=virtio,hostfwd=tcp::2222-:22
----

.Launching FCOS with QEMU (persistent storage)
[source, bash]
----
qemu-img create -f qcow2 -F qcow2 -b "${IMAGE}" my-fcos-vm.qcow2
qemu-kvm -m 2048 -cpu host -nographic \
-drive if=virtio,file=my-fcos-vm.qcow2 ${IGNITION_DEVICE_ARG} \
-nic user,model=virtio,hostfwd=tcp::2222-:22
----

=== Exploring the OS

With QEMU usermode networking, the assigned IP address is not reachable from the host.

The examples above use `hostfwd` to selectively forward the SSH port on the guest machine to the local host (port 2222).

If you set up an xref:authentication.adoc[SSH key] for the default `core` user, you can SSH into the VM via the forwarded port:

[source, bash]
----
ssh -p 2222 core@localhost
----
= Provisioning Fedora CoreOS on the Raspberry Pi 4

Fedora CoreOS produces 64-bit ARM (`aarch64`) artifacts. These images can be used as the Operating System for the Raspberry Pi 4 device. Before trying to get FCOS up and running on your Raspberry Pi 4 you'll want to xref:#_updating_eeprom_on_raspberry_pi_4[Update the EEPROM] to the latest version and choose how you want to boot the Raspberry Pi 4. There are two options:

- xref:#_installing_fcos_and_booting_via_u_boot[Installing FCOS and Booting via U-Boot]
- xref:#_installing_fcos_and_booting_via_edk2[Installing FCOS and Booting via EDK2]

U-Boot is the way the Raspberry Pi 4 has traditionally been booted. The https://github.com/pftf/RPi4/[EDK2 Firmware] is an effort to provide a layer that will make RPi4 SystemReady ES (SBBR compliant) similar to most larger 64-bit ARM servers.

== Updating EEPROM on Raspberry Pi 4

The Raspberry Pi 4 uses an EEPROM to boot the system. The EEPROM/Firmware in the past https://github.com/raspberrypi/rpi-eeprom/blob/master/firmware-2711/release-notes.md#2021-10-04---add-support-for-gpt-fat16-and-increase-usb-timeouts---beta[had problems reading a FAT16 EFI partition], which https://github.com/coreos/fedora-coreos-tracker/issues/993[FCOS uses]. For the best experience getting FCOS to run on the RPi4 please update the EEPROM to the latest version. To check if you have the latest version you can go to the https://github.com/raspberrypi/rpi-eeprom/releases[raspberrypi/rpi-eeprom releases page] and make sure the version reported by your Raspberry Pi on boot is from around the same date as the last release.

The https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#bootloader_update_stable[Raspberry Pi Documentation] recommends using the Raspberry Pi Imager for creating a boot disk that can be used to update the EEPROM. If you're on a flavor of Fedora Linux the Raspberry Pi Imager is packaged up and available in the repositories. You can install it with:

[source, bash]
----
dnf install rpi-imager
----

NOTE: You can successfully use `rpi-imager` from inside a https://containertoolbx.org/[Toolbx container].

If not on Fedora Linux you'll need to follow the documentation for obtaining the imager.

Once you have the imager up and running (on Fedora you can run it with `rpi-imager` on the command line), you'll see the imager application load:

image::raspberry-pi-imager.png["Raspberry Pi Imager Application"]

At this point you can follow https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#bootloader_update_stable[the documentation] for how to create the disk and then update your Raspberry Pi 4.

== Installing FCOS and Booting via U-Boot

To run FCOS on a Raspberry Pi 4 via U-Boot the SD card or USB disk needs to be prepared on another system and then the disk moved to the RPi4. After writing FCOS to the disk a few more files will need to be copied in place on the EFI partition of the FCOS disk. Check out the https://www.raspberrypi.com/documentation/computers/configuration.html#boot-folder-contents[Raspberry Pi Documentation] to read more about what these files are for.

In this case we can grab these files from the `uboot-images-armv8`, `bcm2711-firmware`, `bcm283x-firmware`, `bcm283x-overlays` RPMs from the Fedora Linux repositories. First download them and store them in a temporary directory on your system:

[source, bash]
----
RELEASE=43 # The target Fedora Release. Use the same one that current FCOS is based on.
mkdir -p /tmp/RPi4boot/boot/efi/
dnf download --resolve --releasever=$RELEASE --forcearch=aarch64 --destdir=/tmp/RPi4boot/ uboot-images-armv8 bcm283x-firmware bcm283x-overlays
----

Now extract the contents of the RPMs and copy the proper `u-boot.bin` for the RPi4 into place:

WARNING: The following commands to extract the contents of the RPMs, the use of `coreos-installer`, and copying of the files to the ESP partition should be done from outside a Toolbx container. The `root` user in the container maps to a different user ID (UID) than the `root` (`UID=0`) user on the host. Attempting to run some of these commands in the container and some of these commands on the host can result in permission errors or ownership errors and may impact your ability to successfully install Fedora CoreOS.

NOTE: On Arch Linux it's recommended to install `rpm-tools` instead of `rpmextract`, so RPMs are decompressed automatically. If you don't want to or can't do that, you have to pipe the output of `rpm2cpio` through the correct decompressor, e.g. `zstd -d`.

[source, bash]
----
for rpm in /tmp/RPi4boot/*rpm; do rpm2cpio $rpm | cpio -idv -D /tmp/RPi4boot/; done
mv /tmp/RPi4boot/usr/share/uboot/rpi_arm64/u-boot.bin /tmp/RPi4boot/boot/efi/rpi-u-boot.bin
----

Run `coreos-installer` to install to the target disk. There are https://coreos.github.io/coreos-installer/getting-started/[various ways] to run `coreos-installer` and install to a target disk. We won't cover them all here, but this workflow most closely mirrors the xref:bare-metal.adoc#_installing_from_the_container["Installing from the container"] documentation.

[source, bash]
----
FCOSDISK=/dev/sdX
STREAM=stable # or `next` or `testing`
sudo coreos-installer install -a aarch64 -s $STREAM -i config.ign $FCOSDISK
----

NOTE: Make sure you provide an xref:producing-ign.adoc[Ignition config] when you run `coreos-installer`.

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

Now mount the ESP partition and copy the files over:

[source, bash]
----
FCOSEFIPARTITION=$(lsblk $FCOSDISK -J -oLABEL,PATH |
jq -r '.blockdevices[] | select(.label == "EFI-SYSTEM") | .path')
mkdir /tmp/FCOSEFIpart
sudo mount $FCOSEFIPARTITION /tmp/FCOSEFIpart
sudo rsync -avh --ignore-existing --chown 0:0 \
/tmp/RPi4boot/boot/efi/ /tmp/FCOSEFIpart/
sudo umount $FCOSEFIPARTITION
----

Now take the USB/SD card and attach it to the RPi4 and boot.

TIP: It can take some time to boot, especially if the disk is slow. Be patient. You may not see anything on the screen for 20-30 seconds.


== Installing FCOS and Booting via EDK2

There is a EDK2 UEFI firmware implementation for the RPi4 (https://github.com/pftf/RPi4/[pftf/RPi4]) that attempts to make the RPi4 SystemReady ES (SBBR compliant) and allows you to pretend that the RPi4 is similar to any other SystemReady server hardware.

You can write the firmware to a disk (USB or SD card) and then boot/install FCOS as you would on any bare metal server. However, the firmware files need to be on an SD card or USB disk and will take up either the SD card slot or a USB slot. Depending on your needs this may be acceptable or not. Depending on the answer you have a few options:

- Separate Firmware Disk (aka "separate disk mode")
- Combined Fedora CoreOS + EDK2 Firmware Disk (aka "combined disk mode")

These options are covered in the following sections. Regardless of which option you choose you'll want to consider if you need to either xref:#_edk2_firmware_changing_the_3g_limit[Change the 3G RAM limit] or xref:#_edk2_firmware_gpio_via_devicetree[Enable DeviceTree Boot].


=== EDK2: Separate Firmware Disk Mode

In separate disk mode the EDK2 firmware will take up either the SD card slot or a USB slot on your RPi4. Once the firmware disk is attached to the system you'll be able to follow the xref:bare-metal.adoc[bare metal install documentation] and pretend like the RPi4 is any other server hardware.

To create a disk (SD or USB) with the firmware on it you can do something like:

[source, bash]
----
VERSION=v1.50  # use latest one from https://github.com/pftf/RPi4/releases
UEFIDISK=/dev/sdX
sudo mkfs.vfat $UEFIDISK
mkdir /tmp/UEFIdisk
sudo mount $UEFIDISK /tmp/UEFIdisk
pushd /tmp/UEFIdisk
sudo curl -LO https://github.com/pftf/RPi4/releases/download/${VERSION}/RPi4_UEFI_Firmware_${VERSION}.zip
sudo unzip RPi4_UEFI_Firmware_${VERSION}.zip
sudo rm RPi4_UEFI_Firmware_${VERSION}.zip
popd
sudo umount /tmp/UEFIdisk
----

Attaching this disk to your Pi4 you can now install FCOS as you would on any bare metal server.

NOTE: The separate firmware disk will need to stay attached permanently for future boots to work.


=== EDK2: Combined Fedora CoreOS + EDK2 Firmware Disk

In combined disk mode the EDK2 firmware will live inside the EFI partition of Fedora CoreOS, allowing for a single disk to be used for the EDK2 firmware and FCOS.

There are a few ways to achieve this goal:

- Install Directly on RPi4
- Prepare Pi4 Disk on Alternate Machine


==== EDK2: Combined Disk Mode Direct Install

When performing a direct install, meaning you boot (via the EDK2 firmware) into the Fedora CoreOS live environment (ISO or PXE) and run `coreos-installer`, you can mount the EFI partition (2nd partition) of the installed FCOS disk after the installation is complete and copy the EDK2 firmware files over:

[source, bash]
----
UEFIDISK=/dev/mmcblkX or /dev/sdX
FCOSDISK=/dev/sdY
FCOSEFIPARTITION=$(lsblk $FCOSDISK -J -oLABEL,PATH |
jq -r '.blockdevices[] | select(.label == "EFI-SYSTEM") | .path')
mkdir /tmp/mnt{1,2}
sudo mount $UEFIDISK /tmp/mnt1
sudo mount $FCOSEFIPARTITION /tmp/mnt2
sudo rsync -avh /tmp/mnt1/ /tmp/mnt2/
sudo umount /tmp/mnt1 /tmp/mnt2
----

Now you can remove the extra disk from the RPi4 and reboot the machine.

TIP: It can take some time to boot, especially if the disk is slow. Be patient. You may not see anything on the screen for 20-30 seconds.

==== EDK2: Combined Disk Mode Alternate Machine Disk Preparation

When preparing the RPi4 disk from an alternate machine (i.e. creating the disk from your laptop) then you can mount the 2nd partition **after** running `coreos-installer` and pull down the EDK2 firmware files.

First, run `coreos-installer` to install to the target disk:

[source, bash]
----
FCOSDISK=/dev/sdX
STREAM="stable" # or `next` or `testing`
sudo coreos-installer install -a aarch64 -s $STREAM -i config.ign $FCOSDISK
----

Now you can mount the 2nd partition and pull down the EDK2 firmware files:

[source, bash]
----
FCOSEFIPARTITION=$(lsblk $FCOSDISK -J -oLABEL,PATH |
jq -r '.blockdevices[] | select(.label == "EFI-SYSTEM") | .path')
mkdir /tmp/FCOSEFIpart
sudo mount $FCOSEFIPARTITION /tmp/FCOSEFIpart
pushd /tmp/FCOSEFIpart
VERSION=v1.50  # use latest one from https://github.com/pftf/RPi4/releases
sudo curl -LO https://github.com/pftf/RPi4/releases/download/${VERSION}/RPi4_UEFI_Firmware_${VERSION}.zip
sudo unzip RPi4_UEFI_Firmware_${VERSION}.zip
sudo rm RPi4_UEFI_Firmware_${VERSION}.zip
popd
sudo umount /tmp/FCOSEFIpart
----

Now take the USB/SD card and attach it to the RPi4 and boot.

TIP: It can take some time to boot, especially if the disk is slow. Be patient. You may not see anything on the screen for 20-30 seconds.

=== EDK2 Firmware: Changing the 3G limit

If you have a Pi4 with more than 3G of memory you'll most likely want to disable the 3G memory limitation. In the EDK2 firmware menu go to

- `Device Manager` -> `Raspberry Pi Configuration` -> `Advanced Configuration` -> `Limit RAM to 3GB` -> `Disabled`
- `F10` to save -> `Y` to confirm
- `Esc` to top level menu and select `reset` to cycle the system.

=== EDK2 Firmware: GPIO via DeviceTree

With the EDK2 Firmware in ACPI mode (the default) you won't get access to GPIO (i.e. no Pi HATs will work). To get access to GPIO pins you'll need to change the setting to DeviceTree mode in the EDK2 menus.

- `Device Manager` -> `Raspberry Pi Configuration` -> `Advanced Configuration` -> `System Table Selection` -> `DeviceTree`
- `F10` to save -> `Y` to confirm
- `Esc` to top level menu and select `reset` to cycle the system.

After boot you should see entries under `/proc/device-tree/` and also see `/dev/gpiochip1` and `/dev/gpiochip2`:

[source, bash]
----
[core@localhost ~]$ ls /proc/device-tree/ | wc -l
35
[core@localhost ~]$ ls /dev/gpiochip*
/dev/gpiochip0  /dev/gpiochip1
----

TIP: You can interface with GPIO from userspace using `libgpiod` and associated bindings or tools.
= Provisioning Fedora CoreOS on VirtualBox

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on the VirtualBox hypervisor.

== Prerequisites

Before importing an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

=== Downloading the OVA

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.
Once you have picked the relevant stream, you can download the latest OVA:

[source, bash]
----
STREAM="stable"
coreos-installer download -s $STREAM -p virtualbox -f ova
----

Alternatively, OVA images can be manually downloaded from the https://fedoraproject.org/coreos/download/?stream=stable#baremetal[download page].

== Booting a new VM on VirtualBox

You can set up a VirtualBox virtual machine through the GUI or via the https://www.virtualbox.org/manual/UserManual.html#vboxmanage[`VBoxManage` CLI]. This guide will use the CLI for setting up the VM.

=== Importing the OVA

To import the OVA, use `VBoxManage import`:

[source, bash, subs="attributes"]
----
VM_NAME=my-instance
VBoxManage import --vsys 0 --vmname "$VM_NAME" fedora-coreos-{stable-version}-virtualbox.x86_64.ova
----

=== Setting the Ignition config

Ignition reads its configuration from the `/Ignition/Config` https://docs.oracle.com/en/virtualization/virtualbox/6.0/user/guestadd-guestprops.html[guest property] of the virtual machine. At present, guest properties can only be set from the host command line, and not via the GUI. To set the Ignition config for a VM:

[source, bash]
----
IGN_PATH="/path/to/config.ign"
VM_NAME=my-instance
VBoxManage guestproperty set "$VM_NAME" /Ignition/Config "$(cat $IGN_PATH)"
----

==== Ignition config size limitations

The length of the `/Ignition/Config` guestinfo property is constrained by the maximum length of a command line on your host operating system. The OS-specific limits are approximately:

[cols="1,1"]
|===
|OS
|Limit

|Linux
|128 KiB
|macOS
|256 KiB
|Windows shells
|8 KiB
|===

If your Ignition config is larger than this limit, you can host the config on an HTTPS server and refer to it from a small _pointer config_, as follows:

. Upload your Ignition config to an HTTPS server.
. xref:remote-ign.adoc[Create a Butane pointer config] that specifies the URL of your full Ignition config:
+
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
replace:
source: https://example.com/config.ign
----
. Use xref:producing-ign.adoc[Butane] to convert the Butane config to an Ignition config.
. Set the `/Ignition/Config` guest property to the contents of the pointer Ignition config, following the instructions in <<_setting_the_ignition_config>>.

=== Configuring networking

By default, the VM will use https://www.virtualbox.org/manual/UserManual.html#networkingmodes[NAT networking]. This will share the IP address of your host. Alternatively, if you want the VM to use a different IP address than your host, you can set the VM's network adapter to "Bridged networking".

==== NAT networking

By default, NAT networking does not allow inbound connections to the VM. To allow inbound SSH connections, you can forward connections to e.g. port 2222 on the host to the SSH server in the VM:

[source, bash]
----
VM_NAME=my-instance
VBoxManage modifyvm "$VM_NAME" --natpf1 "guestssh,tcp,,2222,,22"
----

After booting the VM, you can SSH to the VM from your host:

[source, bash]
----
ssh core@localhost -p 2222
----

==== Bridged networking

If you want the VM to use a different IP address than your host, you can set the VM's network adapter to "Bridged networking".

. Determine the network adapter that should be bridged to the VM. To get the name of your host's default network adapter, you can run:
+
[source, bash]
----
ip route ls default | grep -Po '(?<= dev )(\S+)'
----

. Modify the VM's network adapter settings:
+
[source, bash]
----
VM_NAME=my-instance
ADAPTER=adapter-name
VBoxManage modifyvm "$VM_NAME" --nic1 bridged --bridgeadapter1 "$ADAPTER"
----

=== Starting the VM

You can now boot the VM you have configured:

[source, bash]
----
VM_NAME=my-instance
VBoxManage startvm "$VM_NAME"
----

== Troubleshooting first-boot problems

You may encounter problems with your Ignition config that require access to the console log messages which appear during the first boot. To obtain a copy of the console log you can attach a https://www.virtualbox.org/manual/UserManual.html#serialports[serial device] to the VM before booting.

To attach a serial device to a powered-off VM:

[source, bash]
----
VM_NAME=my-instance
VM_LOG=$(realpath .)/$VM_NAME.log
VBoxManage modifyvm "$VM_NAME" --uart1 0x3F8 4
VBoxManage modifyvm "$VM_NAME" --uartmode1 file "$VM_LOG"
----

When you power on the VM, console output will be logged to the file you specified.
= Provisioning Fedora CoreOS on VMware

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on the VMware hypervisor.

NOTE: Fedora CoreOS supports VMware ESXi &ge; 7.0, VMware Workstation &ge; 16, and VMware Fusion &ge; 12. It may be possible to
xref:provisioning-vmware.adoc#_modifying_ovf_metadata[modify the metadata of the OVF] to run in older VMware products, but compatibility and supportability cannot be guaranteed.

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

You also need to have access to a working VMware infrastructure, supporting VMs with at least hardware version 13.
The examples below use the https://github.com/vmware/govmomi/blob/v0.29.0/govc/README.md[govc] command-line tool for remote vSphere provisioning and the https://code.vmware.com/web/tool/4.4.0/ovf[ovftool] for local Workstation or Fusion provisioning.

=== Downloading the OVA

Fedora CoreOS is designed to be updated automatically, with different schedules per stream.
Once you have picked the relevant stream, you can download the latest OVA:

[source, bash]
----
STREAM="stable"
coreos-installer download -s $STREAM -p vmware -f ova
----

Alternatively, OVA images can be manually downloaded from the https://fedoraproject.org/coreos/download/?stream=stable#baremetal[download page].

=== Encoding Ignition configuration

For the `vmware` provider, Ignition requires two "guestinfo" fields to be present when the VM is first booted:

* `guestinfo.ignition.config.data.encoding`: the encoding of the Ignition configuration.
* `guestinfo.ignition.config.data`: the content of the Ignition configuration, encoded according to the format above.

For maximum compatibility, it is recommended to use `base64` encoding and to prepare the Ignition configuration as such:

[source, bash]
----
CONFIG_ENCODING='base64'
CONFIG_ENCODED=$(base64 -w0 example.ign)
----

An alternative to plain `base64` encoding is `gzip+base64` as described in the https://coreos.github.io/ignition/supported-platforms/[Ignition supported platforms]. This is especially useful when submitting the Ignition config via `govc` as an _inline_ argument. In that case the encoded config is limited to slightly under 128 KiB on Linux, 256 KiB on macOS, and 32 KiB on Windows (8 KiB if using `cmd.exe` or PowerShell). If your config is larger than that limit, you may be able to submit it inline after compressing it with `gzip`.

[source, bash]
----
CONFIG_ENCODING='gzip+base64'
CONFIG_ENCODED=$(gzip -9c example.ign | base64 -w0 -)
----

If your generated Ignition configuration is still too large, you will encounter an `Argument list too long` error or similar. The solution to that problem depends on whether you are working with vSphere or Workstation/Fusion.

For vSphere, instead of inlining the configuration file within your shell, `govc` allows you to specify a path to a local file with the https://github.com/vmware/govmomi/blob/main/govc/USAGE.md#vmchange[`vm.change`]-command and will handle reading and writing it internally, circumventing any shell limitations.

[source, bash]
----
CONFIG_ENCODING="gzip+base64"
CONFIG_FILE="example.ign"
CONFIG_FILE_ENCODED="${CONFIG_FILE}.gz.b64"

gzip -9c "${CONFIG_FILE}" | base64 -w0 - > "${CONFIG_FILE_ENCODED}"

govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data.encoding=${CONFIG_ENCODING}"
govc vm.change -vm "${VM_NAME}" -f "guestinfo.ignition.config.data=${CONFIG_FILE_ENCODED}" # using `-f` with a file path instead of `-e`
----

NOTE: Using `gzip` for this solution is optional and primarily used for consistent examples.

In the case of Workstation/Fusion, or as a last resort in general, there is the option to use a configuration file. Instead of setting an environment variable containing your Ignition configuration, create an `ovftool` compatible configuration file in the directory you are invoking from like so:

[source, bash]
----
echo "extraConfig:guestinfo.ignition.config.data=$(base64 -w0 example.ign)" > ovftool.cfg
----

== Booting a new VM on Workstation or Fusion

This section shows how to use Workstation and Fusion facilities to configure and run VMs from the command-line. Some steps can potentially be performed via the graphical UI too.

=== Importing the OVA

The downloaded OVA has to be imported into the Workstation or Fusion library locally. At the same time the Ignition has to be provided for it to be applied to the VM.

[source, bash]
----
VM_NAME='fcos-node01'
FCOS_OVA='./ova-templates/fedora-coreos-31.20200210.3.0-vmware.x86_64.ova'
LIBRARY="$HOME/Virtual Machines.localized"
ovftool \
--powerOffTarget \
--name="${VM_NAME}" \
--allowExtraConfig \
--extraConfig:guestinfo.ignition.config.data.encoding="${CONFIG_ENCODING}" \
--extraConfig:guestinfo.ignition.config.data="${CONFIG_ENCODED}" \
"${FCOS_OVA}" "${LIBRARY}"
----

Afterwards you can refresh the list of VMs in the Workstation or Fusion UI and the new `fcos-node01` VM should appear ready for booting. Its hardware configuration can be further customized at this point, and then powered-up.

If you set up an xref:authentication.adoc[SSH key] for the default `core` user, you can SSH into the VM and explore the OS:

[source, bash]
----
ssh core@<ip address>
----

== Booting a new VM on vSphere

This section shows how to use vSphere facilities to configure and run VMs from the command-line. Similar steps can be performed via the graphical UI too.

TIP: While the examples below use `govc session.login` to authenticate, you can also use environment variables to provide credentials. Check the https://github.com/vmware/govmomi/tree/main/govc#usage[official documentation] for details.

=== Setting up a new VM

You can now deploy a new VM, starting from the OVA and the encoded Ignition configuration:

[source, bash]
----
FCOS_OVA='./ova-templates/fedora-coreos-31.20200210.3.0-vmware.x86_64.ova'
VM_NAME='fcos-node01'
govc session.login -u 'user:password@host'
govc import.ova -name ${VM_NAME} ${FCOS_OVA}
govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data.encoding=${CONFIG_ENCODING}"
govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data=${CONFIG_ENCODED}"
----

A new `fcos-node01` VM is now available for booting. Its hardware configuration can be further customized at this point, and then powered-up:

[source, bash]
----
govc vm.info -e "${VM_NAME}"
govc vm.power -on "${VM_NAME}"
----

If you set up an xref:authentication.adoc[SSH key] for the default `core` user, you can SSH into the VM and explore the OS:

[source, bash]
----
ssh core@<ip address>
----

=== Using the OVA from the vSphere library

In case you want to spawn multiple, different VMs based on the same base image you can import it into the vSphere library for easy reuse:

[source, bash]
----
FCOS_OVA='./ova-templates/fedora-coreos-31.20200210.3.0-vmware.x86_64.ova'
LIBRARY='fcos-images'
TEMPLATE_NAME='fcos-31.20200210.3.0'
govc session.login -u 'user:password@host'
govc library.create "${LIBRARY}"
govc library.import -n "${TEMPLATE_NAME}" "${LIBRARY}" "${FCOS_OVA}"
----

Creating a new instance can now be done using the `govc library.deploy` command:

[source, bash]
----
VM_NAME='fcos-node01'
govc library.deploy "${LIBRARY}/${TEMPLATE_NAME}" "${VM_NAME}"
govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data.encoding=${CONFIG_ENCODING}"
govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data=${CONFIG_ENCODED}"
----

Note: If the vCenter has multiple datacenters and datastores, you must specify them explicitly:
[source, bash]
----
# Get resource pool using `$ govc find / -type ResourcePool`
RESOURCE_POOL="/Datacenter6.5/host/Cluster6.5/Resources"
DATASTORE="datastore-129"
govc library.deploy -pool=${RESOURCE_POOL} -ds=${DATASTORE} "${LIBRARY}/${TEMPLATE_NAME}" "${VM_NAME}"
----

=== First-boot networking and Ignition

Ignition supports referencing remote content in configuration and fetching it at provisioning time.
For this reason, on first-boot FCOS instances try to perform network autoconfiguration via DHCP.

If your VMware setup employs static network configuration instead, you can override this automatic DHCP setup with your own custom configuration.
Custom networking command-line `ip=` parameter can be configured via guestinfo properties as shown below, before booting a VM for the first time.

The provisioning flow follows the usual steps, plus an additional `guestinfo.afterburn.initrd.network-kargs` entry.

NOTE: if you are using a provisioning method other than `govc`, make sure that the guestinfo attribute is provisioned in the VM's Advanced Configuration Parameters (also known as `ExtraConfig`). Some management tools may default to a vApp Property instead, which does not work in this scenario.

[source, bash]
----
VM_NAME='fcos-node02'
IFACE='ens192'
IPCFG="ip=192.0.2.42::192.0.2.1:255.255.255.0:${VM_NAME}:${IFACE}:off"

govc library.deploy "${LIBRARY}/${TEMPLATE_NAME}" "${VM_NAME}"
govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data.encoding=${CONFIG_ENCODING}"
govc vm.change -vm "${VM_NAME}" -e "guestinfo.ignition.config.data=${CONFIG_ENCODED}"
govc vm.change -vm "${VM_NAME}" -e "guestinfo.afterburn.initrd.network-kargs=${IPCFG}"
govc vm.info -e "${VM_NAME}"
govc vm.power -on "${VM_NAME}"
----

The full syntax of the `ip=` parameter is documented in https://www.man7.org/linux/man-pages/man7/dracut.cmdline.7.html[Dracut manpages].

For further information on first-boot networking, see https://coreos.github.io/afterburn/usage/initrd-network-cmdline/[Afterburn documentation].

== Troubleshooting First-boot Problems

You may encounter problems with your Ignition configuration that require access to the system log which appears during first-boot. To make a copy of the system log you can attach a serial device to the VM before booting. vSphere as well as Workstation and Fusion allow this and will save the output to a file of your choice.

To attach a serial device, modify the hardware settings of the powered off VM and add a `Serial Port`. Select the destination and name of the file to be created. Afterwards power on the VM. When encountering an error, check the file you initially specified - it should contain a copy of the system log.

The serial device can also be added to the VM via `govc` as described in the https://github.com/vmware/govmomi/blob/master/govc/USAGE.md#deviceserialconnect[official usage documentation]:

[source, bash]
----
VM_NAME='fcos-node01'

govc device.serial.add -vm "${VM_NAME}"
govc device.serial.connect -vm "${VM_NAME}" "[datastore] ${VM_NAME}/console.log"
----

== Modifying OVF metadata

NOTE: While we provide these instructions for modifying the OVF metadata, we cannot
guarantee that any modifications to the OVF metadata will result in a usable
guest VM.

Fedora CoreOS is intended to run on
https://lifecycle.vmware.com/[generally supported] releases of VMware ESXi,
VMware Workstation, and VMware Fusion. Accordingly, the Fedora CoreOS VMware
OVA image specifies a
https://kb.vmware.com/s/article/1003746[virtual hardware version]
that may not be compatible with older, unsupported VMware products.
However, you can modify the image's OVF metadata to specify an older
virtual hardware version.

The VMware OVA is a tarball that contains the files `disk.vmdk` and
`coreos.ovf`. In order to edit the metadata used by FCOS as a guest VM, you
should untar the OVA artifact, edit the OVF file, then create a new OVA file.

The example commands below change the OVF hardware version from the preconfigured value to hardware version `13`.

NOTE: The defaults in the OVF are subject to change.

[source,bash,subs="attributes"]
----
tar -xvf fedora-coreos-{stable-version}-vmware.x86_64.ova
sed -iE 's/vmx-[0-9]*/vmx-13/' coreos.ovf
tar -H posix -cvf fedora-coreos-{stable-version}-vmware-vmx-13.x86_64.ova coreos.ovf disk.vmdk
----
= Provisioning Fedora CoreOS on Vultr

This guide shows how to provision new Fedora CoreOS (FCOS) nodes on Vultr. Vultr publishes FCOS images, but they are out of date, so **we do not recommend using the standard Vultr images**. Instead, a current FCOS release can be uploaded as a https://www.vultr.com/docs/requirements-for-uploading-an-os-iso-to-vultr[custom image].

== Prerequisites

Before provisioning an FCOS machine, you must have an Ignition configuration file containing your customizations. If you do not have one, see xref:producing-ign.adoc[Producing an Ignition File].

NOTE: Fedora CoreOS has a default `core` user that can be used to explore the OS. If you want to use it, finalize its xref:authentication.adoc[configuration] by providing e.g. an SSH key.

If you do not want to use Ignition to get started, you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support].

You also need to have access to a Vultr account. The examples below use the https://github.com/vultr/vultr-cli[vultr-cli] and https://s3tools.org/s3cmd[s3cmd] command-line tools. Both of these tools are available in Fedora and can be installed via `sudo dnf install vultr-cli s3cmd`.

== Using a custom snapshot

Vultr supports creating custom snapshots from public raw images.

These steps show how to download an FCOS image and upload it to an existing storage bucket, in order to create a snapshot from that.

See https://www.vultr.com/docs/vultr-object-storage[Vultr documentation] for further details on how to create a bucket and configure `s3cmd` to use it.

=== Creating a snapshot

Fedora CoreOS comes in three streams, with different update schedules per stream. These steps show the `stable` stream as an example, but can be used for other streams too.

. Fetch the latest image suitable for your target stream (or https://fedoraproject.org/coreos/download/[download and verify] it from the web).
+
[source, bash]
----
STREAM='stable'
coreos-installer download -s $STREAM -p vultr -f raw.xz --decompress
----

. https://www.vultr.com/docs/how-to-use-s3cmd-with-vultr-object-storage[Use s3cmd to upload] the raw image to your bucket, and note its public URL.
+
[source, bash]
----
BUCKET='my-bucket'
FCOS_VERSION='...'
s3cmd put --acl-public "fedora-coreos-${FCOS_VERSION}-vultr.x86_64.raw" "s3://${BUCKET}/"
----

. Create the snapshot from your object URL, and note its ID.
+
[source, bash]
----
IMAGE_URL='https://...'
VULTR_API_KEY='<token>'
vultr-cli snapshot create-url -u "${IMAGE_URL}"
----

NOTE: You'll need to wait for the snapshot to finish processing before using it. Monitor with `*vultr-cli snapshot list*`.

=== Launching an instance from a snapshot

You can now create an FCOS Vultr instance using the snapshot ID above.

This example creates a 2 vCPU, 4GB RAM instance named `instance1` in the New Jersey region. Use `vultr-cli regions list` and `vultr-cli plans list` for other options.

[source, bash]
----
NAME='instance1'
SNAPSHOT_ID='...'
REGION='ewr'
PLAN='vc2-2c-4gb'
vultr-cli instance create --region "${REGION}" --plan "${PLAN}" \
--snapshot "${SNAPSHOT_ID}" --label "${NAME}" --host "${NAME}" \
--userdata "$(cat example.ign)"
----

NOTE: While the Vultr documentation mentions `cloud-init` and scripts, FCOS does not support `cloud-init` or the ability to run scripts from user-data. It accepts only Ignition configuration files.

TIP: You can find out the instance's assigned IP by running `vultr-cli instance list`.

You now should be able to SSH into the instance using the associated IP address.

.Example connecting
[source, bash]
----
ssh core@<ip address>
----
* System Configuration
= Producing an Ignition Config

== Ignition overview

Ignition is a provisioning utility that reads a configuration file (in JSON format) and provisions a Fedora CoreOS system based on that configuration. Configurable components include storage and filesystems, systemd units, and users.

Ignition runs only once during the first boot of the system (while in the initramfs). Because Ignition runs so early in the boot process, it can re-partition disks, format filesystems, create users, and write files before the userspace begins to boot. As a result, systemd services are already written to disk when systemd starts, speeding the time to boot.

== Configuration process

Ignition configurations are formatted as JSON, which is quick and easy for a machine to read. However, these files are not easy for humans to read or write. The solution is a two-step configuration process that is friendly for both humans and machines:

. Produce a YAML-formatted Butane config.
. Run Butane to convert the YAML file into a JSON Ignition config.

During the transpilation process, Butane verifies the syntax of the YAML file, which can catch errors before you use it to launch the FCOS system.

Once you have an Ignition (`.ign`) file, you can use it to boot an FCOS system in a VM or install it on bare metal.

TIP: Try to plan your configuration with the full set of customization details before provisioning a Fedora CoreOS instance. But don't worry if you forgot something as you can fix the configuration and re-deploy the instance from a fresh image.

== Getting Butane

You can run Butane as a container with docker or podman or download it as a standalone binary.

NOTE: Unless otherwise noted, new releases of Butane are backwards compatible with old releases.

=== Via a container with `podman` or `docker`

You can get Butane from a container hosted on https://quay.io/[quay.io]:

[source,bash]
----
podman pull quay.io/coreos/butane:release
----

NOTE: The `release` tag tracks the most recent release, and the `latest` tag tracks the Git development branch.

Run Butane either by using standard input and standard output or by using files:

.Example running Butane using standard input and standard output
[source,bash]
----
podman run --interactive --rm quay.io/coreos/butane:release \
--pretty --strict < your_config.bu > transpiled_config.ign
----

.Example running Butane using a file as input and standard output
[source,bash]
----
podman run --interactive --rm --security-opt label=disable \
--volume "${PWD}:/pwd" --workdir /pwd quay.io/coreos/butane:release \
--pretty --strict your_config.bu > transpiled_config.ign
----

To make it simpler to type, you may also add the following alias to your shell configuration:

[source,bash]
----
alias butane='podman run --rm --interactive         \
--security-opt label=disable          \
--volume "${PWD}:/pwd" --workdir /pwd \
quay.io/coreos/butane:release'
----

NOTE: Those examples use podman, but you can use docker in a similar manner.

=== Installing via distribution packages

==== Installing on Fedora

Butane is available as a Fedora package:

[source,bash]
----
sudo dnf install -y butane
----

==== Installing via Homebrew

Butane is available as a https://brew.sh[Homebrew] package:

[source,bash]
----
brew install butane
----

==== Installing via MacPorts

Butane is available as a https://www.macports.org/[MacPorts] package:

[source,bash]
----
sudo port install butane
----

==== Installing via Scoop

Butane is available as a https://scoop.sh[Scoop] package via the https://github.com/lukesampson/scoop-extras[extras]:

[source,powershell]
----
scoop bucket add extras
scoop install butane
----

==== Installing via Windows Package Manager Client (winget)

Butane is available as a https://docs.microsoft.com/en-us/windows/package-manager/[winget] package:

[source,powershell]
----
winget install --id Fedora.CoreOS.butane
----

=== Standalone binary

==== Linux
To use the Butane binary on Linux, follow these steps:

. If you have not already done so, download the https://fedoraproject.org/security/[Fedora signing keys] and import them:
+
[source,bash]
----
curl https://fedoraproject.org/fedora.gpg | gpg --import
----
. Download the latest version of Butane and the detached signature from the https://github.com/coreos/butane/releases[releases page].
. Verify it with gpg:
+
[source,bash]
----
gpg --verify butane-x86_64-unknown-linux-gnu.asc
----

==== macOS
To use the Butane binary on macOS, follow these steps:

. If you have not already done so, download the https://fedoraproject.org/fedora.gpg[Fedora signing keys] and import them:
+
[source,bash]
----
curl https://fedoraproject.org/fedora.gpg | gpg --import
----
. Download the latest version of Butane and the detached signature from the https://github.com/coreos/butane/releases[releases page].
. Verify it with gpg:
+
[source,bash]
----
gpg --verify butane-x86_64-apple-darwin.asc
----

==== Windows
To use the Butane binary on Windows, follow these steps:

. If you have not already done so, download the https://fedoraproject.org/fedora.gpg[Fedora signing keys] and import them:
+
[source,powershell]
----
Invoke-RestMethod -Uri https://fedoraproject.org/fedora.gpg | gpg --import
----
. Download the latest version of Butane and the detached signature from the https://github.com/coreos/butane/releases[releases page].
. Verify it with gpg:
+
[source,powershell]
----
gpg --verify butane-x86_64-pc-windows-gnu.exe.asc
----

== Example

Create a basic Ignition config that modifies the default Fedora CoreOS user `core` to allow this user to log in with an SSH key.

The overall steps are as follows:

. Write the Butane config in the YAML format.
. Use Butane to convert the Butane config into an Ignition (JSON) config.
. Boot a fresh Fedora CoreOS image with the resulting Ignition configuration.

=== Prerequisite

This example uses a pair of SSH public and private keys. If you don't already have it, you can https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/system_administrators_guide/index#sec-SSH[generate an SSH key pair].

The SSH public key will be provisioned to the Fedora CoreOS machine (via Ignition). The SSH private key needs to be available to your user on the local workstation, in order to remotely authenticate yourself over SSH.

=== Writing the Butane config

. Copy the following example into a text editor:
+
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAA...
----
+
. Replace the above line starting with `ssh-rsa` with the contents of your SSH public key file.
+
. Save the file with the name `example.bu`.

TIP: YAML files must have consistent indentation. Although Butane checks for syntax errors, ensure that the indentation matches the above example. Overall, the Butane configs must conform to Butane's https://coreos.github.io/butane/specs/[configuration specification] format.

TIP: If you use VS Code with the https://github.com/redhat-developer/vscode-yaml[Red Hat Yaml extension] you can associate `*.bu` files to `yaml` in the `files.associations` setting and get help/auto completion.

TIP: You may also set a login password for the console (password authentication over SSH is disabled). See https://docs.fedoraproject.org/en-US/fedora-coreos/authentication/#_using_password_authentication[Using Password Authentication].

=== Using Butane

. Run Butane on the Butane config:
+
[source,bash]
----
butane --pretty --strict example.bu > example.ign
----
+
. Use the `example.ign` file to xref:getting-started.adoc[boot Fedora CoreOS].

NOTE: If using Butane on Windows, `> example.ign` will create an UTF-16 encoded Ignition file. This can prevent Fedora CoreOS from booting properly. Use `--output example.ign` instead.
** link:https://coreos.github.io/butane/specs/[Butane Specification]
= Using a remote Ignition config

With Ignition, you are not limited to the configuration provided locally to a system and can retrieve other Ignition configs from a remote source. Those configs will then either replace or be merged into the existing config.

The complete list of supported protocols and related options for remote Ignition files is described in the https://coreos.github.io/ignition/specs/[Ignition specification].

The following examples show how to retrieve an Ignition file from a remote source. They are both set to replace the current configuration with a remote Ignition file.

.Retrieving a remote Ignition file via HTTPS
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
replace:
source: https://example.com/sample.ign
----

.Retrieving a remote Ignition file via HTTPS with a custom certificate authority
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
replace:
source: https://example.com/sample.ign
security:
tls:
certificate_authorities:
- source: https://example.com/source1
----

NOTE: The certificate authorities listed here are not automatically added to the host filesystem. They are solely used by Ignition itself when fetching over `https`. If you'd like to also install them on the host filesystem, include them as usual under the `storage.files` array.

In some cases, if you need to merge a local configuration and one or several remote ones, you can use `merge` rather than `replace` in a Butane config.

.Retrieving a remote Ignition file via HTTPS and merging it with the current config
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
merge:
- source: https://example.com/sample.ign
passwd:
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHn2eh...
----

Retrieving remote Ignition files via plain HTTP is also possible as shown below.

WARNING: Retrieving a remote Ignition config via HTTP exposes the contents of the config to anyone monitoring network traffic. When using HTTP, it is advisable to use the verification option to ensure the contents haven't been tampered with.

.Retrieving a remote Ignition file via HTTP
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
replace:
source: http://example.com/sample.ign
verification:
hash: sha512-e2bb19fdbc3604f511b13d66f4c675f011a63dd967b97e2fe4f5d50bf6cb224e902182221ba0f9dd87c0bb4abcbd2ab428eb7965aa7f177eb5630e7a1793e2e6
----

If you need to retrieve a remote Ignition file but have no direct access to the remote host, you can specify a proxy for plain HTTP and/or HTTPS. You can also specify hosts that should be excluded from proxying.

.Retrieving a remote Ignition file via a proxy
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
ignition:
config:
merge:
- source: https://example.com/sample.ign
- source: https://example.org/example.ign
proxy:
https_proxy: https://example.net
no_proxy:
- example.org
----
= Configuring Storage

Fedora CoreOS ships with a simple default storage layout: the root partition is the last one and expands to take the full size of the disk. Apart from the boot partition, all data is stored on the root partition. See the xref:#_disk_layout[Disk layout section] for more details.

Below, we provide examples of various ways you can customize this.

[NOTE]
====
*Fedora CoreOS requires the root filesystem to be at least 8 GiB.* For practical reasons, disk images for some platforms ship with a smaller root filesystem, which by default automatically expands to fill all available disk space. If you add additional partitions after the root filesystem, you must make sure to explicitly resize the root partition as shown below so that it is at least 8 GiB.

Currently, if the root filesystem is smaller than 8 GiB, a warning is emitted on login. Starting from June 2021, if the root filesystem is smaller than 8 GiB and is followed by another partition, Fedora CoreOS will refuse to boot. For more details, see https://github.com/coreos/fedora-coreos-tracker/issues/586[this bug].
====

== Referencing block devices from Ignition

Many of the examples below will reference a block device, such as `/dev/vda`. The name of the available block devices depends on the underlying infrastructure (bare metal vs cloud), and often the specific instance type. For example in AWS, some instance types have NVMe drives (`/dev/nvme*`), others use `/dev/xvda*`.

If your disk configuration is simple and uses the same disk the OS was booted from then the `/dev/disk/by-id/coreos-boot-disk` link can be used to conveniently refer to that device. This link is only available during provisioning for the purpose of making it easy to refer to the same disk the OS was booted from.

If you need to access other disks, you can boot a single machine with an Ignition configuration with only SSH access, and then inspect the block devices via e.g. the `lsblk` command.

For physical hardware, it is recommended to reference devices via the `/dev/disk/by-id/` or `/dev/disk/by-path` links.

== Setting up separate /var mounts

Here's an example Butane config to set up `/var` on a separate partition on the same primary disk:

.Adding a /var partition to the primary disk
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
- # The link to the block device the OS was booted from.
device: /dev/disk/by-id/coreos-boot-disk
# We do not want to wipe the partition table since this is the primary
# device.
wipe_table: false
partitions:
- number: 4
label: root
# Allocate at least 8 GiB to the rootfs. See NOTE above about this.
size_mib: 8192
resize: true
- size_mib: 0
# We assign a descriptive label to the partition. This is important
# for referring to it in a device-agnostic way in other parts of the
# configuration.
label: var
filesystems:
- path: /var
device: /dev/disk/by-partlabel/var
# We can select the filesystem we'd like.
format: ext4
# Ask Butane to generate a mount unit for us so that this filesystem
# gets mounted in the real root.
with_mount_unit: true
----

You can of course mount only a subset of `/var` into a separate partition. For example, to mount `/var/lib/containers`:

.Adding a /var/lib/containers partition to the primary disk
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
- device: /dev/disk/by-id/coreos-boot-disk
wipe_table: false
partitions:
- number: 4
label: root
# Allocate at least 8 GiB to the rootfs. See NOTE above about this.
size_mib: 8192
resize: true
- size_mib: 0
label: containers
filesystems:
- path: /var/lib/containers
device: /dev/disk/by-partlabel/containers
format: xfs
with_mount_unit: true
----

Alternatively, you can also mount storage from a separate disk. For example, here we mount `/var/log` from a partition on `/dev/vdb`:

.Adding /var/log from a secondary disk
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
- device: /dev/vdb
wipe_table: false
partitions:
- size_mib: 0
start_mib: 0
label: log
filesystems:
- path: /var/log
device: /dev/disk/by-partlabel/log
format: xfs
with_mount_unit: true
----


.Defining a disk with multiple partitions
In this example, we wipe the disk and create two new partitions.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
-
# Mandatory. We use the World-Wide Number ID of the drive to ensure
# uniqueness.
device: /dev/disk/by-id/wwn-0x50014e2eb507fcdf
# This ensures that the partition table is re-created, along with all
# the partitions.
wipe_table: true
partitions:
# The first partition (slot number 1) is 32 GiB and starts at the
# beginning of the device. Its type_guid identifies it as a Linux
# swap partition.
- label: part1
number: 1
size_mib: 32768
start_mib: 0
type_guid: 0657fd6d-a4ab-43c4-84e5-0933c84b4f4f
# The second partition (implicit slot number 2) will be placed after
# partition 1 and will occupy the rest of the available space.
# Since type_guid is not specified, it will be a Linux native
# partition.
- label: part2
----

== Reconfiguring the root filesystem

It is possible to reconfigure the root filesystem itself. You can use the path `/dev/disk/by-label/root` to refer to the original root partition. You must ensure that the new filesystem also has a label of `root`.

NOTE: You must have at least 4 GiB of RAM for root reprovisioning to work.

Here's an example of moving from xfs to ext4, but reusing the same partition on the primary disk:

.Changing the root filesystem to ext4
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
filesystems:
- device: /dev/disk/by-partlabel/root
wipe_filesystem: true
format: ext4
label: root
----

Similarly to the previous section, you can also move the root filesystem entirely. Here, we're moving root to a RAID0 device:

.Moving the root filesystem to RAID0
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
raid:
- name: myroot
level: raid0
devices:
- /dev/disk/by-id/virtio-disk1
- /dev/disk/by-id/virtio-disk2
filesystems:
- device: /dev/md/myroot
format: xfs
wipe_filesystem: true
label: root
----

NOTE: You don't need the `path` or `with_mount_unit` keys; FCOS knows that the root partition is special and will figure out how to find it and mount it.

If you want to replicate the boot disk across multiple drives for resiliency to drive failure, you need to mirror all the default partitions (root, boot, EFI System Partition, and bootloader code). There is special Butane config syntax for this:

.Mirroring the boot disk onto two drives
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
boot_device:
mirror:
devices:
- /dev/sda
- /dev/sdb
----

== Defining a filesystem

This example demonstrates the process of creating the filesystem by defining and labeling the partitions, combining them into a RAID array, and formatting that array as ext4.

.Defining a filesystem on a RAID storage device
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
# This defines two partitions, each on its own disk. The disks are
# identified by their WWN.
- device: /dev/disk/by-id/wwn-0x50014ee261e524e4
wipe_table: true
partitions:
-
# Each partition gets a human-readable label.
label: "raid.1.1"
# Each partition is placed at the beginning of the disk and is 64 GiB
# long.
number: 1
size_mib: 65536
start_mib: 0
- device: /dev/disk/by-id/wwn-0x50014ee0b8442cd3
wipe_table: true
partitions:
- label: "raid.1.2"
number: 1
size_mib: 65536
start_mib: 0
# We use the previously defined partitions as devices in a RAID1 md array.
raid:
- name: publicdata
level: raid1
devices:
- /dev/disk/by-partlabel/raid.1.1
- /dev/disk/by-partlabel/raid.1.2
# The resulting md array is used to create an EXT4 filesystem.
filesystems:
- path: /var/publicdata
device: /dev/md/publicdata
format: ext4
label: PUB
with_mount_unit: true
----

== Encrypted storage (LUKS)

Here is an example to configure a LUKS device at `/var/lib/data`.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
luks:
- name: data
device: /dev/vdb
filesystems:
- path: /var/lib/data
device: /dev/mapper/data
format: xfs
label: DATA
with_mount_unit: true
----

The root filesystem can also be moved to LUKS. In that case, the LUKS device must be pinned by https://coreos.github.io/ignition/operator-notes/#clevis-based-devices[Clevis]. There are two primary pin types available: TPM2 and Tang (or a combination of those using Shamir Secret Sharing).

CAUTION: TPM2 pinning just binds encryption to the physical machine in use. Make sure to understand its threat model before choosing between TPM2 and Tang pinning. For more information, see https://github.com/latchset/clevis/blob/master/src/pins/tpm2/clevis-encrypt-tpm2.1.adoc#threat-model[this section] of the Clevis TPM2 pin documentation.

NOTE: You must have at least 4 GiB of RAM for root reprovisioning to work.

There is simplified Butane config syntax for configuring root filesystem encryption and pinning. Here is an example of using it to create a TPM2-pinned encrypted root filesystem:

.Encrypting the root filesystem with a TPM2 Clevis pin
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
boot_device:
luks:
tpm2: true
----

This is equivalent to the following expanded config:

.Encrypting the root filesystem with a TPM2 Clevis pin without using boot_device
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
luks:
- name: root
label: luks-root
device: /dev/disk/by-partlabel/root
clevis:
tpm2: true
wipe_volume: true
filesystems:
- device: /dev/mapper/root
format: xfs
wipe_filesystem: true
label: root
----

The expanded config doesn't include the `path` or `with_mount_unit` keys; FCOS knows that the root partition is special and will figure out how to find it and mount it.

This next example binds the root filesystem encryption to PCR 7 which corresponds to the https://uapi-group.org/specifications/specs/linux_tpm_pcr_registry/[UEFI Boot Component] used to track the Secure Boot certificate from memory. Therefore, updates to the UEFI firmware/certificates should not affect the value stored in PCR 7.

NOTE: Binding for PCR 8 (UEFI Boot Component used to track commands and kernel command line) is not supported as the kernel command line changes with every OS update.

.Encrypting the root filesystem with a TPM2 Clevis pin bound to PCR 7
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
luks:
- name: root
label: luks-root
device: /dev/disk/by-partlabel/root
clevis:
custom:
needs_network: false
pin: tpm2
config: '{"pcr_bank":"sha1","pcr_ids":"7"}'
wipe_volume: true
filesystems:
- device: /dev/mapper/root
format: xfs
wipe_filesystem: true
label: root
----

More documentation for the `config` fields can be found in the `clevis` man pages: `man clevis-encrypt-tpm2`

The following `clevis` command can be used to confirm that the root file system encryption is bound to PCR 7.

[source,shell]
----
$ sudo clevis luks list -d /dev/disk/by-partlabel/root
1: tpm2 '{"hash":"sha256","key":"ecc","pcr_bank":"sha1","pcr_ids":"7"}'
----

Here is an example of the simplified config syntax with Tang:

.Encrypting the root filesystem with a Tang Clevis pin
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
boot_device:
luks:
tang:
- url: http://192.168.122.1:80
thumbprint: bV8aajlyN6sYqQ41lGqD4zlhe0E
----

The system will contact the Tang server on boot.

NOTE: For more information about setting up a Tang server, see https://github.com/latchset/tang[the upstream documentation].

You can configure both Tang and TPM2 pinning (including multiple Tang servers for redundancy). By default, only the TPM2 device or a single Tang server is needed to unlock the root filesystem. This can be changed using the `threshold` key:

.Encrypting the root filesystem with both TPM2 and Tang pins
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
boot_device:
luks:
tang:
- url: http://192.168.122.1:80
thumbprint: bV8aajlyN6sYqQ41lGqD4zlhe0E
tpm2: true
# this will allow rootfs unlocking only if both TPM2 and Tang pins are
# accessible and valid
threshold: 2
----

== Sizing the root partition

If you use Ignition to reconfigure or move the root partition, that partition is not automatically grown on first boot (see related discussions in https://github.com/coreos/fedora-coreos-tracker/issues/570[this issue]). In the case of moving the root partition to a new disk (or multiple disks), you should set the desired partition size using the `size_mib` field. If reconfiguring the root filesystem in place, as in the LUKS example above, you can resize the existing partition using the `resize` field:

.Resizing the root partition to its maximum size
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
- device: /dev/disk/by-id/coreos-boot-disk
partitions:
- label: root
number: 4
# 0 means to use all available space
size_mib: 0
resize: true
luks:
- name: root
device: /dev/disk/by-partlabel/root
clevis:
tpm2: true
wipe_volume: true
filesystems:
- device: /dev/mapper/root
format: xfs
wipe_filesystem: true
label: root
----

== Adding swap

This example creates a swap partition spanning all of the `sdb` device, creates a swap area on it, and creates a systemd swap unit so the swap area is enabled on boot.

.Configuring a swap partition on a second disk
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
disks:
- device: /dev/sdb
wipe_table: true
partitions:
- number: 1
label: swap
filesystems:
- device: /dev/disk/by-partlabel/swap
format: swap
wipe_filesystem: true
with_mount_unit: true
----

== Adding network storage

Fedora CoreOS systems can be configured to mount network filesystems such as NFS and CIFS. This is best achieved by using Ignition to create systemd units. Filesystems can be mounted on boot by creating a standard mount unit. Alternatively, a filesystem can be mounted when users access the mountpoint by creating an additional automount unit. Below are examples of each for an NFS filesystem.

=== Configuring NFS mounts

.Creating a systemd unit to mount an NFS filesystem on boot.
NOTE: The `.mount` file must be named based on the path (e.g. `/var/mnt/data` = `var-mnt-data.mount`)
[source,yaml,subs="attributes"]
----
variant: fcos
version: 1.3.0
systemd:
units:
- name: var-mnt-data.mount
enabled: true
contents: |
[Unit]
Description=Mount data directory

[Mount]
What=example.org:/data
Where=/var/mnt/data
Type=nfs4

[Install]
WantedBy=multi-user.target
----

.Creating a systemd unit to mount an NFS filesystem when users access the mount point (automount)
[source,yaml,subs="attributes"]
----
variant: fcos
version: 1.3.0
systemd:
units:
- name: var-mnt-data.mount
contents: |
[Unit]
Description=Mount data directory

[Mount]
What=example.org:/data
Where=/var/mnt/data
Type=nfs4

[Install]
WantedBy=multi-user.target

- name: var-mnt-data.automount
enabled: true
contents: |
[Unit]
Description=Automount data directory

[Automount]
TimeoutIdleSec=20min
Where=/var/mnt/data

[Install]
WantedBy=multi-user.target
----

== Advanced examples

This example configures a mirrored boot disk with a TPM2-encrypted root filesystem, overrides the sizes of the automatically-generated root partition replicas, and adds an encrypted mirrored `/var` partition which consumes the remainder of the disks.

.Encrypted mirrored boot disk with separate /var
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
boot_device:
luks:
tpm2: true
mirror:
devices:
- /dev/sda
- /dev/sdb
storage:
disks:
- device: /dev/sda
partitions:
# Override size of root partition on first disk, via the label
# generated for boot_device.mirror
- label: root-1
size_mib: 10240
# Add a new partition filling the remainder of the disk
- label: var-1
- device: /dev/sdb
partitions:
# Similarly for second disk
- label: root-2
size_mib: 10240
- label: var-2
raid:
- name: md-var
level: raid1
devices:
- /dev/disk/by-partlabel/var-1
- /dev/disk/by-partlabel/var-2
luks:
- name: var
device: /dev/md/md-var
# No key material is specified, so a random key will be generated
# and stored in the root filesystem
filesystems:
- device: /dev/mapper/var
path: /var
label: var
format: xfs
wipe_filesystem: true
with_mount_unit: true
----

== Disk Layout

All Fedora CoreOS systems start with the same disk image which varies slightly between architectures based on what is needed for bootloading. On first boot the root filesystem is expanded to fill the rest of the disk. The disk image can be customized using Butane configs to repartition the disk and create/reformat filesystems. Bare metal installations are not different; the installer only copies the raw image to the target disk and injects the specified config into `/boot` for use on first boot.

NOTE: See xref:#_reconfiguring_the_root_filesystem[Reconfiguring the root filesystem] for examples regarding the supported changes to the root partition.

=== Partition Tables

Using partition numbers to refer to specific partitions is discouraged and labels or UUIDs should be used instead. Fedora CoreOS reserves the `boot`, `boot-<number>`, `root`, `root-<number>`, `BIOS-BOOT`, `bios-<number>`, `EFI-SYSTEM`, and `esp-<number>` labels, and the `md-boot` and `md-root` RAID device names. Creating partitions, filesystems, or RAID devices with those labels is not supported.

=== x86_64 Partition Table

The x86_64 disk image is GPT formatted with a protective MBR. It supports booting via both BIOS and UEFI (including Secure Boot).

The partition table layout has changed over time. The current layout is:

.Partition Table for x86_64
|============================================================================================
| Number | Label      | Description                                          | Partition Type
| 1      | BIOS-BOOT  | Contains BIOS GRUB image                             | raw data
| 2      | EFI-SYSTEM | Contains EFI GRUB image and Secure Boot shim         | FAT32
| 3      | boot       | Contains GRUB configuration, kernel/initramfs images | ext4
| 4      | root       | Contains the root filesystem                         | xfs
|============================================================================================

The EFI-SYSTEM partition can be deleted or reformatted when BIOS booting. Similarly, the BIOS-BOOT partition can be deleted or reformatted when EFI booting.

== Mounted Filesystems

Fedora CoreOS uses OSTree, which is a system for managing multiple bootable operating system trees that share storage. Each operating system version is part of the `/` filesystem. All deployments share the same `/var` which can be on the same filesystem, or mounted separately.

This shows the default mountpoints for a Fedora CoreOS system installed on a `/dev/vda` disk:

.Default mountpoints on x86_64
[source,bash]
----
$ findmnt --real # Some details are elided
TARGET        SOURCE                                                   FSTYPE  OPTIONS
/             /dev/vda4[/ostree/deploy/fedora-coreos/deploy/$hash]     xfs     rw
|-/sysroot    /dev/vda4                                                xfs     ro
|-/etc        /dev/vda4[/ostree/deploy/fedora-coreos/deploy/$hash/etc] xfs     rw
|-/usr        /dev/vda4[/ostree/deploy/fedora-coreos/deploy/$hash/usr] xfs     ro
|-/var        /dev/vda4[/ostree/deploy/fedora-coreos/deploy/var]       xfs     rw
`-/boot       /dev/vda3                                                ext4    ro
----

The EFI System Partition was formerly mounted on `/boot/efi`, but this is no longer the case. On systems configured with boot device mirroring, there are independent EFI partitions on each constituent disk.

=== Immutable `/`, read only `/usr`

As OSTree is used to manage all files belonging to the operating system, the `/` and `/usr` mountpoints are not writable. Any changes to the operating system should be applied via https://coreos.github.io/rpm-ostree/administrator-handbook/[`rpm-ostree`].

Similarly, the `/boot` mountpoint is not writable, and the EFI System Partition is not mounted by default. These filesystems are managed by `rpm-ostree` and `bootupd`, and must not be directly modified by an administrator.

Adding top level directories (i.e. `/foo`) is currently unsupported and disallowed by the immutable attribute.

The *real* `/` (as in the root of the filesystem in the `root` partition) is mounted readonly in `/sysroot` and must not be accessed or modified directly.

=== Configuration in `/etc` and state in `/var`

The only supported writable locations are `/etc` and `/var`. `/etc` should contain only configuration files and is not expected to store data. All data must be kept under `/var` and will not be touched by system upgrades. Traditional places that might hold state (e.g. `/home`, or `/srv`) are symlinks to directories in `/var` (e.g. `/var/home` or `/var/srv`).

=== Version selection and bootup

A GRUB menu entry is created for each version of Fedora CoreOS currently available on a system. This menu entry references an `ostree` deployment which consists of a Linux kernel, an initramfs and a hash linking to an `ostree` commit (passed via the `ostree=` kernel argument). During bootup, `ostree` will read this kernel argument to determine which deployment to use as the root filesystem. Each update or change to the system (package installation, addition of kernel arguments) creates a new deployment. This enables rolling back to a previous deployment if the update causes problems.
= Managing Files, Directories and Links

You can use Ignition to create, replace or update files, directories or links.

This example creates a directory with the default mode (set to `0755`: readable
and recurseable by all), and writable only by the owner (by default `root`).

.Example to create a directory with default ownership and permissions
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
directories:
- path: /opt/tools
overwrite: true
----

This example creates a file named `/var/helloworld` with some content defined
in-line. It also sets the file mode to `0644` (readable by all, writable by the
owner) and sets ownership to `dnsmasq:dnsmasq`.

.Example to create a file with in-line content
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /var/helloworld
overwrite: true
contents:
inline: Hello, world!
mode: 0644
user:
name: dnsmasq
group:
name: dnsmasq
----

This example creates a file with its content fetched from a remote location. In
this case, it fetches an HTTPS URL and expects the file to be compressed with
gzip and will decompress it before writing it on the disk. The decompressed
content is checked against the hash value specified in the config. The format
is `sha512-` followed by the 128 hex characters given by the sha512sum command.
The resulting file is made readable and executable by all.

.Example to create a file from a remote source
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /opt/tools/transmogrifier
overwrite: true
contents:
source: https://mytools.example.com/path/to/archive.gz
compression: gzip
verification:
hash: sha512-00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
mode: 0555
----

This example creates a symbolic link in `/usr/local/bin` to a path in `/opt`.
This is useful to let local processes invoke a program without altering their
PATH environment variable.

.Example to create a symbolic link
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
links:
- path: /usr/local/bin/transmogrifier
overwrite: true
target: /opt/tools/transmogrifier
hard: false
----

If you need a directory and some of its parents to be owned by a specific user,
you currently have to explicitly list them in your Butane config. See
https://github.com/coreos/butane/issues/380[butane#380] for the tracking issue
in Butane for a future better syntax for this case.

.Example to create directories with specific ownership
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
directories:
- path: /home/builder/.config
user:
name: builder
group:
name: builder
- path: /home/builder/.config/systemd
user:
name: builder
group:
name: builder
- path: /home/builder/.config/systemd/user
user:
name: builder
group:
name: builder
- path: /home/builder/.config/systemd/user/default.target.wants
user:
name: builder
group:
name: builder
- path: /home/builder/.config/systemd/user/timers.target.wants
user:
name: builder
group:
name: builder
- path: /home/builder/.config/systemd/user/sockets.target.wants
user:
name: builder
group:
name: builder
----
= Host Network Configuration

== Host Network Configuration Options

=== Background

Unless xref:#_disabling_automatic_configuration_of_ethernet_devices[otherwise configured], Fedora CoreOS (FCOS) will attempt DHCP on every interface with a cable plugged in. However, if you need to use static addressing or more complex networking (vlans, bonds, bridges, teams, etc..), you can do so in a number of ways which are summarized below. Regardless of the way you choose to configure networking it all ends up as configuration for NetworkManager, which takes the form of NetworkManager keyfiles. More information on the keyfile format can be found https://networkmanager.dev/docs/api/latest/nm-settings-keyfile.html[here]. More information on the subsection options for keyfiles can be found https://networkmanager.dev/docs/api/latest/ref-settings.html[here].

=== Configuration Options

FCOS machines are primarily configured via Ignition, which runs from the initramfs on the machine's first boot. Depending on the platform the machine may need network access to retrieve remote resources; either the Ignition config itself, or remote resources specified inside the Ignition config.

NOTE: Networking will only be started in the initramfs if determined to be required, or if explicitly requested by the user with `*rd.neednet=1*`.

Whether or not a machine needs networking in the initramfs can dictate how a user will configure networking for the machine. The options for configuring networking for a machine are:

* via kernel arguments
** these get processed by dracut modules in the initramfs on first boot
* via live image customization
** by embedding network configuration in the live ISO or PXE image
* via `coreos-installer install --copy-network`
** by propagating the installation environment networking configuration
* via Afterburn
** by applying network configuration injected by various platforms
* via Ignition
** by laying down files that NetworkManager then uses on startup

NOTE: If you need networking connectivity to pull your Ignition configuration, or if your Ignition has remote references, you won't be able to provide your networking configuration via Ignition.

NOTE: If you provide networking configuration in multiple ways (i.e. via kernel arguments and via Ignition) then the configuration supplied via Ignition will win and be what is applied to the real root of the machine. It is not supported to provide half configuration via kernel arguments and half via Ignition.

We'll cover each one of these options now.


==== via Kernel Arguments

On the first boot of a machine a user can provide kernel arguments that define networking configuration. These kernel arguments are mostly defined in the https://man7.org/linux/man-pages/man7/dracut.cmdline.7.html[dracut.cmdline man page]. There are a few different ways to apply these kernel arguments on first boot.

1. In the most generic form, you can stop an instance at the GRUB prompt on the first boot (Ignition boot) and add them to the existing set of kernel arguments.

2. For a bare metal install where you automate the installation via kernel arguments added, (i.e., `coreos.inst.install_dev=`), you can also append networking arguments there and they will apply to the install boot and also the first boot (Ignition boot) of the installed machine.

3. For a PXE boot you can add networking kernel arguments to your existing set of kernel arguments in your PXE configuration.

An example set of kernel arguments for statically configuring an IP address for `ens2` looks like:

[source, bash]
----
ip=10.10.10.10::10.10.10.1:255.255.255.0:myhostname:ens2:none:8.8.8.8
----

The syntax is a bit hard to work with. An easy way to work with it is to write a small script that will fill in the items for you. For the example above, something like this should work:

[source, bash]
----
ip='10.10.10.10'
gateway='10.10.10.1'
netmask='255.255.255.0'
hostname='myhostname'
interface='ens2'
nameserver='8.8.8.8'
echo "ip=${ip}::${gateway}:${netmask}:${hostname}:${interface}:none:${nameserver}"
----


==== via live image customization

coreos-installer allows you to embed NetworkManager keyfiles directly in a live ISO or PXE image by using the `--network-keyfile` option to `coreos-installer iso customize` or `coreos-installer pxe customize`. The configuration is applied in the initramfs before Ignition runs. If you also use the `--installer-config` option or any of the `--dest-*` options to configure automatic installation, or the `--copy-network` option when installing manually, the network configuration will be forwarded to the installed system.

For more details on embedding network configuration in a live image, see the xref:live-reference.adoc#_passing_network_configuration_to_a_live_iso_or_pxe_system[live ISO/PXE image reference].


==== via `coreos-installer install --copy-network`

For manual bare metal install workflows it may not be preferable to use dracut kernel arguments for configuring network:

- the syntax is not very user-friendly
- manipulating kernel arguments by grabbing the GRUB prompt can be challenging

The `--copy-network` option to `coreos-installer install` will copy the files from `/etc/NetworkManager/system-connections/` directory into the installed system. For an interactive install this allows the user to populate networking configuration in a variety of ways before doing the installation:

- using the `nmcli` command
- using the `nmtui` TUI interface
- writing files directly
- using another tool of choice

It also allows the user to do hardware discovery on the node (i.e. "what are my interface names?"). For an example of this workflow see link:++https://dustymabe.com/2020/11/18/coreos-install-via-live-iso--copy-network/++[this demo] which shows it in detail.


==== via Afterburn

On certain platforms Afterburn will inject networking configuration, either configured by the user or by the platform, during the initramfs.

Currently, this is only utilized on VMWare. The implementation there allows for users to specify networking configuration in the form of dracut networking arguments without having to stop the boot of the machine and manually inject those arguments themselves.

See https://coreos.github.io/afterburn/usage/initrd-network-cmdline/[the Afterburn documentation] for more information.

==== via Ignition

WARNING: If you need networking to grab your Ignition config and your environment requires more complex networking than the default of DHCP to grab the Ignition config, then you'll need to use another method other than Ignition to configure the network.

Networking configuration can be performed by writing out files described in an Ignition config. These are https://networkmanager.dev/docs/api/latest/nm-settings-keyfile.html[NetworkManager keyfiles] that are written to `/etc/NetworkManager/system-connections/` that tell NetworkManager what to do.

Any configuration provided via Ignition will be considered at a higher priority than any other method of configuring the Network for a Fedora CoreOS instance. If you specify Networking configuration via Ignition, try not to use other mechanisms to configure the network.

An example https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/[Butane] config for the same static networking example that we showed above is:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=ens2
type=ethernet
interface-name=ens2
[ipv4]
address1=10.10.10.10/24,10.10.10.1
dns=8.8.8.8;
dns-search=
may-fail=false
method=manual
----


== Host Network Configuration Examples

In this section we'll go through common examples of setting up different types of networking devices using both dracut kernel arguments as well as NetworkManager keyfiles via Ignition/Butane.

Examples in this section that use a static IP will assume these values unless otherwise stated:

[source, bash]
----
ip='10.10.10.10'
gateway='10.10.10.1'
netmask='255.255.255.0'
prefix='24'
hostname='myhostname'
interface='ens2'
nameserver='8.8.8.8'
bondname='bond0'
teamname='team0'
bridgename='br0'
subnic1='ens2'
subnic2='ens3'
vlanid='100'
----

NOTE: FCOS uses https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/[predictable interface names] by https://lists.fedoraproject.org/archives/list/coreos-status@lists.fedoraproject.org/thread/6IPTZL57Z5NLBMPYMXNVSYAGLRFZBLIP/[default]. Please take care to use the correct interface name for your hardware.

=== Generating NetworkManager Keyfiles using `nm-initrd-generator`

NetworkManager ships a tool, https://networkmanager.dev/docs/api/latest/nm-initrd-generator.html[nm-initrd-generator], that can generate keyfiles from dracut kernel argument syntax. This might be a good way to either convert from kernel arguments to keyfiles or to just quickly generate some keyfiles giving a small amount of input and then tweak some more detailed settings.

Here's an example of generating keyfiles for a bond via `nm-initrd-generator`:

[source, bash]
----
$ kargs="ip=bond0:dhcp bond=bond0:ens2,ens3:mode=active-backup,miimon=100 nameserver=8.8.8.8"
$ /usr/libexec/nm-initrd-generator -s -- $kargs

*** Connection 'bond0' ***

[connection]
id=bond0
uuid=643c17b5-b364-4137-b273-33f450a45476
type=bond
interface-name=bond0
multi-connect=1
permissions=

[ethernet]
mac-address-blacklist=

[bond]
miimon=100
mode=active-backup

[ipv4]
dns=8.8.8.8;
dns-search=
may-fail=false
method=auto

[ipv6]
addr-gen-mode=eui64
dns-search=
method=auto

[proxy]

*** Connection 'ens3' ***

[connection]
id=ens3
uuid=b42cc917-fd87-47df-9ac2-34622ecddd8c
type=ethernet
interface-name=ens3
master=643c17b5-b364-4137-b273-33f450a45476
multi-connect=1
permissions=
slave-type=bond

[ethernet]
mac-address-blacklist=

*** Connection 'ens2' ***

[connection]
id=ens2
uuid=e111bb4e-3ee3-4612-afc2-1d2dfff97671
type=ethernet
interface-name=ens2
master=643c17b5-b364-4137-b273-33f450a45476
multi-connect=1
permissions=
slave-type=bond

[ethernet]
mac-address-blacklist=
----

This run generates three keyfiles. One for `bond0`, one for `ens3`, and one for `ens2`. You can take the generated output, add more settings or tweak existing settings, and then deliver the files via Ignition.


=== Configuring a Static IP

==== Dracut Kernel Arguments

.Template
[source, bash]
----
ip=${ip}::${gateway}:${netmask}:${hostname}:${interface}:none:${nameserver}
----

.Rendered
[source, bash]
----
ip=10.10.10.10::10.10.10.1:255.255.255.0:myhostname:ens2:none:8.8.8.8
----

==== Butane config

.Template
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/${interface}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${interface}
type=ethernet
interface-name=${interface}
[ipv4]
address1=${ip}/${prefix},${gateway}
dhcp-hostname=${hostname}
dns=${nameserver};
dns-search=
may-fail=false
method=manual
----

.Rendered
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=ens2
type=ethernet
interface-name=ens2
[ipv4]
address1=10.10.10.10/24,10.10.10.1
dhcp-hostname=myhostname
dns=8.8.8.8;
dns-search=
may-fail=false
method=manual
----



=== Configuring a Bond (Static IP)

==== Dracut Kernel Arguments

.Template
[source, bash]
----
ip=${ip}::${gateway}:${netmask}:${hostname}:${bondname}:none:${nameserver}
bond=${bondname}:${subnic1},${subnic2}:mode=active-backup,miimon=100
----

.Rendered
[source, bash]
----
ip=10.10.10.10::10.10.10.1:255.255.255.0:myhostname:bond0:none:8.8.8.8
bond=bond0:ens2,ens3:mode=active-backup,miimon=100
----

==== Butane config

.Template
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/${bondname}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}
type=bond
interface-name=${bondname}
[bond]
miimon=100
mode=active-backup
[ipv4]
address1=${ip}/${prefix},${gateway}
dhcp-hostname=${hostname}
dns=${nameserver};
dns-search=
may-fail=false
method=manual
- path: /etc/NetworkManager/system-connections/${bondname}-slave-${subnic1}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}-slave-${subnic1}
type=ethernet
interface-name=${subnic1}
master=${bondname}
slave-type=bond
- path: /etc/NetworkManager/system-connections/${bondname}-slave-${subnic2}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}-slave-${subnic2}
type=ethernet
interface-name=${subnic2}
master=${bondname}
slave-type=bond
----

.Rendered
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/bond0.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0
type=bond
interface-name=bond0
[bond]
miimon=100
mode=active-backup
[ipv4]
address1=10.10.10.10/24,10.10.10.1
dhcp-hostname=myhostname
dns=8.8.8.8;
dns-search=
may-fail=false
method=manual
- path: /etc/NetworkManager/system-connections/bond0-slave-ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0-slave-ens2
type=ethernet
interface-name=ens2
master=bond0
slave-type=bond
- path: /etc/NetworkManager/system-connections/bond0-slave-ens3.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0-slave-ens3
type=ethernet
interface-name=ens3
master=bond0
slave-type=bond
----


=== Configuring a Bridge (DHCP)

==== Dracut Kernel Arguments

.Template
[source, bash]
----
ip=${bridgename}:dhcp
bridge=${bridgename}:${subnic1},${subnic2}
----

.Rendered
[source, bash]
----
ip=br0:dhcp
bridge=br0:ens2,ens3
----

==== Butane config

.Template
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/${bridgename}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bridgename}
type=bridge
interface-name=${bridgename}
[bridge]
[ipv4]
dns-search=
may-fail=false
method=auto
- path: /etc/NetworkManager/system-connections/${bridgename}-slave-${subnic1}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bridgename}-slave-${subnic1}
type=ethernet
interface-name=${subnic1}
master=${bridgename}
slave-type=bridge
[bridge-port]
- path: /etc/NetworkManager/system-connections/${bridgename}-slave-${subnic2}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bridgename}-slave-${subnic2}
type=ethernet
interface-name=${subnic2}
master=${bridgename}
slave-type=bridge
[bridge-port]
----

.Rendered
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/br0.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=br0
type=bridge
interface-name=br0
[bridge]
[ipv4]
dns-search=
may-fail=false
method=auto
- path: /etc/NetworkManager/system-connections/br0-slave-ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=br0-slave-ens2
type=ethernet
interface-name=ens2
master=br0
slave-type=bridge
[bridge-port]
- path: /etc/NetworkManager/system-connections/br0-slave-ens3.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=br0-slave-ens3
type=ethernet
interface-name=ens3
master=br0
slave-type=bridge
[bridge-port]
----


=== Configuring a Team (DHCP)

==== Dracut Kernel Arguments

.Template
[source, bash]
----
ip=${teamname}:dhcp
team=${teamname}:${subnic1},${subnic2}
----

.Rendered
[source, bash]
----
ip=team0:dhcp
team=team0:ens2,ens3
----

==== Butane config

.Template
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/${teamname}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${teamname}
type=team
interface-name=${teamname}
[team]
config={"runner": {"name": "activebackup"}, "link_watch": {"name": "ethtool"}}
[ipv4]
dns-search=
may-fail=false
method=auto
- path: /etc/NetworkManager/system-connections/${teamname}-slave-${subnic1}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${teamname}-slave-${subnic1}
type=ethernet
interface-name=${subnic1}
master=${teamname}
slave-type=team
[team-port]
config={"prio": 100}
- path: /etc/NetworkManager/system-connections/${teamname}-slave-${subnic2}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${teamname}-slave-${subnic2}
type=ethernet
interface-name=${subnic2}
master=${teamname}
slave-type=team
[team-port]
config={"prio": 100}
----

.Rendered
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/team0.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=team0
type=team
interface-name=team0
[team]
config={"runner": {"name": "activebackup"}, "link_watch": {"name": "ethtool"}}
[ipv4]
dns-search=
may-fail=false
method=auto
- path: /etc/NetworkManager/system-connections/team0-slave-ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=team0-slave-ens2
type=ethernet
interface-name=ens2
master=team0
slave-type=team
[team-port]
config={"prio": 100}
- path: /etc/NetworkManager/system-connections/team0-slave-ens3.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=team0-slave-ens3
type=ethernet
interface-name=ens3
master=team0
slave-type=team
[team-port]
config={"prio": 100}
----


=== Configuring a Vlan (Static IP)

==== Dracut Kernel Arguments

.Template
[source, bash]
----
ip=${ip}::${gateway}:${netmask}:${hostname}:${interface}.${vlanid}:none:${nameserver}
vlan=${interface}.${vlanid}:${interface}
----

.Rendered
[source, bash]
----
ip=10.10.10.10::10.10.10.1:255.255.255.0:myhostname:ens2.100:none:8.8.8.8
vlan=ens2.100:ens2
----

==== Butane config

.Template
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/${interface}.${vlanid}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${interface}.${vlanid}
type=vlan
interface-name=${interface}.${vlanid}
[vlan]
egress-priority-map=
flags=1
id=${vlanid}
ingress-priority-map=
parent=${interface}
[ipv4]
address1=${ip}/${prefix},${gateway}
dhcp-hostname=${hostname}
dns=${nameserver};
dns-search=
may-fail=false
method=manual
- path: /etc/NetworkManager/system-connections/${interface}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${interface}
type=ethernet
interface-name=${interface}
[ipv4]
dns-search=
method=disabled
[ipv6]
addr-gen-mode=eui64
dns-search=
method=disabled
----

.Rendered
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/ens2.100.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=ens2.100
type=vlan
interface-name=ens2.100
[vlan]
egress-priority-map=
flags=1
id=100
ingress-priority-map=
parent=ens2
[ipv4]
address1=10.10.10.10/24,10.10.10.1
dhcp-hostname=myhostname
dns=8.8.8.8;
dns-search=
may-fail=false
method=manual
- path: /etc/NetworkManager/system-connections/ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=ens2
type=ethernet
interface-name=ens2
[ipv4]
dns-search=
method=disabled
[ipv6]
addr-gen-mode=eui64
dns-search=
method=disabled
----

=== Configuring a Vlan on a Bond (DHCP)

==== Dracut Kernel Arguments

.Template
[source, bash]
----
ip=${bondname}.${vlanid}:dhcp
bond=${bondname}:${subnic1},${subnic2}:mode=active-backup,miimon=100
vlan=${bondname}.${vlanid}:${bondname}
----

.Rendered
[source, bash]
----
ip=bond0.100:dhcp
bond=bond0:ens2,ens3:mode=active-backup,miimon=100
vlan=bond0.100:bond0
----

==== Butane config

.Template
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/${bondname}.${vlanid}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}.${vlanid}
type=vlan
interface-name=${bondname}.${vlanid}
[vlan]
egress-priority-map=
flags=1
id=${vlanid}
ingress-priority-map=
parent=${bondname}
[ipv4]
dns-search=
may-fail=false
method=auto
- path: /etc/NetworkManager/system-connections/${bondname}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}
type=bond
interface-name=${bondname}
[bond]
miimon=100
mode=active-backup
[ipv4]
method=disabled
[ipv6]
method=disabled
- path: /etc/NetworkManager/system-connections/${bondname}-slave-${subnic1}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}-slave-${subnic1}
type=ethernet
interface-name=${subnic1}
master=${bondname}
slave-type=bond
- path: /etc/NetworkManager/system-connections/${bondname}-slave-${subnic2}.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=${bondname}-slave-${subnic2}
type=ethernet
interface-name=${subnic2}
master=${bondname}
slave-type=bond
----

.Rendered
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/system-connections/bond0.100.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0.100
type=vlan
interface-name=bond0.100
[vlan]
egress-priority-map=
flags=1
id=100
ingress-priority-map=
parent=bond0
[ipv4]
dns-search=
may-fail=false
method=auto
- path: /etc/NetworkManager/system-connections/bond0.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0
type=bond
interface-name=bond0
[bond]
miimon=100
mode=active-backup
[ipv4]
method=disabled
[ipv6]
method=disabled
- path: /etc/NetworkManager/system-connections/bond0-slave-ens2.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0-slave-ens2
type=ethernet
interface-name=ens2
master=bond0
slave-type=bond
- path: /etc/NetworkManager/system-connections/bond0-slave-ens3.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=bond0-slave-ens3
type=ethernet
interface-name=ens3
master=bond0
slave-type=bond
----

=== Disabling Automatic Configuration of Ethernet Devices

By default, FCOS will attempt to autoconfigure (DHCP/SLAAC) on every interface with a cable plugged in. In some network environments this may not be desirable. It's possible to change this behavior of NetworkManager with a configuration file dropin:


.Disable NetworkManager autoconfiguration of ethernet devices
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/NetworkManager/conf.d/noauto.conf
mode: 0644
contents:
inline: |
[main]
# Do not do automatic (DHCP/SLAAC) configuration on ethernet devices
# with no other matching connections.
no-auto-default=*
----

WARNING: If NetworkManager autoconfiguration of ethernet devices is disabled and no other network configuration is provided the system will boot without network access.
= Enabling Wi-Fi

The primary use for Fedora CoreOS has been driving server hardware in individual datacenters or cloud environments, which have high speed wired networking without the need for Wi-Fi enablement. Since there are many different types of wireless cards, link:https://github.com/coreos/fedora-coreos-tracker/issues/862[adding Wi-Fi enablement to Fedora CoreOS by default] would require many large firmware binaries to be installed for a non-standard use, which isn't ideal.

On the other hand, Fedora CoreOS is versatile enough to run on smaller devices in IoT applications or in home labs where Wi-Fi may be required. In these cases it is easy enough to add a layer with the needed tools and firmware.

== Adding Wi-Fi tools and firmware

Typically enabling Wi-Fi on Fedora CoreOS involves adding the `NetworkManager-wifi` package along with the firmware package that corresponds to the wireless card in your system. Here is a list of some of the wireless firmware packages in Fedora:

.Wi-Fi firmware packages in Fedora
[source, text]
----
atheros-firmware - Firmware for Qualcomm Atheros WiFi/Bluetooth adapters
b43-fwcutter - Firmware extraction tool for Broadcom wireless driver
b43-openfwwf - Open firmware for some Broadcom 43xx series WLAN chips
brcmfmac-firmware - Firmware for Broadcom/Cypress brcmfmac WiFi/Bluetooth adapters
iwlegacy-firmware - Firmware for Intel(R) Wireless WiFi Link 3945(A)BG and 4965AGN adapters
iwlwifi-dvm-firmware - DVM Firmware for Intel(R) Wireless WiFi adapters
iwlwifi-mvm-firmware - MVM Firmware for Intel(R) Wireless WiFi adapters
libertas-firmware - Firmware for Marvell Libertas SD/USB WiFi Network Adapters
mt7xxx-firmware - Firmware for Mediatek 7600/7900 series WiFi/Bluetooth adapters
nxpwireless-firmware - Firmware for NXP WiFi/Bluetooth/UWB adapters
realtek-firmware - Firmware for Realtek WiFi/Bluetooth adapters
tiwilink-firmware - Firmware for Texas Instruments WiFi/Bluetooth adapters
atmel-firmware - Firmware for Atmel at76c50x wireless network chips
zd1211-firmware - Firmware for wireless devices based on zd1211 chipset
----

For example, if a system has a Qualcomm wireless card then adding the `NetworkManager-wifi` and `atheros-firmware` packages would sufficiently enable the system for connecting to Wi-Fi. You can try to inspect your wireless card to determine what driver you need by running `lspci` (provided by the `pciutils` package) xref:debugging-with-toolbox.adoc[inside a Toolbx container].


== When installing Fedora CoreOS

For new systems the packages can be added using the xref:os-extensions.adoc[Adding OS Extensions] workflow. A NetworkManager configuration for the Wi-Fi connection will also need to be added so the system knows which wireless network to connect to. For more information on network configuration in Fedora CoreOS see xref:sysconfig-network-configuration.adoc[Network Configuration].

An example Butane config that combines the extension and network configuration is shown below.

.Butane config for Wi-Fi enablement
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
# Enable Wi-Fi in NetworkManager for an Intel wireless card
- name: rpm-ostree-install-wifi.service
enabled: true
contents: |
[Unit]
Description=Enable Wi-Fi
Wants=network-online.target
After=network-online.target
Before=zincati.service
ConditionPathExists=!/var/lib/%N.stamp
[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=/usr/bin/rpm-ostree install -y --allow-inactive NetworkManager-wifi iwlwifi-dvm-firmware
ExecStart=/bin/touch /var/lib/%N.stamp
ExecStart=/bin/systemctl --no-block reboot
[Install]
WantedBy=multi-user.target
storage:
files:
- path: /etc/NetworkManager/system-connections/wifi-guest.nmconnection
mode: 0600
contents:
inline: |
[connection]
id=wifi-guest
type=wifi
autoconnect=true
[wifi]
cloned-mac-address=permanent
mode=infrastructure
ssid=guest
mac-address=ab:cd:01:02:03:04
[wifi-security]
auth-alg=open
key-mgmt=wpa-psk
psk=PASSWORD
[ipv4]
method=auto
----

NOTE: When installing a system and adding Wi-Fi enablement in this way the system will need to be on a wired network for the initial install since it will need to use the network to retrieve the Wi-Fi enabling packages.


== On an existing Fedora CoreOS system

If you have a system up already and want to add Wi-Fi capabilities (i.e. if you want to move it to a location without wired access) you can request the required packages.

.Request NetworkManager-wifi and a specific Wi-Fi firmware
[source, text]
----
$ sudo rpm-ostree install -y --allow-inactive \
NetworkManager-wifi iwlwifi-dvm-firmware
----

If you don't know what firmware to request you can request all the wireless firmware available in Fedora. Please note this approach is sub-optimal as it will add many unneeded packages on your system.

.Request NetworkManager-wifi and all available Wi-Fi firmware
----
$ sudo rpm-ostree install -y --allow-inactive \
NetworkManager-wifi  \
atheros-firmware     \
b43-fwcutter         \
b43-openfwwf         \
brcmfmac-firmware    \
iwlegacy-firmware    \
iwlwifi-dvm-firmware \
iwlwifi-mvm-firmware \
libertas-firmware    \
mt7xxx-firmware      \
nxpwireless-firmware \
realtek-firmware     \
tiwilink-firmware    \
atmel-firmware       \
zd1211-firmware
----

Then reboot the system.
= Kernel tunables (sysctl)

The Linux kernel offers a plethora of knobs under `/proc/sys` to control the availability of different features and tune performance parameters.

Values under `/proc/sys` can be changed directly at runtime, but such changes will not be persisted across reboots.
Persistent settings should be written under `/etc/sysctl.d/` during provisioning, in order to be applied on each boot.

As an example, the xref:producing-ign.adoc[Butane] snippet below shows how to disable _SysRq_ keys:

.Example: configuring kernel tunable to disable SysRq keys
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/sysctl.d/90-sysrq.conf
contents:
inline: |
kernel.sysrq = 0
----

Further details can be found in the systemd man pages https://www.freedesktop.org/software/systemd/man/sysctl.d.html[sysctl.d(5)] and https://www.freedesktop.org/software/systemd/man/systemd-sysctl.service.html[systemd-sysctl.service(8)].
= Running Containers

== Introduction
Fedora CoreOS ships with both the `docker` CLI tool (as provided via https://mobyproject.org/[Moby]) and https://podman.io[podman] installed. This page explains how to use systemd units to start and stop containers with podman.

== Example configuration
The following Butane config snippet configures the systemd `hello.service` to run https://www.busybox.net[busybox].

TIP: You may be able to use local file references to systemd units instead of inlining them. See xref:tutorial-services.adoc#_using_butanes__files_dir_parameter_to_embed_files[Using butane's `--files-dir` Parameter to Embed Files] for more information.

.Example for running busybox using systemd and podman
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: hello.service
enabled: true
contents: |
[Unit]
Description=MyApp
After=network-online.target
Wants=network-online.target

[Service]
TimeoutStartSec=0
ExecStartPre=-/bin/podman kill busybox1
ExecStartPre=-/bin/podman rm busybox1
ExecStartPre=/bin/podman pull busybox
ExecStart=/bin/podman run --name busybox1 busybox /bin/sh -c "trap 'exit 0' INT TERM; while true; do echo Hello World; sleep 1; done"

[Install]
WantedBy=multi-user.target
----

.Example for running busybox using Podman Quadlet

https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html[Podman Quadlet] is functionality included in podman that allows starting containers via systemd using a systemd generator. The example below is the same `hello.service` that was previously shown but deployed via the Podman Quadlet functionality.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/containers/systemd/hello.container
contents:
inline: |
[Unit]
Description=Hello Service
Wants=network-online.target
After=network-online.target

[Container]
ContainerName=busybox1
Image=docker.io/busybox
Exec=/bin/sh -c "trap 'exit 0' INT TERM; while true; do echo Hello World; sleep 1; done"

[Install]
WantedBy=multi-user.target
----

=== Running etcd

https://etcd.io[etcd] is not shipped as part of Fedora CoreOS. To use it, run it as a container, as shown below.

.Butane config for setting up single node etcd
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: etcd-member.service
enabled: true
contents: |
[Unit]
Description=Run single node etcd
After=network-online.target
Wants=network-online.target

[Service]
ExecStartPre=mkdir -p /var/lib/etcd
ExecStartPre=-/bin/podman kill etcd
ExecStartPre=-/bin/podman rm etcd
ExecStartPre=-/bin/podman pull quay.io/coreos/etcd
ExecStart=/bin/podman run --name etcd --volume /var/lib/etcd:/etcd-data:z --net=host quay.io/coreos/etcd:latest /usr/local/bin/etcd --data-dir /etcd-data --name node1 \
--initial-advertise-peer-urls http://127.0.0.1:2380 --listen-peer-urls http://127.0.0.1:2380 \
--advertise-client-urls http://127.0.0.1:2379 \
--listen-client-urls http://127.0.0.1:2379 \
--initial-cluster node1=http://127.0.0.1:2380

ExecStop=/bin/podman stop etcd

[Install]
WantedBy=multi-user.target
----

=== For more information
See the https://etcd.io/docs/latest/op-guide/container/#docker[etcd documentation] for more information on running etcd in containers and how to set up multi-node etcd.
= Configuring Users

== Default User

By default, a privileged user named `core` is created on the Fedora CoreOS system, but it is not configured with a default password or SSH key. If you wish to use the `core` user, you must provide an Ignition config which includes a password and/or SSH key(s) for the `core` user. Alternatively you may create additional, new users via Ignition configs.

If you do not want to use Ignition to manage the default user's SSH key(s), you can make use of the https://coreos.github.io/afterburn/platforms/[Afterburn support] and provide an SSH key via your cloud provider.

== Creating a New User

To create a new user (or users), add it to the `users` list of your Butane config. In the following example, the config creates two new usernames, but doesn't configure them to be especially useful.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: jlebon
- name: miabbott
----

You will typically want to configure SSH keys or a password, in order to be able to log in as those users.

== Using an SSH Key

To configure an SSH key for a local user, you can use a Butane config:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHn2eh...
- name: jlebon
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDC5QFS...
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIveEaMRW...
- name: miabbott
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDTey7R...
----

=== Using File References to SSH Keys

Depending on the configuration variant and version you use, you can use local file references to SSH public keys instead
of inlining them.
The example from the xref:#_using_an_ssh_key[previous section] can thus be rewritten as follows:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys_local:
- users/core/id_rsa.pub
- name: jlebon
ssh_authorized_keys_local:
- users/jlebon/id_rsa.pub
- users/jlebon/id_ed25519.pub
- name: miabbott
ssh_authorized_keys_local:
- users/miabbott/id_rsa.pub
----

You have to use `butane` with the `--files-dir` parameter to allow loading files from disk when converting to Ignition configurations for this to work.

NOTE: Check the https://coreos.github.io/butane/specs/[Configuration specifications] for more details and which versions
of your selected variant support it. Generally, each file may contain multiple SSH keys, one per line, and you may
additionally specify inline `ssh_authorized_keys` as well as long as the SSH keys are unique.

=== SSH Key Locations

https://man.openbsd.org/sshd_config[sshd] uses a https://github.com/coreos/ssh-key-dir[helper program], specified via
the `AuthorizedKeysCommand` directive, to read public keys from files in a user's `~/.ssh/authorized_keys.d` directory.
The `AuthorizedKeysCommand` is tried after the usual `AuthorizedKeysFile` files (defaulting to `~/.ssh/authorized_keys`)
and will not be executed if a matching key is found there. Key files in `~/.ssh/authorized_keys.d` are read in
alphabetical order, ignoring dotfiles.

Ignition writes configured SSH keys to `~/.ssh/authorized_keys.d/ignition`. On platforms where SSH keys can be configured at the platform level, such as AWS, Afterburn writes those keys to `~/.ssh/authorized_keys.d/afterburn`.

To debug the reading of `~/.ssh/authorized_keys.d`, manually run the helper program and inspect its output:

[source,bash]
----
/usr/libexec/ssh-key-dir
----

To view and validate the effective configuration for sshd, two test modes (`-t`, `-T`) are available as documented on the https://man.openbsd.org/sshd[manual pages].

== Using Password Authentication

Fedora CoreOS ships with no default passwords. You can use a Butane config to set a password for a local user. Building on the previous example, we can configure the `password_hash` for one or more users:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHn2eh...
- name: jlebon
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDC5QFS...
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIveEaMRW...
- name: miabbott
password_hash: $y$j9T$aUmgEDoFIDPhGxEe2FUjc/$C5A...
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDTey7R...
----

To generate a secure password hash, use `mkpasswd` from the `whois` package. Your Linux distro may ship a different `mkpasswd` implementation; you can ensure you're using the correct one by running it from a container:

[source]
----
$ podman run -ti --rm quay.io/coreos/mkpasswd --method=yescrypt
Password:
$y$j9T$A0Y3wwVOKP69S.1K/zYGN.$S596l11UGH3XjN...
----

The `yescrypt` hashing method is recommended for new passwords. For more details on hashing methods, see `man 5 crypt`.

The configured password will be accepted for local authentication at the console. By default, Fedora CoreOS does not allow <<_enabling_ssh_password_authentication,password authentication via SSH>>.

== Configuring Groups

Fedora CoreOS comes with a few groups configured by default: `root`, `adm`, `wheel`, `sudo`, `systemd-journal`, `docker`

When configuring users via Butane configs, we can specify groups that the user(s) should be a part of.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHn2eh...
- name: jlebon
groups:
- wheel
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDC5QFS...
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIveEaMRW...
- name: miabbott
groups:
- docker
- wheel
password_hash: $y$j9T$aUmgEDoFIDPhGxEe2FUjc/$C5A...
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDTey7R...
----

If a group does not exist, users should create them as part of the Butane config.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
groups:
- name: engineering
- name: marketing
gid: 9000
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHn2eh...
- name: jlebon
groups:
- engineering
- wheel
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDC5QFS...
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIveEaMRW...
- name: miabbott
groups:
- docker
- marketing
- wheel
password_hash: $y$j9T$aUmgEDoFIDPhGxEe2FUjc/$C5A...
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDTey7R...
----

== Configuring Administrative Privileges

The easiest way for users to be granted administrative privileges is to have them added to the `sudo` and `wheel` groups as part of the Butane config.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
groups:
- name: engineering
- name: marketing
gid: 9000
users:
- name: core
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDHn2eh...
- name: jlebon
groups:
- engineering
- wheel
- sudo
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDC5QFS...
- ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIveEaMRW...
- name: miabbott
groups:
- docker
- marketing
- wheel
- sudo
password_hash: $y$j9T$aUmgEDoFIDPhGxEe2FUjc/$C5A...
ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDTey7R...
----

== Enabling SSH Password Authentication

To enable password authentication via SSH, add the following to your Butane config:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/ssh/sshd_config.d/20-enable-passwords.conf
mode: 0644
contents:
inline: |
# Fedora CoreOS disables SSH password login by default.
# Enable it.
# This file must sort before 40-disable-passwords.conf.
PasswordAuthentication yes
----
= Setting a Hostname

To set a custom hostname for your system, use the following Butane config to write to `/etc/hostname`:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/hostname
mode: 0644
overwrite: true
contents:
inline: myhostname
----

Once booted, you can also verify that the desired hostname has been set using `hostnamectl`.

NOTE: We use `overwrite: true` to make sure that we overwrite the hostname that is set up by Afterburn on some platforms.
If Afterburn does not support setting the hostname on your platform, you can remove it.
= Proxied Internet Access

If you are deploying to an environment requiring internet access via a proxy, you will want to configure services so that they can access resources as intended.

This is best done by defining a single file with required environment variables in your Butane configuration, and to reference this via systemd drop-in unit files for all such services.

== Defining common proxy environment variables

This common file has to be subsequently referenced explicitly by each service that requires internet access.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/example-proxy.env
mode: 0644
contents:
inline: |
https_proxy="http://example.com:8080"
all_proxy="http://example.com:8080"
http_proxy="http://example.com:8080"
HTTP_PROXY="http://example.com:8080"
HTTPS_PROXY="http://example.com:8080"
no_proxy="*.example.com,127.0.0.1,0.0.0.0,localhost"
----

== Defining drop-in units for core services

https://github.com/coreos/zincati[Zincati] polls for OS updates, and https://github.com/coreos/rpm-ostree[rpm-ostree] is used to apply OS and layered package updates both therefore requiring internet access. The optional anonymized https://docs.fedoraproject.org/en-US/fedora-coreos/counting/[countme] service also requires access if enabled.

TIP: You may be able to use local file references to systemd units instead of inlining them. See xref:tutorial-services.adoc#_using_butanes__files_dir_parameter_to_embed_files[Using butane's `--files-dir` Parameter to Embed Files] for more information.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: rpm-ostreed.service
dropins:
- name: 99-proxy.conf
contents: |
[Service]
EnvironmentFile=/etc/example-proxy.env
- name: zincati.service
dropins:
- name: 99-proxy.conf
contents: |
[Service]
EnvironmentFile=/etc/example-proxy.env
- name: rpm-ostree-countme.service
dropins:
- name: 99-proxy.conf
contents: |
[Service]
EnvironmentFile=/etc/example-proxy.env
----

== Defining drop-in units for container daemons

If using docker then the `docker.service` drop-in is sufficient. If running Kubernetes with containerd (and no docker) then the `containerd.service` drop-in may be necessary.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: docker.service
enabled: true
dropins:
- name: 99-proxy.conf
contents: |
[Service]
EnvironmentFile=/etc/example-proxy.env
- name: containerd.service
enabled: true
dropins:
- name: 99-proxy.conf
contents: |
[Service]
EnvironmentFile=/etc/example-proxy.env
----

== Defining proxy use for podman systemd units

Podman has no daemon and so configuration is for each individual service scheduled, and can be done as part of the full systemd unit definition.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: example-svc.service
enabled: true
contents: |
[Unit]
After=network-online.target
Wants=network-online.target

[Service]
EnvironmentFile=/etc/example-proxy.env
ExecStartPre=-/bin/podman kill example-svc
ExecStartPre=-/bin/podman rm example-svc
ExecStartPre=-/bin/podman pull example-image:latest
ExecStart=/bin/podman run --name example-svc example-image:latest
ExecStop=/bin/podman stop example-svc

[Install]
WantedBy=multi-user.target
----
= Setting Keyboard Layout

To set your system keyboard layout (keymap), use the following Butane config to write to `/etc/vconsole.conf`:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/vconsole.conf
mode: 0644
contents:
inline: KEYMAP=de
----

Once booted, you can also verify that the desired keymap has been set using `localectl`.
= Adding OS extensions to the host system

Fedora CoreOS keeps the base image as simple and small as possible for security and maintainability reasons. That is why you should in general prefer the usage of `podman` containers over layering software. However, in some cases it is necessary to add software to the base OS itself. For example, drivers or VPN software are potential candidates because they are harder to containerize and may be brought in as extensions to the OS.

NOTE: If you're making nontrivial changes to the base operating system, you may instead consider using https://docs.fedoraproject.org/en-US/bootc/[Fedora Bootc], which is oriented around custom OS builds derived from a starting base image. There is more information on the relationship between Fedora CoreOS and Fedora Bootc in xref:faq.adoc#_how_does_fedora_coreos_relate_to_fedora_bootc[our FAQ].

To add in additional software to a Fedora CoreOS system, you can use https://coreos.github.io/rpm-ostree/[`rpm-ostree install`]. Consider these packages as "extensions": they extend the functionality of the base OS rather than e.g. providing runtimes for user applications. That said, there are no restrictions on which packages one can actually install. By default, packages are downloaded from the https://docs.fedoraproject.org/en-US/quick-docs/repositories/[Fedora repositories].

To start the layering of a package, you need to write a systemd unit that executes the `rpm-ostree` command to install the wanted package(s).
Changes are applied to a new deployment and a reboot is necessary for those to take effect.

== Example: Layering vim and setting it as the default editor

Fedora CoreOS includes both `nano` and `vi` as text editors, with the former set as default (see the corresponding https://fedoraproject.org/wiki/Changes/UseNanoByDefault[Fedora change]).

This example shows how to install the fully fledged `vim` text editor and how to set it up as default for all users by setting up the required configuration in `/etc/profile.d/`.

NOTE: In the future, we will have a more Ignition-friendly method of doing this with stronger guarantees. See upstream issues https://github.com/coreos/butane/issues/81[butane#81] and https://github.com/coreos/fedora-coreos-tracker/issues/681[fedora-coreos-tracker#681] for more information.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
# Installing vim as a layered package with rpm-ostree
- name: rpm-ostree-install-vim.service
enabled: true
contents: |
[Unit]
Description=Layer vim with rpm-ostree
Wants=network-online.target
After=network-online.target
# We run before `zincati.service` to avoid conflicting rpm-ostree
# transactions.
Before=zincati.service
ConditionPathExists=!/var/lib/%N.stamp

[Service]
Type=oneshot
RemainAfterExit=yes
# `--allow-inactive` ensures that rpm-ostree does not return an error
# if the package is already installed. This is useful if the package is
# added to the root image in a future Fedora CoreOS release as it will
# prevent the service from failing.
ExecStart=/usr/bin/rpm-ostree install -y --allow-inactive vim
ExecStart=/bin/touch /var/lib/%N.stamp
ExecStart=/bin/systemctl --no-block reboot

[Install]
WantedBy=multi-user.target
storage:
files:
# Set vim as default editor
# We use `zz-` as prefix to make sure this is processed last in order to
# override any previously set defaults.
- path: /etc/profile.d/zz-default-editor.sh
overwrite: true
contents:
inline: |
export EDITOR=vim
----
= Installing Docker CE

By default, Fedora CoreOS comes with out-of-the-box support for `docker` CLI (as provided via https://mobyproject.org/[Moby]).
However, in some cases Docker Community Edition (CE) may be preferred for various reasons.
This page explains how to replace the provided version with the latest from the upstream Docker sources.

The recommended approach from the official https://docs.docker.com/engine/install/fedora/[Docker documentation] is to add the Docker repository to your system.
You can then install and update Docker CE from this repository.


== Installing Docker CE on first boot

On provisioning, you can install Docker CE during the first boot of the system via Ignition configuration.

.Example Butane config for setting up Docker CE
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
# Install Docker CE
- name: rpm-ostree-install-docker-ce.service
enabled: true
contents: |
[Unit]
Description=Install Docker CE
Wants=network-online.target
After=network-online.target
Before=zincati.service
ConditionPathExists=!/var/lib/%N.stamp

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=/usr/bin/curl --output-dir "/etc/yum.repos.d" --remote-name https://download.docker.com/linux/fedora/docker-ce.repo
ExecStart=/usr/bin/rpm-ostree override remove moby-engine containerd runc docker-cli --install docker-ce
ExecStart=/usr/bin/touch /var/lib/%N.stamp
ExecStart=/usr/bin/systemctl --no-block reboot

[Install]
WantedBy=multi-user.target
----


== Installing Docker CE on a running system

First, download and setup the Docker repository.
Then you need to remove `moby-engine` and several other conflicting packages that ship by default in the Fedora CoreOS image, install the necessary Docker CE packages, and reboot the system.

[source, bash]
----
curl --remote-name https://download.docker.com/linux/fedora/docker-ce.repo
sudo install --owner 0 --group 0 --mode 644 docker-ce.repo /etc/yum.repos.d/docker-ce.repo
sudo rpm-ostree override remove moby-engine containerd runc docker-cli --install docker-ce --reboot
----

=== Upgrading Docker CE

Docker CE should be upgraded automatically with each new release of Fedora CoreOS.

[NOTE]
====
If you have Docker CE installed and are still using Fedora CoreOS 40, upgrading to Fedora CoreOS 41 will likely fail.
This is due to the new `docker-cli` package added in Fedora CoreOS 41 and later.
To upgrade to Fedora CoreOS 41 youll need to reset the overrides and uninstall layered Docker CE packages with the following command.

[source, bash]
----
sudo rpm-ostree override reset containerd moby-engine runc --uninstall docker-ce
----

After upgrading to Fedora CoreOS 41, you can follow the instructions for xref:#_installing_docker_ce_on_a_running_system[Installing Docker CE on a running system].
====
= How to Customize a NIC Name

== Using a systemd Link File
You can create a systemd https://www.freedesktop.org/software/systemd/man/systemd.link.html[link file] with Ignition configs.

For example, to name NIC with the MAC address `12:34:56:78:9a:bc` to "infra", place a systemd link file at `/etc/systemd/network/25-infra.link` using the xref:producing-ign.adoc[Butane] config snippet shown below:

.Example: Customize NIC via systemd Link File
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/systemd/network/25-infra.link
mode: 0644
contents:
inline: |
[Match]
MACAddress=12:34:56:78:9a:bc
[Link]
Name=infra
----

== Using Udev Rules
Similarly, also through Ignition configs, to name NIC with the MAC address `12:34:56:78:9a:bc` to "infra", create a https://man7.org/linux/man-pages/man7/udev.7.html[udev rule] at `/etc/udev/rules.d/80-ifname.rules` using the xref:producing-ign.adoc[Butane] config snippet shown below:

.Example: Customize NIC via Udev Rules
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/udev/rules.d/80-ifname.rules
mode: 0644
contents:
inline: |
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="12:34:56:78:9a:bc", ATTR{type}=="1", NAME="infra"
----

== Networking in the Initramfs via Kernel Arguments
If networking in the initramfs is required, the kernel argument `ifname=` will dynamically create a udev rule to change the name of a NIC.

Currently, unlike other parts of the networking config from the initramfs (e.g. static IPs, hostnames, etc.), these udev rules are not persisted into the real root. If the custom name needs to be applied to the real root, either a link file or udev rule must be created, as shown above. See https://github.com/coreos/fedora-coreos-tracker/issues/553[this issue] for more details.

For example, to give the NIC with the MAC address `12:34:56:78:9a:bc` a name of "infra", provide a `ifname=infra:12:34:56:78:9a:bc` kernel argument. A udev rule would be created in the initramfs like:
[source]
----
# cat /etc/udev/rules.d/80-ifname.rules
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="12:34:56:78:9a:bc", ATTR{type}=="1", NAME="infra"
----
= Configuring Swap on ZRAM

In Fedora 33 some editions https://www.fedoraproject.org/wiki/Releases/33/ChangeSet#swap_on_zram[enabled swap on ZRAM by default]. Fedora CoreOS currently has the `zram-generator` included but no configuration in place to enable swap on ZRAM by default. In order to configure swap on ZRAM you can lay down a configuration file via Ignition that will tell the zram generator to set up swap on top of a zram device.

The documentation for the config file format lives in the https://github.com/systemd/zram-generator/blob/main/man/zram-generator.conf.md[upstream documentation] along with a comprehensive https://github.com/systemd/zram-generator/blob/main/zram-generator.conf.example[example]. The most basic form of a configuration file that will set up a `zram0` device for swap is:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/systemd/zram-generator.conf
mode: 0644
contents:
inline: |
# This config file enables a /dev/zram0 device with the default settings
[zram0]
----

Once booted, you can verify the swap device is set up by viewing the `swapon --show` output. You can also view the true compression ratio of the currently configured zram devices by running `zramctl`.
= Configuring WireGuard

From https://www.wireguard.com/[wireguard.com]:

> WireGuard is a novel VPN that runs inside the Linux Kernel and uses state-of-the-art cryptography.
> It aims to be faster, simpler, leaner, and more useful than IPSec, while avoiding the massive headache.
> It intends to be considerably more performant than OpenVPN.
> WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances.
> It runs over UDP.

You might also want to read the https://www.wireguard.com/[Conceptual Overview], the https://www.wireguard.com/quickstart/[Quickstart] and the https://www.wireguard.com/papers/wireguard.pdf[Whitepaper].

Fedora CoreOS has full support for WireGuard out of the box.
This page shows how to set up a single connection between a Fedora CoreOS server and another computer.
It goes over the basic client configuration, but it does not cover installing WireGuard on your client.

There are two options to set up WireGuard on Fedora CoreOS:

- Importing the WireGuard configuration in NetworkManager
- Using https://www.man7.org/linux/man-pages/man8/wg-quick.8.html[`wg-quick`]

== Generating Keys

You will need to generate keys to configure WireGuard.
You can generate the keys on your workstation or a running Fedora CoreOS system.

First, let's create the WireGuard keys for the Fedora CoreOS system:

.Generate WireGuard keys for the Fedora CoreOS system
[source,bash]
----
umask 077
wg genkey | tee fcos_private_key | wg pubkey > fcos_public_key
----

Now let's generate the WireGuard keys for the client:

.Generate WireGuard keys for the client
[source,bash]
----
umask 077
wg genkey | tee client_private_key | wg pubkey > client_public_key
----

You can optionnaly generate a pre-shared key to increase security:

.Generate a preshared key for this peer pair
[source,bash]
----
wg genpsk > fcos_client_psk
----

[NOTE]
====
You should generate a pre-shared key for each peer pair.
====

== Configuring WireGuard on Fedora CoreOS

You can now modify your Butane config to create the `wg0` configuration file:

.Example Butane config with a WireGuard configuration file
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/wireguard/wg0.conf
mode: 0600
contents:
inline: |
[Interface]
Address = 192.168.71.1/24,fdc9:3c6b:21c7:e6bd::1/64
PrivateKey = <fcos_private_key>
ListenPort = 51820

[Peer]
PublicKey = <client_public_key>
PresharedKey = <fcos_client_psk>
AllowedIPs = 192.168.71.2/32,fdc9:3c6b:21c7:e6bd::2/128
----

=== Using NetworkManager

If you want to use the support in NetworkManager, you can import the WireGuard configuration with a oneshot unit:

.Example systemd service unit to import the WireGuard configuration
[source,yaml,subs="attributes"]
----
systemd:
units:
- name: import-wireguard-config.service
enabled: true
contents: |
[Unit]
ConditionPathExists=!/etc/NetworkManager/system-connections/wg0.nmconnection
Description=Import wireguard configuration to NetworkManager
[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=nmcli connection import type wireguard file /etc/wireguard/wg0.conf
[Install]
WantedBy=multi-user.target
----

[NOTE]
====
NetworkManager will ignGre `PostUp` and `PostDown` directives in the WireGuard config.
If you have firewall configuration to apply, make sure to apply it with a separate unit, or manually.
====

[NOTE]
====
If you need to make further changes to update WireGuard's configuration, delete the connection and re-import it from the updated configuration file.

.Re-import updated WireGuard configuration
[source,console]
----
$ sudo nmcli con delete wg0 && sudo nmcli con import type wireguard file /etc/wireguard/wg0.conf
Connection 'wg0' (1e4f869e-f95c-4221-b2b9-99726ffde92b) successfully deleted.
Connection 'wg0' (18cd8e61-1cc2-43a2-9f2e-467b75cd99da) successfully added.
----
====

=== Using wg-quick

If you want to use `wg-quick` instead of the support in NetworkManager, you can add the following to your Butane config:

[source,yaml,subs="attributes"]
----
systemd:
units:
- name: wg-quick@wg0.service
enabled: true
----

[NOTE]
====
If you need to make further changes to WireGuard's configuration, reload the service with:

[source,bash]
----
systemctl reload wg-quick@wg0.conf
----
====

== Verifying the configuration on the Fedora CoreOS system

Boot Fedora CoreOS and log in.
When you run `sudo wg show` you should see this:

.Check WireGuard configuration
[source,console]
----
[core@server ~]$ sudo wg show
interface: wg0
public key: <fcos_public_key>
private key: (hidden)
listening port: 51820

peer: <client_one_public_key>
preshared key: (hidden)
endpoint: <Client IP Address>:51821
allowed ips: 192.168.71.0/24, fdc9:3c6b:21c7:e6bd::/64

[core@server ~]$ sudo ip addr show wg0
12: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000
link/none
inet 192.168.71.1/24 scope global wg0
valid_lft forever preferred_lft forever
inet6 fdc9:3c6b:21c7:e6bd::1/64 scope global
valid_lft forever preferred_lft forever
----

[NOTE]
====
<Client IP address> above is the IP or FQDN of the Client computer.
====

== Configuring WireGuard on a client

You will now have to configure WireGuard on your client computer with the following configuration:

.Client WireGuard configuration
[source,ini]
----
[Interface]
Address = 192.168.71.2/24,fdc9:3c6b:21c7:e6bd::2/64
PrivateKey = <client_private_key>
ListenPort = 51821

[Peer]
PublicKey = <fcos_public_key>
PresharedKey = <fcos_client_psk>
Endpoint = <FCOS IP address>:51820
AllowedIPs = 192.168.71.0/24,fdc9:3c6b:21c7:e6bd::/64
----

[NOTE]
====
<FCOS IP address> is the IP or FQDN of the FCOS server.
====

Write the above config to `/etc/wireguard/wg0.conf`, set the access mode on the configuration file and then import the configuration on your client:

.Import the WireGuard configuration on the client
[source,console]
----
[core@client ~]$ sudo chmod 0600 /etc/wireguard/wg0.conf
[core@client ~]$ sudo nmcli con import type wireguard file /etc/wireguard/wg0.conf
----

Then check your configuration:

.Check WireGuard configuration on the client
[source,bash]
----
[core@client ~]$ sudo wg show
interface: wg0
public key: <client_one_public_key>
private key: (hidden)
listening port: 51821

peer: <fcos_public_key>
preshared key: (hidden)
endpoint: <FCOS IP address>:51820
allowed ips: 192.168.71.0/24, fdc9:3c6b:21c7:e6bd::/64

[core@client ~]$ sudo ip addr show wg0
21: wg0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000
link/none
inet 192.168.71.2/24 scope global wg0
valid_lft forever preferred_lft forever
inet6 fdc9:3c6b:21c7:e6bd::2/64 scope global
valid_lft forever preferred_lft forever
----

== Testing the WireGuard connection

You can now ping the Fedora CoreOS server's WireGuard IP address:

.Ping the Fedora CoreOS server over WireGuard from the client
[source,bash]
----
[core@client ~]$ ping 192.168.71.1
PING 192.168.71.1 (192.168.71.1) 56(84) bytes of data.
64 bytes from 192.168.71.1: icmp_seq=1 ttl=64 time=0.439 ms
64 bytes from 192.168.71.1: icmp_seq=2 ttl=64 time=0.422 ms
64 bytes from 192.168.71.1: icmp_seq=3 ttl=64 time=0.383 ms
^C
--- 192.168.71.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2027ms
rtt min/avg/max/mdev = 0.383/0.414/0.439/0.023 ms

[core@client ~]$ ping6 fdc9:3c6b:21c7:e6bd::1
PING fdc9:3c6b:21c7:e6bd::1(fdc9:3c6b:21c7:e6bd::1) 56 data bytes
64 bytes from fdc9:3c6b:21c7:e6bd::1: icmp_seq=1 ttl=64 time=1.55 ms
64 bytes from fdc9:3c6b:21c7:e6bd::1: icmp_seq=2 ttl=64 time=0.454 ms
64 bytes from fdc9:3c6b:21c7:e6bd::1: icmp_seq=3 ttl=64 time=0.424 ms
64 bytes from fdc9:3c6b:21c7:e6bd::1: icmp_seq=4 ttl=64 time=0.424 ms
^C
--- fdc9:3c6b:21c7:e6bd::1 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3054ms
rtt min/avg/max/mdev = 0.424/0.712/1.546/0.481 ms
----

When you run `sudo wg show` on your client you should see a recent handshake and a transfer sections with sent and received:

.Verify handshake and transfer metrics
[source,bash]
----
[core@client ~]$ sudo wg show
interface: wg0
public key: <client_one_public_key>
private key: (hidden)
listening port: 51821

peer: <fcos_public_key>
preshared key: (hidden)
endpoint: <Client IP address>:51820
allowed ips: 192.168.71.0/24, fdc9:3c6b:21c7:e6bd::/64
latest handshake: 9 seconds ago
transfer: 22.02 KiB received, 22.28 KiB sent
----

== Routing all traffic over WireGuard

If you plan on forwarding all of your client's traffic through the Fedora CoreOS instance, you will need to enable IP Forwarding and set some PostUp and PostDown directives:

.Example Fedora CoreOS WireGuard configuration with IP forwarding
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/sysctl.d/90-ipv4-ip-forward.conf
mode: 0644
contents:
inline: |
net.ipv4.ip_forward = 1

- path: /etc/sysctl.d/90-ipv6-ip-forwarding.conf
mode: 0644
contents:
inline: |
net.ipv6.conf.all.forwarding = 1

- path: /etc/wireguard/wg0.conf
mode: 0600
contents:
inline: |
[Interface]
Address = 192.168.71.1/24,fdc9:3c6b:21c7:e6bd::1/64
PrivateKey = <fcos_private_key>
ListenPort = 51820

PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o enp1s0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o enp1s0 -j MASQUERADE
PostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o enp1s0 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o enp1s0 -j MASQUERADE

[Peer]
PublicKey = <client_public_key>
PresharedKey = <fcos_client_psk>
AllowedIPs = 192.168.71.0/24,fdc9:3c6b:21c7:e6bd::/64

systemd:
units:
- name: wg-quick@wg0.service
enabled: true
----

[NOTE]
====
Fedora CoreOS uses https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/[predictable interface names] by https://lists.fedoraproject.org/archives/list/coreos-status@lists.fedoraproject.org/thread/6IPTZL57Z5NLBMPYMXNVSYAGLRFZBLIP/[default].
Make sure to use the correct interface name for your hardware in the above PostUp and PostDown commands.
====

Then set `AllowedIPs = 0.0.0.0/0,::/0` in `/etc/wireguard/wg0.conf` in the client configuration to route all IPv4 and IPv6 traffic on the client computer over the WireGuard interface:

.A configuration for routing all traffic on the client over WireGuard:
----
[Interface]
Address = 192.168.71.1/24,fdc9:3c6b:21c7:e6bd::2/64
PrivateKey = <client_private_key>
ListenPort = 51821

[Peer]
PublicKey = <fcos_public_key>
PresharedKey = <fcos_client_psk>
Endpoint = <FCOS IP Address>:51820
AllowedIPs = 0.0.0.0/0,::/0
----
= Modifying Kernel Arguments

== Modifying Kernel Arguments via Ignition

You can specify kernel arguments in a Butane config using the `kernel_arguments` section.

=== Example: Disabling all CPU vulnerability mitigations

Here's an example `kernelArguments` section which switches `mitigations=auto,nosmt` to `mitigations=off` to disable all CPU vulnerability mitigations:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
kernel_arguments:
should_exist:
- mitigations=off
should_not_exist:
- mitigations=auto,nosmt
----

== Modifying Console Configuration During Bare Metal Install

`coreos-installer` has special support for changing the console configuration when performing a bare-metal installation. This functionality can be used to add `console` arguments to the kernel command line and equivalent parameters to the GRUB bootloader configuration. For more information, see xref:emergency-shell.adoc[Emergency Console Access]. For more information about bare metal installs, see xref:bare-metal.adoc[Installing CoreOS on Bare Metal].

== Modifying Kernel Arguments on Existing Systems

Kernel arguments changes are managed by `rpm-ostree` via the https://www.mankier.com/1/rpm-ostree[`rpm-ostree kargs`] subcommand. Changes are applied to a new deployment and a reboot is necessary for those to take effect.

=== Adding kernel arguments

You can append kernel arguments. An empty value for an argument is allowed:

[source,bash]
----
$ sudo rpm-ostree kargs --append=KEY=VALUE
----

.Example: Add reserved memory for Kdump support

[source,bash]
----
$ sudo rpm-ostree kargs --append='crashkernel=256M'
----

See also xref:debugging-kernel-crashes.adoc[Debugging kernel crashes using kdump].

=== Removing existing kernel arguments

You can delete a specific kernel argument key/value pair or an entire argument with a single key/value pair:

[source,bash]
----
$ sudo rpm-ostree kargs --delete=KEY=VALUE
----

.Example: Re-enable SMT on vulnerable CPUs

[source,bash]
----
$ sudo rpm-ostree kargs --delete=mitigations=auto,nosmt
----

.Example: Update an existing system from cgroupsv1 to cgroupsv2 and immediately reboot

[source,bash]
----
$ sudo rpm-ostree kargs --delete=systemd.unified_cgroup_hierarchy --reboot
----

=== Replacing existing kernel arguments

You can replace an existing kernel argument with a new value. You can directly use `KEY=VALUE` if only one value exists for that argument. Otherwise, you can specify the new value using the following format:

[source,bash]
----
$ sudo rpm-ostree kargs --replace=KEY=VALUE=NEWVALUE
----

.Example: Disable all CPU vulnerability mitigations

[source,bash]
----
$ sudo rpm-ostree kargs --replace=mitigations=auto,nosmt=off
----

This switches `mitigations=auto,nosmt` to `mitigations=off` to disable all CPU vulnerability mitigations.

=== Interactive editing

To use an editor to modify the kernel arguments:

[source,bash]
----
$ sudo rpm-ostree kargs --editor
----
= Setting alternatives

Due to an https://github.com/fedora-sysv/chkconfig/issues/9[ongoing issue] in how alternatives configurations are stored on the system, Fedora CoreOS systems can not use the usual `alternatives` commands to configure them.

Instead, until this issue is resolved, you can set the symlinks directly in `/etc/alternatives`. For example, to use the legacy-based variants of the `iptables` commands:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
links:
- path: /etc/alternatives/iptables
target: /usr/sbin/iptables-legacy
overwrite: true
hard: false
- path: /etc/alternatives/iptables-restore
target: /usr/sbin/iptables-legacy-restore
overwrite: true
hard: false
- path: /etc/alternatives/iptables-save
target: /usr/sbin/iptables-legacy-save
overwrite: true
hard: false
- path: /etc/alternatives/ip6tables
target: /usr/sbin/ip6tables-legacy
overwrite: true
hard: false
- path: /etc/alternatives/ip6tables-restore
target: /usr/sbin/ip6tables-legacy-restore
overwrite: true
hard: false
- path: /etc/alternatives/ip6tables-save
target: /usr/sbin/ip6tables-legacy-save
overwrite: true
hard: false
----

== Using alternatives commands

Starting with Fedora CoreOS based on Fedora 41, you can use `alternatives` commands to configure the default command.

.Example Butane config using a systemd unit to configure the default iptables backend
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: custom-iptables-default.service
enabled: true
contents: |
[Unit]
Description=Set the default backend for iptables
[Service]
ExecStart=/usr/sbin/alternatives --set iptables /usr/sbin/iptables-legacy
RemainAfterExit=yes
[Install]
WantedBy=multi-user.target
----

NOTE: We don't recommend configuring the default iptables backend to `iptables-legacy`. This is just an example.

You can also manually run the `alternatives` commands to configure the default command runtime.

.Example to manually configure the default iptables backend
[source,bash]
----
# Check the link info
alternatives --display iptables
iptables --version

# Configure iptables to point to iptables-nft
sudo alternatives --set iptables /usr/sbin/iptables-nft

# Verify iptables version is iptables-nft
alternatives --display iptables
iptables --version
----
= Node counting

Fedora CoreOS nodes are counted by the Fedora infrastructure via the Count Me feature. This system is explicitly designed to make sure that no personally identifiable information is sent from counted systems. It also ensures that the Fedora infrastructure does not collect any personal data. The nickname for this counting mechanism is "Count Me", from the option name. Implementation details of this feature are available in https://fedoraproject.org/wiki/Changes/DNF_Better_Counting[DNF Better Counting change request for Fedora 32]. In short, the Count Me mechanism works by telling Fedora servers how old your system is (with a very large approximation).

On Fedora CoreOS nodes, this functionality is implemented in https://coreos.github.io/rpm-ostree/countme/[rpm-ostree as a stand-alone method]. The new implementation has the same privacy preserving properties as the original DNF implementation.

== Opting out of counting

You can use the following command to disable counting on existing nodes:

[source,bash]
----
$ sudo systemctl mask --now rpm-ostree-countme.timer
----

You can use the following Butane config to disable counting during provisioning on first boot:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: rpm-ostree-countme.timer
enabled: false
mask: true
----
= Configuring time zone

By default, Fedora CoreOS machines keep time in the Coordinated Universal Time (UTC) zone and synchronize their clocks with the Network Time Protocol (NTP). This page contains information about customizing the time zone.

== Viewing and changing time zone

The `timedatectl` command displays and sets the date, time, and time zone.

[source,bash]
----
$ timedatectl status
Local time: Mon 2021-05-17 20:10:20 UTC
Universal time: Mon 2021-05-17 20:10:20 UTC
RTC time: Mon 2021-05-17 20:10:20
Time zone: UTC (UTC, +0000)
System clock synchronized: yes
NTP service: active
RTC in local TZ: no
----

You can use the `list-timezones` subcommand to list the available time zones. Available time zones are represented by https://man7.org/linux/man-pages/man5/tzfile.5.html[`tzfile`] entries in the system's time zone database under `/usr/share/zoneinfo`.

[source,bash]
----
$ timedatectl list-timezones
Africa/Abidjan
Africa/Accra
Africa/Addis_Ababa

----

See the https://www.freedesktop.org/software/systemd/man/timedatectl.html[manual page] for more information about how `timedatectl` can be used; however, we do not recommend changing the time zone per-machine imperatively via SSH.

=== Recommended time zone: Coordinated Universal Time (UTC)

We recommend that all machines in Fedora CoreOS clusters use the default UTC time zone. It is strongly discouraged to set a non-UTC time zone for reasons including, but not limited to, time zone confusions, complexities of adjusting clocks for daylight savings time depending on regional customs, difficulty in correlating log files across systems, possibility of a stale time zone database, and unpredictability, as local time zones are subject to arbitrary local policies and laws.

If your applications require a different time zone, in most cases, it is possible to set a different time zone than the system one for individual applications by setting the `TZ` environment variable.

=== Setting the time zone via Ignition

If you are aware of the downsides to setting a system time zone that is different from the default UTC time zone, you can set a different system time zone by setting the local time zone configuration file, https://www.freedesktop.org/software/systemd/man/localtime.html[`/etc/localtime`], to be an absolute or relative symlink to a `tzfile` entry under `/usr/share/zoneinfo/`.
It is recommended that you set the same time zone across all your machines in the cluster.

For example, you can set the time zone to `America/New_York` by using a Butane config like the following:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
links:
- path: /etc/localtime
target: ../usr/share/zoneinfo/America/New_York
----

== Time synchronization

Fedora CoreOS uses the https://chrony.tuxfamily.org/[`chrony`] implementation of NTP, with some additional custom logic for specific clouds. For details, see the https://github.com/coreos/fedora-coreos-tracker/blob/main/internals/README-internals.md#time-synchronization[Fedora CoreOS internals documentation].
= Setting a GRUB password

You can set up a password to prevent unauthorized users from accessing the GRUB command line, modifying kernel command-line arguments, or booting non-default OSTree deployments.

== Creating the password hash

You can use `grub2-mkpasswd-pbkdf2` to create a password hash for GRUB.

[source, bash]
----
$ grub2-mkpasswd-pbkdf2
Enter password: <PASSWORD>
Reenter password: <PASSWORD>
PBKDF2 hash of your password is grub.pbkdf2.sha512.10000.5AE6255...
----

NOTE: `grub2-mkpasswd-pbkdf2` tool is a component of the `grub2-tools-minimal` package on Fedora.

== Butane config

With the password hash ready, you can now create the Butane config.

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
grub:
users:
- name: root
password_hash: grub.pbkdf2.sha512.10000.5AE6255...
----

The Butane config defines a GRUB superuser `root` and sets the password for that user using a hash.

You can now use this config to boot a Fedora CoreOS instance.
= Managing the audit daemon (`auditd`)

Starting with the first release based on Fedora 39, Fedora CoreOS includes the audit daemon (`auditd`) to load and manage audit rules.

Like all system daemons on Fedora CoreOS, the audit daemon is managed by systemd but with an exception: it cannot be stopped or restarted via `systemctl stop auditd` or `systemctl restart auditd` for compliance reasons.

From https://access.redhat.com/solutions/2664811[Unable to restart/stop auditd service using systemctl command in RHEL]:

[quote]
____
"The reason for this unusual handling of restart/stop requests is that auditd is treated specially by the kernel: the credentials of a process that sends a killing signal to auditd are saved to the audit log. The audit developers do not want to see the credentials of PID 1 logged there. They want to see the login UID of the user who initiated the action."
____

To stop and restart the audit daemon, you should use the following commands:

[source,bash]
----
$ sudo auditctl --signal stop
$ sudo systemctl start auditd.service  # Only if you want it started again
----

You may also use the following commands to reload the rules, rotate the logs, resume logging or dump the daemon state:

[source,bash]
----
$ sudo auditctl --signal reload
$ sudo auditctl --signal rotate
$ sudo auditctl --signal resume
$ sudo auditctl --signal state
----

See https://man7.org/linux/man-pages/man8/auditctl.8.html[auditctl(8)] and https://man7.org/linux/man-pages/man8/auditd.8.html[auditd(8)] for more details about those commands.
= Composefs

Fedora CoreOS introduced composefs enabled by default starting in Fedora 41.
Composefs is an overlay filesystem where the data comes from the usual ostree deployment, and metadata is in the composefs file.
The result is a truly read-only root (`/`) filesystem, increasing the system integrity and robustness.

This is a first step towards a full verification of filesystem integrity, even at runtime.

== What does it change?

The main visible change will be that the root filesystem (`/`) is now small and full (a few MB, 100% used).

== Known issues

=== Top-level directories

Another consequence is that it is now impossible to create top-level directories in `/`.
A common use case for those top level directories is to use them as mount points.
We recommend using sub directories in `/var` instead.
Currently, the only way around that is to disable composefs as shown below.

== Disable composefs

Composefs can be disabled through a kernel argument: `ostree.prepare-root.composefs=0`.

.Disabling composefs at provisioning
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
kernel_arguments:
should_exist:
- ostree.prepare-root.composefs=0
----

.Disabling composefs on a running FCOS system
[source,bash]
----
$ sudo rpm-ostree kargs --append='ostree.prepare-root.composefs=0'
----
Note that a reboot is required for the change to take effect.

== Links

https://fedoraproject.org/wiki/Changes/ComposefsAtomicCoreOSIoT[Enabling composefs by default for CoreOS and IoT]
* OS updates
= Update Streams

== Individual Update Streams

Fedora CoreOS (FCOS) has several individual update streams that are available to end users. They are:

* `stable`

** The `stable` stream is the most reliable stream offered with changes
only reaching that stream after spending a period of time in the `testing`
stream.

* `testing`

** The `testing` stream represents what is coming in the next `stable`
release. Content in this stream is updated regularly and offers our
community an opportunity to catch breaking changes before they hit
the `stable` stream.

* `next`

** The `next` stream represents the future. It will often be
used to experiment with new features and also test out rebases of our
platform on top of the next major version of Fedora. The content in
the `next` stream will also eventually filter down into `testing`
and on to `stable`.

When following a stream, a system is updated automatically when a new release is rolled out on that stream. While all streams of FCOS are automatically tested, it is strongly encouraged for users to devote a percentage of their FCOS deployment to running the `testing` and `next` streams. This ensures possible breaking changes can be caught early enough that `stable` deployments experience fewer regressions.

== Switching to a Different Stream

In order to switch between the different streams of Fedora CoreOS (FCOS) a user can leverage the `rpm-ostree rebase` command.

[TIP]
====
It may be a good idea to backup data under `/var` before switching streams.
====

[NOTE]
====
Software updates generally follow the `next` -> `testing` -> `stable` flow, meaning `next` has the newest software and `stable` has the oldest software. Upstream software components are generally tested for upgrading, not downgrading, which means that upstream software typically can handle a data/configuration migration forward (upgrade), but not backwards (downgrade). For this reason it is typically safer to rebase from `stable` -> `testing` or `testing` -> `next`, but less safe to go the other direction.
====


[NOTE]
====
Switching between streams may introduce regressions or bugs due to skipping update barriers. If you experience a regression please attempt a xref:manual-rollbacks.adoc[rollback].
====

[source,bash]
----
# Stop the service that performs automatic updates
sudo systemctl stop zincati.service

# Perform the rebase to a different stream
# Available streams: "stable", "testing", and "next"
STREAM="testing"
sudo rpm-ostree rebase "ostree-image-signed:docker://quay.io/fedora/fedora-coreos:${STREAM}"
----

After inspecting the package difference the user can reboot. After boot the system will be loaded into the latest release on the new stream and will follow that stream for future updates.
= Auto-Updates and Manual Rollbacks

Fedora CoreOS provides atomic updates and rollbacks via https://ostreedev.github.io/ostree/[OSTree] deployments.

By default, the OS performs continual auto-updates via two components:

* https://github.com/coreos/rpm-ostree[rpm-ostree] handles multiple on-disk OSTree deployments and can switch between them at boot-time.
* https://github.com/coreos/zincati[Zincati] continually checks for OS updates and applies them via rpm-ostree.

== Wariness to updates

The local Zincati agent periodically checks with a remote service to see when updates are available.
A custom "rollout wariness" value (see https://coreos.github.io/zincati/usage/auto-updates/#phased-rollouts-client-wariness-canaries[documentation]) can be provided to let the server know how eager, or how risk-averse, the node is to receiving updates.

The `rollout_wariness` parameter can be set to a floating point value between `0.0` (most eager) and `1.0` (most conservative).
In order to receive updates very early in the phased rollout cycle, a node can be configured with a low value (e.g. `0.001`).
This can be done during provisioning by using the xref:producing-ign.adoc[Butane] config snippet shown below:

.Example: configuring Zincati rollout wariness
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/zincati/config.d/51-rollout-wariness.toml
contents:
inline: |
[identity]
rollout_wariness = 0.001
----

== OS update finalization

To finalize an OS update, the machine must reboot.
As this is an invasive action which may cause service disruption, Zincati allows the cluster administrator to control when nodes are allowed to reboot for update finalization.

The following finalization strategies are available:

* As soon as the update is downloaded and staged locally, immediately reboot to apply an update.
* Use an external lock-manager to coordinate the reboot of a fleet of machines.
* Allow reboots only within configured maintenance windows, defined on a weekly UTC schedule.

A specific finalization strategy can be configured on each node.

The xref:producing-ign.adoc[Butane] snippet below shows how to define two maintenance windows during weekend days, starting at 22:30 UTC and lasting one hour each:

.Example: configuring Zincati updates strategy
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/zincati/config.d/55-updates-strategy.toml
contents:
inline: |
[updates]
strategy = "periodic"
[[updates.periodic.window]]
days = [ "Sat", "Sun" ]
start_time = "22:30"
length_minutes = 60
----
For further details on updates finalization, check the https://coreos.github.io/zincati/usage/updates-strategy/[Zincati documentation].

= Updating the bootloader

== bootupd

The bootloader update is now performed automatically by default. The https://github.com/coreos/bootupd/[bootupd]
project is included in Fedora CoreOS and supports both manual and automatic updates.

This is usually only relevant on bare metal scenarios, or virtualized
hypervisors that support Secure Boot. An example reason to update the
bootloader is for https://eclypsium.com/2020/07/29/theres-a-hole-in-the-boot/[the BootHole vulnerability].

Both the EFI system partition and BIOS MBR can be updated by bootupd.
Bootloader updates are performed automatically by `bootloader-update.service`.
Use `journalctl -u bootloader-update` to inspect its logs.

Inspect the system status:

[source,bash]
----
# bootupctl status
Component EFI
Installed: grub2-efi-x64-1:2.04-31.fc33.x86_64,shim-x64-15-8.x86_64
Update: At latest version
----

If an update is available, use `bootupctl update` to apply it manually;
the change will take effect for the next reboot.

[source,bash]
----
# bootupctl update
...
Updated: grub2-efi-x64-1:2.04-31.fc33.x86_64,shim-x64-15-8.x86_64
----

=== Using images that predate bootupd

Older CoreOS images that predate the existence of bootupd need
an explicit "adoption" phase. If `bootupctl status` says the component
is `Adoptable`, perform the adoption with `bootupctl adopt-and-update`.

[source,bash]
----
# bootupctl adopt-and-update
...
Updated: grub2-efi-x64-1:2.04-31.fc33.x86_64,shim-x64-15-8.x86_64
----
= Major Changes in Fedora CoreOS
:toc:

This is a list of major changes that were introduced in Fedora CoreOS with the
notes associated with them.
Those changes are also announced on the https://lists.fedoraproject.org/archives/list/coreos-status@lists.fedoraproject.org/[coreos-status mailing list].
Note that this is not an exhaustive list of changes in Fedora CoreOS and only includes major changes that may require manual actions.
This list is in reverse chronological order to keep recent changes at the top.

// To add a new change here, see the template at the end of the file.

== Switch to OCI updates

Starting in Fedora 42, Fedora CoreOS will be updating through OCI images instead of the OSTree repo.
Fedora Change request: https://fedoraproject.org/wiki/Changes/CoreOSOstree2OCIUpdates
More discussion: https://github.com/coreos/fedora-coreos-tracker/issues/1823

=== Planning

This change has been introduced first in new disk images for the `next` stream, as part of the Fedora 42 rebase.
The `testing` and `next` streams will follow when they rebase to Fedora 42.
After a few releases we will migrate existing machines through a barrier release.

|===
|Update Stream |Release date

|`next` | 42.20250316.1.0 (2025-03-18)
|`testing`| 42.20250705.2.0 (2025-07-06)
|`stable`| 42.20250803.3.0 (2025-08-19)
|===

=== Notes

Currently, Fedora CoreOS hosts pull updates from the OSTree repository.
With this change, the hosts will pull updates from the Quay.io container registry instead.
This should be a transparent change, although proxied environments require attention as the nodes will reach to a different address for updates.

Note: Disk images will be updated first, so new installations of Fedora CoreOS based on Fedora 42 will use OCI images.
After a few releases, we will migrate existing nodes.

This change is only scoped to switching to OCI as the transport for Fedora CoreOS content.
Derivation support is still a work in progress, see the tracking issue for more details : https://github.com/coreos/fedora-coreos-tracker/issues/1726

== cgroups v1 support disabled

In systemd v256, cgroups v1 support was disabled.
If youve opted out of the cgroups v2 migration in the past, your system will fail to boot after upgrading.
You must update your kernel arguments before the update.

.Update an existing system from cgroupsv1 to cgroupsv2 and immediately reboot

[source,bash]
----
$ sudo rpm-ostree kargs --delete=systemd.unified_cgroup_hierarchy --reboot
----

=== Planning

This change has been rolled out as part of the rebase to Fedora 41.

|===
|Update Stream |Targeted release date

|`next` | 41.20240916.1.0 (Sep 16, 2024)
|`testing`| 41.20241027.2.0 (Oct 28, 2024)
|`stable`| 41.20241027.3.0 (Nov 08, 2024)
|===

=== Notes

See: https://github.com/coreos/fedora-coreos-tracker/issues/1715#issuecomment-2331986149[fedora-coreos-tracker/issues#1715].

Detailed description of the change, the impacts, how to test, what manual actions are needed, etc.

== Podman v5.0

The Podman container runtime will be upgraded from v4 to v5. This is a https://blog.podman.io/2024/03/podman-5-0-has-been-released/[major release] that removes support for CNI networking in favor of Netavark.

See also the https://fedoraproject.org/wiki/Changes/Podman5[Fedora Change] and the https://github.com/coreos/fedora-coreos-tracker/issues/1629[tracking issue].

=== Planning

This change will be rolled out together with the rebase to Fedora 40.

|===
|Update Stream |Targeted release date

|`next` | 40.20240322.1.0 (Mar 24, 2024)
|`testing`| 40.20240416.2.0 (Apr 22, 2024)
|`stable`| 40.20240416.3.1 (May 07, 2024)
|===

=== Notes

The full release notes for Podman v5 are available https://github.com/containers/podman/releases/tag/v5.0.0[on GitHub] and the breaking changes are explained in https://blog.podman.io/2024/03/podman-5-0-breaking-changes-in-detail/[Podman 5.0 breaking changes in detail]. Here is a summary of how this will impact Fedora CoreOS nodes:

- CNI networking support has been removed and Netavark is now the only supported option.

- Pasta is now the default rootless networking backend.

- In the (unlikely) case that you were using podman machine inside of Fedora CoreOS, you will have to delete and re-create your podman machine. See https://blog.podman.io/2024/03/migration-of-podman-4-to-podman-5-machines/[Migration of Podman 4 to Podman 5 machines] for more details.

- Support for cgroups v1 is deprecated and will be removed in a future version.

- Rollbacks to a previous version with Podman v4.x will likely require manual action.

== System console changing to platform-specific defaults

The system console setup will be changed to get a better user experience by default.
The new defaults will depend on both the CPU architecture and platform.

NOTE: The changes will only affect new Fedora CoreOS installations.
Upgraded systems will retain their current console settings.

See also the https://lists.fedoraproject.org/archives/list/coreos-status@lists.fedoraproject.org/thread/GHLXX4MXNHUEAXQLK6BZN45IQYHRVQB4/[coreos-status announcement].

=== Planning

This change will be rolled out progressively:

|===
|Update Stream |Targeted release date

|`next` | 2022-10-03 (37.20221003.1.0)
|`testing`| 2022-11-28
|`stable`| Will follow `testing` as usual
|===

=== Notes

The current default depends on the CPU architecture:

- On x86_64, the first serial port `ttyS0` is the primary console and the graphical console is secondary.

- On other architectures, Fedora CoreOS generally does not configure a particular console, leaving the bootloader and kernel to follow their own defaults.
This typically means that a graphical console is used if one is available, and a serial console otherwise.

The new defaults will depend on both the CPU architecture and platform.
The exact configuration is in https://github.com/coreos/fedora-coreos-config/blob/next-devel/platforms.yaml[`platform.yaml` (next-devel branch)].
In summary:

- On many architecture/platform pairs, Fedora CoreOS will allow GRUB and the kernel to follow their own defaults.
On x86_64, this causes the graphical console to be selected, even if no video card is available.
In particular, *x86_64 bare metal installations will no longer use a serial console by default*.

- On platforms that expect specific system consoles to be used, such as AWS, Azure, and GCP, Fedora CoreOS will select those consoles by default.

- On OpenStack, VirtualBox, and VMware, Fedora CoreOS will use a primary graphical console but continue providing a serial console for debugging.

- The QEMU image will continue to select `ttyS0` as the primary console and the graphical console as secondary.

If the new defaults aren't appropriate for your environment, you can override them in several ways.
See the xref:emergency-shell.adoc[Emergency console access] documentation page for details.

== Podman v4.0

The Podman container runtime will be upgraded from v3 to v4. This is a https://podman.io/release/2022/02/22/podman-release-v4.0.0[major release] that introduces backward incompatible changes to configuration files and APIs.

See also the https://fedoraproject.org/wiki/Changes/Podman4.0[Fedora Change] and the https://github.com/coreos/fedora-coreos-tracker/issues/1106[tracking issue].

=== Planning

This change will be rolled out together with the rebase to Fedora 36.

|===
|Update Stream |Targeted release date

|`next` | 2022-03-15
|`testing`| 2022-04-19
|`stable`| Will follow `testing` as usual
|===

=== Notes

The full release notes for Podman v4 are available https://github.com/containers/podman/releases/tag/v4.0.0[on GitHub]. Here is a summary of how this will impact Fedora CoreOS nodes:

- Existing containers will be preserved without any change required.

- Compatibility for the Docker API is fully preserved.

- Users of the Podman remote API will need matching server/client versions: The Podman remote APIs for Manifest List and Network operations have been completely rewritten to address issues and inconsistencies in the previous APIs. Incompatible APIs should warn if they are used with an older Podman client. Clients and servers must thus use the same API version. This means that if you are currently using the v3 API from a client, you will need to upgrade it to v4 at the same time. If you are not using the remote API, no change is required.

- Rollbacks to a version with Podman v3.x will require manual action: Podman v4.0 will perform several schema migrations in the Podman database when it is first run. These schema migrations will cause Podman v3.x and earlier to be unable to read certain network configuration information from the database. This means that it will not be possible to roll back to a release with Podman v3.x without losing some functionality in existing containers.

- Only new installations will use the new network stack by default: Existing systems will keep using the CNI network stack with Podman v4.0. To benefit from the new network stack, you will have to remove all existing containers, images and network with the `podman system reset` command. It is recommended to reboot to apply the change.

To validate this change in advance in your deployment, you can use the following instructions to try Podman v4.0 on a node for testing purposes:

[source, bash]
----
$ cat /etc/yum.repos.d/podman4.repo
[copr:copr.fedorainfracloud.org:rhcontainerbot:podman4]
name=Copr repo for podman4 owned by rhcontainerbot
baseurl=https://download.copr.fedorainfracloud.org/results/rhcontainerbot/podman4/fedora-$releasever-$basearch/
type=rpm-md
skip_if_unavailable=True
gpgcheck=1
gpgkey=https://download.copr.fedorainfracloud.org/results/rhcontainerbot/podman4/pubkey.gpg
repo_gpgcheck=0
enabled=1
enabled_metadata=1
$ sudo rpm-ostree override replace --experimental podman containers-common catatonit --freeze --from repo=copr:copr.fedorainfracloud.org:rhcontainerbot:podman4 --install aardvark-dns --install netavark
$ sudo systemctl reboot
----

== Moving to iptables-nft

All new and upgrading Fedora CoreOS nodes will migrate to the nft backend of iptables. This will be done by updating the relevant symbolic links in `/etc/alternatives`. The legacy backend is considered deprecated.

See also the https://github.com/coreos/fedora-coreos-tracker/issues/676[tracking issue].

=== Planning

This change will be rolled out together with the rebase to Fedora 36.

|===
|Update Stream |Targeted release date

|`next` | 2022-03-15
|`testing`| 2022-04-19
|`stable`| Will follow `testing` as usual
|===

=== Notes

If you need to stay on the legacy backend, create an empty file at `/etc/coreos/iptables-legacy.stamp`. For existing nodes, you can manually create the file now:

[source, bash]
----
$ sudo mkdir -m 755 /etc/coreos/
$ sudo touch /etc/coreos/iptables-legacy.stamp
----

For new nodes that get deployed between now and when the migration happens, you can create the `/etc/coreos/iptables-legacy.stamp` file using Ignition to ensure they don't get migrated. After the migration, you can bring up new nodes on the legacy backend by manually setting the symbolic links via Ignition. Below is a Butane config that does both of these:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /etc/coreos/iptables-legacy.stamp
mode: 0644
links:
- path: /etc/alternatives/iptables
target: /usr/sbin/iptables-legacy
overwrite: true
hard: false
- path: /etc/alternatives/iptables-restore
target: /usr/sbin/iptables-legacy-restore
overwrite: true
hard: false
- path: /etc/alternatives/iptables-save
target: /usr/sbin/iptables-legacy-save
overwrite: true
hard: false
- path: /etc/alternatives/ip6tables
target: /usr/sbin/ip6tables-legacy
overwrite: true
hard: false
- path: /etc/alternatives/ip6tables-restore
target: /usr/sbin/ip6tables-legacy-restore
overwrite: true
hard: false
- path: /etc/alternatives/ip6tables-save
target: /usr/sbin/ip6tables-legacy-save
overwrite: true
hard: false
----

This will ensure that all new nodes will use the legacy backend whether before or after the migration. After all streams are based on Fedora 36, we recommend removing the stamp file from your Butane config.

////
// Template for major changes:

== Name

Short one line summary with links to tracking issues.

=== Planning

This change will be rolled out ...

|===
|Update Stream |Targeted release date

|`next` | <date>
|`testing`| <date>
|`stable`| Will follow `testing` as usual
|===

=== Notes

Detailed description of the change, the impacts, how to test, what manual actions are needed, etc.
////
* Troubleshooting
= Manual Rollbacks

When an update is complete, the previous OS deployment remains on disk. If an update causes issues, you can use it as a fallback. This is a manual operation that requires human intervention and console access.

== Temporary rollback

To temporarily boot the previous OS deployment, hold down `Shift` during the OS boot process. When the bootloader menu appears, select the relevant OS entry in the menu.

== Permanent rollback

To permanently revert to the previous OS deployment, log into the target node and run the following commands:

[source,bash]
----
# Stop the service that performs automatic updates
sudo systemctl stop zincati.service

# Mark the previous OS deployment as the default and immediately reboot into it
sudo rpm-ostree rollback -r
----

Please note that Zincati will keep looking for updates and upgrade to any new available OS deployment, other than the one you just reverted.

If you prefer, you can temporarily turn off auto-updates. Later on, you can re-enable them in order to let the machine catch up with the usual flow of updates:

[source,bash]
----
# Disable Zincati in order to opt-out from future auto-updates
sudo systemctl disable --now zincati.service

[...]

# At a later point, re-enable it to re-align with the tip of stream
sudo systemctl enable --now zincati.service
----
= Access Recovery

If you've lost the private key of an SSH key pair used to log into Fedora CoreOS, and do not have any password logins set up to use at the console, you can gain access back to the machine by booting into single user mode with the `single` kernel command-line argument:

. When booting the system, intercept the GRUB menu and edit the entry to append `single` to the kernel argument list, then press Ctrl-X to resume booting.
. Wait for the system to boot into a shell prompt
. Set or reset the password for the target user using the `passwd` utility.
. Finally, reboot the system with `/sbin/reboot -f`.

You should now be able to log back into the system at the console. From there, you can e.g. fetch a new public SSH key to add to `~/.ssh/authorized_keys` and delete the old one. You may also want to lock the password you've set (using `passwd -l`). Note that Fedora CoreOS by default does not allow SSH login via password authentication.
= Emergency console access

Sometimes you may want to access the node console to perform troubleshooting steps or emergency maintenance.
For instance, you may want to access the emergency shell on the console, in order to debug first boot provisioning issues.

== Default console configuration

All Fedora CoreOS (FCOS) images come with a default configuration for the console which is meant to accommodate most virtualized and bare-metal setups. Older FCOS releases enabled both serial and graphical consoles by default. Newer releases use different defaults for each cloud and virtualization platform, and use the kernel's defaults (typically a graphical console) on bare metal. New installs of Fedora CoreOS will switch to these new defaults starting with releases on these dates:

- `next` stream: October 3, 2022
- `testing` stream: November 28, 2022
- `stable` stream: December 12, 2022

The default consoles may not always match your specific hardware configuration. In that case, you can tweak the console setup. Fedora CoreOS has special support for doing this during xref:bare-metal.adoc[bare-metal installation], and in other cases you can xref:kernel-args.adoc[adjust kernel parameters]. Both approaches use https://www.kernel.org/doc/html/latest/admin-guide/serial-console.html[kernel argument syntax] for specifying the desired consoles. You can specify multiple consoles; kernel messages will appear on all of them, but only the last-specified device will be used as the foreground interactive console (i.e. `/dev/console`) for the machine.

== Configuring the console during bare-metal installation

If you are installing FCOS via `coreos-installer`, you can configure the console at install time.

.Example: Enabling primary serial and secondary graphical console
[source, bash]
----
sudo podman run --pull=always --privileged --rm \
-v /dev:/dev -v /run/udev:/run/udev -v .:/data -w /data \
quay.io/coreos/coreos-installer:release \
install /dev/vdb -i config.ign \
--console tty0 --console ttyS0,115200n8
----

This will configure both the GRUB bootloader and the kernel to use the specified consoles.

== Configuring the console with Ignition

If you are launching FCOS from an image (in a cloud or a virtual machine), you can use Ignition to configure the console at provisioning time.

.Example: Enabling primary serial and secondary graphical console
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
kernel_arguments:
should_exist:
# Order is significant, so group both arguments into the same list entry.
- console=tty0 console=ttyS0,115200n8
should_not_exist:
# Remove any existing defaults. Adjust as needed.
- console=hvc0
- console=tty0
- console=ttyAMA0,115200n8
- console=ttyS0,115200n8
- console=ttyS1,115200n8
----

This will configure the kernel to use the specified consoles. The GRUB bootloader will continue to use its previous default. Ignition will configure the console, then reboot into the new configuration and continue provisioning the node.

== Configuring the console after installation

You can adjust the console configuration of an existing FCOS node via `rpm-ostree`.

.Example: Enabling primary serial and secondary graphical console
[source, bash]
----
sudo rpm-ostree kargs --append=console=tty0 --append=console=ttyS0,115200n8 --reboot
----

`rpm-ostree` will create a new deployment with the specified kernel arguments added and reboot into the new configuration. The GRUB bootloader will continue to use its previous default.
= Debugging with Toolbx

The FCOS image is kept minimal by design to reduce the image size and the attack surface.
This means that it does not include every troubleshooting tool that a normal OS may include.
Instead, the recommended approach is to leverage containers with the https://containertoolbx.org/[toolbox] utility included in the image.

== What is Toolbx?

Toolbx is a utility that allows you to create privileged containers meant to debug and troubleshoot your instance.
It is a wrapper around podman which starts long running containers with default mounts and namespaces to facilitate debugging the host system.

These containers can then be used to install tools that you may need for troubleshooting.

== Using Toolbx

You can create a new toolbox by running the command below. On the first run it will ask you if you want to download an image. Answer yes with `y`.

[source,sh]
----
toolbox create my_toolbox
----

You can then list all the running toolboxes running on the host.
This should show you your newly created toolbox. In this case, it is named `my_toolbox`.

[source,sh]
----
toolbox list
----

As pointed out by the output of the `toolbox create my_toolbox` command, you can enter the following command to enter your toolbox.

[source,sh]
----
toolbox enter my_toolbox
----

Now that you're in the container, you can use the included `dnf` package manager to install packages.
For example, let's install `strace` to look at read syscall done by the host's `toolbox` utility.

[source,sh]
----
sudo dnf install strace
# Some hosts directories are mounted at /run/host
strace -eread /run/host/usr/bin/toolbox list
----

Once done with your container, you can exit the container and then remove it from the host with the following command.

[source,sh]
----
toolbox rm --force my_toolbox
----

NOTE: Toolbx allows you to create toolboxes with your custom images.
You can find more details in the https://github.com/containers/toolbox/tree/main/doc[toolbox manpages].
= Debugging kernel crashes using kdump

== Introduction
kdump is a service that creates crash dumps when there is a kernel crash. It uses https://www.mankier.com/8/kexec[`kexec(8)`] to boot into a secondary kernel (known as a capture kernel), then exports the contents of the kernel's memory (known as a crash dump or vmcore) to the filesystem. The contents of vmcore can then be analyzed to root cause the kernel crash.

Configuring kdump requires setting the `crashkernel` kernel argument and enabling the kdump systemd service. Memory must be reserved for the crash kernel during booting of the first kernel. `crashkernel=auto` generally doesn't reserve enough memory on Fedora CoreOS, so it is recommended to specify `crashkernel=300M`.

By default, the vmcore will be saved in `/var/crash`. It is also possible to write the dump to some other location on the local system or to send it over the network by editing `/etc/kdump.conf`. For additional information, see https://www.mankier.com/5/kdump.conf[`kdump.conf(5)`] and the comments in `/etc/kdump.conf` and `/etc/sysconfig/kdump`.

== Configuring kdump via Ignition
.Example kdump configuration
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
kernel_arguments:
should_exist:
- 'crashkernel=300M'
systemd:
units:
- name: kdump.service
enabled: true
----

== Configuring kdump after initial provision
. Set the crashkernel kernel argument
+
[source, bash]
----
sudo rpm-ostree kargs --append='crashkernel=300M'
----
xref:kernel-args.adoc[More information] on how to modify kargs via `rpm-ostree`.

. Enable the kdump systemd service.
+
[source, bash]
----
sudo systemctl enable kdump.service
----

. Reboot your system.
+
[source, bash]
----
sudo systemctl reboot
----

NOTE: It is highly recommended to test the configuration after setting up the `kdump` service, with extra attention to the amount of memory reserved for the crash kernel. For information on how to test that kdump is properly armed and how to analyze the dump, refer to the https://fedoraproject.org/wiki/How_to_use_kdump_to_debug_kernel_crashes[kdump documentation for Fedora] and https://www.kernel.org/doc/html/latest/admin-guide/kdump/kdump.html[the Linux kernel documentation on kdump].
= SELinux

Fedora CoreOS comes with SELinux enabled in enforcing mode.

== Policy changes

Changing policy booleans and adding SELinux modules is supported on Fedora CoreOS.
However, we do not include `semanage` and there is no sugar in Butane or direct support in Ignition for doing those operations.
See https://github.com/coreos/fedora-coreos-tracker/issues/701[fedora-coreos-tracker#701] for more details.

Here is an example to set an SELinux boolean via a systemd unit that executes on every boot:

.Example Butane config for dynamically applying SELinux boolean
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: setsebool.service
enabled: true
contents: |
[Service]
Type=oneshot
ExecStart=setsebool container_manage_cgroup true
RemainAfterExit=yes
[Install]
WantedBy=multi-user.target
----

== Resetting the SELinux policy to the default

If you encounter unexpected SELinux issue, it may be due to local policy modifications.

.Example SELinux denial
[source, text]
----
systemd-resolved[755]: Failed to symlink /run/systemd/resolve/stub-resolv.conf: Permission denied
audit[755]: AVC avc:  denied  { create } for  pid=755 comm="systemd-resolve" name=".#stub-resolv.confc418434d59d7d93a" scontext=system_u:system_r:systemd_resolved_t:s0 tcontext=system_u:object_r:systemd_resolved_var_run_t:s0 tclass=lnk_file permissive=0
----

You can check the state of the SELinux policy with:

[source,console]
----
$ sudo ostree admin config-diff | grep -E 'selinux/.*/policy'
M    selinux/targeted/active/policy.linked
M    selinux/targeted/active/policy.kern
M    selinux/targeted/policy/policy.31
A    selinux/targeted/policy/policy.30
----

If this command returns a list of entries then your SELinux policy has been modified from the default.

You have two options to reset the SELinux policy to the default one:

* Re-deploy the system from the latest Fedora CoreOS artifacts.
* Manually restore the default policy

To restore the default policy:

. Make a backup of you current policy:
+
[source,bash]
----
sudo cp -al /etc/selinux{,.bak}
----
+
. Restore the default SELinux policy:
+
[source,bash]
----
sudo rsync --archive --links --verbose --delete /usr/etc/selinux/ /etc/selinux/
----
+
After this command, the output from `sudo ostree admin config-diff | grep -E 'selinux/.*/policy'` should no longer indicate the policy is modified.
+
. Finally, reload the SELinux policy or restart your system:
+
[source,bash]
----
sudo semodule -R
----

== Disabling SELinux

We do not support disabling SELinux in Fedora CoreOS.
See https://github.com/coreos/rpm-ostree/issues/971[rpm-ostree#971].
See also the discussion in https://github.com/coreos/fedora-coreos-docs/issues/439[fedora-coreos-docs#439].

== Setting SELinux in permissive mode

We do not recommend setting the entire system in permissive mode (i.e. `set enforce 0`).
Instead, you can set SELinux to permissive for a single application by creating a https://github.com/SELinuxProject/selinux-notebook/blob/main/src/cil_overview.md[CIL policy module].

For example for the `wireguard_t` domain:

[source,console]
----
$ cat permissive-wireguard.cil
(typepermissive wireguard_t)
----

You can then load this module with:

[source,bash]
----
sudo semodule -i permissive-wireguard.cil
----

And remove it once it is no longer needed with:

[source,bash]
----
sudo semodule -r permissive-wireguard
----

See also the discussion in https://github.com/coreos/fedora-coreos-docs/issues/439[fedora-coreos-docs#439].
* Tutorials
= Prerequisites for the tutorials

The following tutorials are focused on helping you get started with Fedora CoreOS by learning how to automatically configure (or provision) an instance on first boot. Each tutorial has its roots in the previous one thus it is recommended to follow them sequentially.

If you don't know what Fedora CoreOS is, you can refer to the xref:faq.adoc[FAQ] for more information.

NOTE: If you need any help or need to ask any questions while going through those tutorials, please join the link:https://chat.fedoraproject.org/#/room/#fedora:fedoraproject.org[Matrix room], or join our https://discussion.fedoraproject.org/tag/coreos[discussion board]. If you find any issue in the tutorial, please report them in the https://github.com/coreos/fedora-coreos-docs/issues[fedora-coreos-docs issue tracker].

You should start with the setup instructions from this page as they must be completed first to be able to follow the tutorials.

* xref:tutorial-autologin.adoc[Enabling autologin and custom hostname]
** In this tutorial, you will write your first Ignition config and start a Fedora CoreOS instance with it.
* xref:tutorial-services.adoc[Starting a service on first boot]
** In this tutorial, you will learn how to start a custom script via a systemd unit on the first boot of a Fedora CoreOS instance.
* xref:tutorial-containers.adoc[SSH access and starting containers]
** In this tutorial, you will learn how to start a container at first boot with podman.
* xref:tutorial-user-systemd-unit-on-boot.adoc[Launching a user-level systemd unit on boot]
** There are times when its helpful to launch a user-level systemd unit without having to log in. This tutorial demonstrates creating a user-level systemd unit that launches on boot.
* xref:tutorial-updates.adoc[Testing Fedora CoreOS updates]
** In this tutorial, you will learn how automatic updates are handled in Fedora CoreOS and how to rollback in case of failures.

== Virtualization with `libvirt`

These tutorials are written targeting a Linux environment with a working `libvirt` setup and hardware virtualization support via `KVM`. There is, however, nothing specific to the `libvirt` environment in those tutorials and you can thus try the same configurations on any platform where you have console access (or you can skip to the SSH access tutorial to get remote access).

For instructions to set up `libvirt` and `KVM` you may refer to the https://docs.fedoraproject.org/en-US/quick-docs/getting-started-with-virtualization/[Getting started with virtualization] guide from Fedora. Although this setup guide is focused on Fedora, the tutorials should work on any distribution with `libvirt` installed and running.

== Local working directory

To keep all configuration files and Fedora CoreOS images in the same place, we will create a new directory to work from:

[source,bash]
----
mkdir ~/coreos
cd ~/coreos
----

== SSH public key

Some of the tutorials add an SSH public key to the instances to allow for SSH access as opposed to serial console access. Please place a public key in your current working directory under the filename
`ssh-key.pub`. For example, for a RSA keypair the default location would be in `~/.ssh/id_rsa.pub`:

[source,bash]
----
cp ~/.ssh/id_rsa.pub ssh-key.pub
----

== CoreOS tools

For the tutorials, we will need the following tools:

* Butane: To generate Ignition configuration from Butane config files.
* `coreos-installer`: To download the latest Fedora CoreOS QCOW2 image.
* `ignition-validate`: To validate Ignition configuration files.


=== Setup with `podman` or `docker`

All the tools required to work with Fedora CoreOS are available from containers hosted on https://quay.io/[quay.io]:

[source,bash]
----
podman pull quay.io/coreos/butane:release
podman pull quay.io/coreos/coreos-installer:release
podman pull quay.io/coreos/ignition-validate:release
----

To make it simpler to type, you may add the following aliases to your shell configuration:

[source,bash]
----
alias butane='podman run --rm --interactive         \
--security-opt label=disable          \
--volume "${PWD}:/pwd" --workdir /pwd \
quay.io/coreos/butane:release'

alias coreos-installer='podman run --pull=always              \
--rm --interactive                    \
--security-opt label=disable          \
--volume "${PWD}:/pwd" --workdir /pwd \
quay.io/coreos/coreos-installer:release'

alias ignition-validate='podman run --rm --interactive         \
--security-opt label=disable          \
--volume "${PWD}:/pwd" --workdir /pwd \
quay.io/coreos/ignition-validate:release'
----

You can then use `coreos-installer` to download the latest stable image with:

[source,bash]
----
coreos-installer download -p qemu -f qcow2.xz --decompress
----

To make the tutorial simpler, you should rename the image that we have just downloaded to a shorter name:

[source,bash,subs="attributes"]
----
mv fedora-coreos-{stable-version}-qemu.x86_64.qcow2 fedora-coreos.qcow2
----

You are now ready to proceed with the xref:tutorial-autologin.adoc[first tutorial].

=== Installing via Fedora packages

All three tools (Butane, `coreos-installer`, and `ignition-validate`) are available as Fedora packages:

[source,bash]
----
# Installing the tools
sudo dnf install -y butane coreos-installer ignition-validate

# Downloading the latest Fedora CoreOS stable QCOW2 image
coreos-installer download -p qemu -f qcow2.xz --decompress
----

To make the tutorial simpler, you should rename the image that we have just downloaded to a shorter name:

[source,bash,subs="attributes"]
----
mv fedora-coreos-{stable-version}-qemu.x86_64.qcow2 fedora-coreos.qcow2
----

You are now ready to proceed with the xref:tutorial-autologin.adoc[first tutorial].

=== Manual download

If none of the previous solutions work for you, you can still manually download Fedora CoreOS from https://fedoraproject.org/coreos/download/?stream=stable#baremetal[fedoraproject.org] with:

[source,bash,subs="attributes"]
----
RELEASE="{stable-version}"
curl -O https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/${RELEASE}/x86_64/fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz
curl -O https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/${RELEASE}/x86_64/fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz.sig
----

Once the archive has been downloaded, make sure to verify its integrity by following the instructions available by clicking on the `Verify signature & SHA256` button. You will have to download the checksum file, the signature and Fedora GPG keys to verify your download:

[source,bash]
----
curl https://fedoraproject.org/fedora.gpg | gpg --import
gpg --verify fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz.sig
----

Once you have verified the archive, you can extract it with:

[source,bash]
----
unxz fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz
----

To make the tutorial simpler, you should rename the image that we have just downloaded to a shorter name:

[source,bash,subs="attributes"]
----
mv fedora-coreos-{stable-version}-qemu.x86_64.qcow2 fedora-coreos.qcow2
----

You should then download the latest https://github.com/coreos/butane/releases[Butane] and https://github.com/coreos/ignition/releases[ignition-validate] releases from GitHub:

[source,bash,subs="attributes"]
----
# Butane
curl -OL https://github.com/coreos/butane/releases/download/v{butane-version}/butane-x86_64-unknown-linux-gnu
curl -OL https://github.com/coreos/butane/releases/download/v{butane-version}/butane-x86_64-unknown-linux-gnu.asc
gpg --verify butane-x86_64-unknown-linux-gnu.asc
mv butane-x86_64-unknown-linux-gnu butane
chmod a+x butane

# ignition-validate
curl -OL https://github.com/coreos/ignition/releases/download/v{ignition-version}/ignition-validate-x86_64-linux
curl -OL https://github.com/coreos/ignition/releases/download/v{ignition-version}/ignition-validate-x86_64-linux.asc
gpg --verify ignition-validate-x86_64-linux.asc
mv ignition-validate-x86_64-linux ignition-validate
chmod a+x ignition-validate
----

You may then set up aliases for `butane` and `ignition-validate`:

[source,bash]
----
alias butane="${PWD}/butane"
alias ignition-validate="${PWD}/ignition-validate"
----

Or move those commands to a folder in your `$PATH`, for example:

[source,bash]
----
mv butane ignition-validate "${HOME}/.local/bin/"
# Or
mv butane ignition-validate "${HOME}/bin"
----

You are now ready to proceed with the xref:tutorial-autologin.adoc[first tutorial].
= Enabling autologin and setting a custom hostname

NOTE: Make sure that you have completed the steps described in the xref:tutorial-setup.adoc[initial setup page] before starting this tutorial.

== Provisioning Fedora CoreOS

Fedora CoreOS does not have a separate install disk. Instead, every instance starts from a generic disk image which is customized on first boot via https://github.com/coreos/ignition[Ignition].

Ignition config files are written in JSON but are typically not user-friendly. Configurations are thus written in a simpler format, the Butane config, that is then converted into an Ignition config. The tool responsible for converting Butane configs into Ignition configs is called Butane.

== First Ignition config via Butane

Let's create a small Butane config that will perform the following actions:

* Add a systemd dropin to override the default `serial-getty@ttyS0.service`.
* The override will make the service automatically log the `core` user in to the serial console of the booted machine.
* Set the system hostname by dropping a file at `/etc/hostname`,
* Add a bash profile that tells systemd to not use a pager for output.

We can create a config file named `autologin.bu` now as:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: serial-getty@ttyS0.service
dropins:
- name: autologin-core.conf
contents: |
[Service]
# Override Execstart in main unit
ExecStart=
# Add new Execstart with `-` prefix to ignore failure
ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
storage:
files:
- path: /etc/hostname
mode: 0644
contents:
inline: |
tutorial
- path: /etc/profile.d/systemd-pager.sh
mode: 0644
contents:
inline: |
# Tell systemd to not use a pager when printing information
export SYSTEMD_PAGER=cat
----

This configuration can then be converted into an Ignition config with Butane:

[source,bash]
----
butane --pretty --strict autologin.bu --output autologin.ign
----

The resulting Ignition configuration produced by Butane as `autologin.ign` has the following content:

[source,json]
----
{
"ignition": {
"version": "3.4.0"
},
"storage": {
"files": [
{
"path": "/etc/hostname",
"contents": {
"compression": "",
"source": "data:,tutorial%0A"
},
"mode": 420
},
{
"path": "/etc/profile.d/systemd-pager.sh",
"contents": {
"compression": "",
"source": "data:,%23%20Tell%20systemd%20to%20not%20use%20a%20pager%20when%20printing%20information%0Aexport%20SYSTEMD_PAGER%3Dcat%0A"
},
"mode": 420
}
]
},
"systemd": {
"units": [
{
"dropins": [
{
"contents": "[Service]\n# Override Execstart in main unit\nExecStart=\n# Add new Execstart with `-` prefix to ignore failure\nExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM\n",
"name": "autologin-core.conf"
}
],
"name": "serial-getty@ttyS0.service"
}
]
}
}
----

Butane outputs valid Ignition configs. However, if you are tweaking the config after Butane, or manually creating Ignition configs, you will have to verify that the config format is valid with `ignition-validate`:

[source,bash]
----
ignition-validate autologin.ign && echo 'Success!'
----

== Booting Fedora CoreOS

Now that we have an Ignition config, we can boot a virtual machine with it. This tutorial uses the QEMU image with `libvirt`, but you should be able to use the same Ignition config on all the platforms supported by Fedora CoreOS.

We use `virt-install` to create a new Fedora CoreOS virtual machine with a specific config:

[source,bash]
----
# Setup the correct SELinux label to allow access to the config
chcon --verbose --type svirt_home_t autologin.ign

# Start a Fedora CoreOS virtual machine
virt-install --name=fcos --vcpus=2 --ram=2048 --os-variant=fedora-coreos-stable \
--import --network=bridge=virbr0 --graphics=none \
--qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${PWD}/autologin.ign" \
--disk="size=20,backing_store=${PWD}/fedora-coreos.qcow2"
----

The `virt-install` command will start an instance named `fcos` from the `fedora-coreos.qcow2` image using the `autologin.ign` Ignition config. It will auto-attach the serial console of the machine so you will be able to see the image bootup messages.

We use the `backing_store` option to `virt-install --disk` to quickly create a new disk image and avoid writing to the original image we have downloaded. This new disk image can be easily thrown away.

NOTE: Depending on your version of `virt-install`, you may not be able to use `--os-variant=fedora-coreos-stable` and will get an error. In this case, you should pick an older Fedora variant (`--os-variant=fedora31` for example). You can find the variants that are supported by you current version of `virt-install` with `osinfo-query os | grep '^\s*fedora'`.

Once the machine is booted up you should see a few prompts and then you should be automatically logged in and presented with a bash shell:

----
Fedora CoreOS 38.20230709.3.0
Kernel 6.3.11-200.fc38.x86_64 on an x86_64 (ttyS0)

SSH host key: SHA256:Eq0GiuflXh/3E+9h689DV4K2C0VQZ5UsXXfbJ7nB4rw (ECDSA)
SSH host key: SHA256:53uunBzHa2kfCO20q8h4cFeM19QRSscwUWUPoL4BP+4 (ED25519)
SSH host key: SHA256:HXrypq4OjKQ267RPhpptulMMYwsnrVWW3PYuvkIyt3k (RSA)
Ignition: ran on 2023/08/03 15:59:14 UTC (this boot)
Ignition: user-provided config was applied
No SSH authorized keys provided by Ignition or Afterburn
tutorial login: core (automatic login)

Fedora CoreOS 38.20230709.3.0
[core@tutorial ~]$
----

Let's verify that our configuration has been correctly applied. As we were automatically logged in to the terminal, we can safely assume that the systemd dropin has been created:

[source,bash]
----
[core@tutorial ~]$ systemctl cat serial-getty@ttyS0.service
# /usr/lib/systemd/system/serial-getty@.service
...

# /etc/systemd/system/serial-getty@ttyS0.service.d/autologin-core.conf
[Service]
# Override Execstart in main unit
ExecStart=
# Add new Execstart with `-` prefix to ignore failure
ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
----

We can also check that the hostname has correctly been set:

----
[core@tutorial ~]$ cat /etc/hostname
tutorial
[core@tutorial ~]$ hostnamectl
Static hostname: tutorial
Icon name: computer-vm
Chassis: vm 
Machine ID: fc4c5d5a14a741babe20559a25dcb846
Boot ID: 22ed3b3c049d42968fb6ca9e35c8055d
Virtualization: kvm
Operating System: Fedora CoreOS 38.20230709.3.0
CPE OS Name: cpe:/o:fedoraproject:fedora:38
OS Support End: Tue 2024-05-14
OS Support Remaining: 9month 1w 3d
Kernel: Linux 6.3.11-200.fc38.x86_64
Architecture: x86-64
Hardware Vendor: QEMU
Hardware Model: Standard PC _Q35 + ICH9, 2009_
Firmware Version: 1.16.2-1.fc38
Firmware Date: Tue 2014-04-01
----

== Exploring Fedora CoreOS internals

Once we have access to the console of the machine we can browse around a bit to see some of the different pieces of the operating system. For example, even though this is an OSTree based system it was still composed via RPMs and we can inspect the system to see what it was composed of:

----
[core@tutorial ~]$ rpm -q ignition kernel moby-engine podman systemd rpm-ostree zincati
ignition-2.15.0-3.fc38.x86_64
kernel-6.3.11-200.fc38.x86_64
moby-engine-20.10.23-1.fc38.x86_64
podman-4.5.1-1.fc38.x86_64
systemd-253.4-1.fc38.x86_64
rpm-ostree-2023.5-1.fc38.x86_64
zincati-0.0.25-4.fc38.x86_64
----

We can also inspect the current revision of Fedora CoreOS:

----
[core@tutorial ~]$ rpm-ostree status
State: idle
AutomaticUpdatesDriver: Zincati
DriverState: active; periodically polling for updates (last checked Thu 2023-08-03 15:59:23 UTC)
Deployments:
 fedora:fedora/x86_64/coreos/stable
Version: 38.20230709.3.0 (2023-07-24T12:25:01Z)
Commit: 552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464
----

And check on `zincati.service`, which communicates with our update server and tells `rpm-ostree` when to do an update and to what version to update to:

----
[core@tutorial ~]$ systemctl status --full zincati.service
 zincati.service - Zincati Update Agent
Loaded: loaded (/usr/lib/systemd/system/zincati.service; enabled; preset: enabled)
Drop-In: /usr/lib/systemd/system/service.d
10-timeout-abort.conf
Active: active (running) since Thu 2023-08-03 16:06:39 UTC; 18s ago
Docs: https://github.com/coreos/zincati
Main PID: 1843 (zincati)
Status: "periodically polling for updates (last checked Thu 2023-08-03 16:06:39 UTC)"
Tasks: 6 (limit: 2238)
Memory: 2.8M
CPU: 257ms
CGroup: /system.slice/zincati.service
1843 /usr/libexec/zincati agent -v

Aug 03 16:06:39 tutorial systemd[1]: Starting zincati.service - Zincati Update Agent...
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::cli::agent] starting update agent (zincati 0.0.25)
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::cincinnati] Cincinnati service: https://updates.coreos.fedoraproject.org
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::cli::agent] agent running on node '8fb5386cba574235a21ad3b2d59885d9', in update group 'default'
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::update_agent::actor] registering as the update driver for rpm-ostree
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::update_agent::actor] initialization complete, auto-updates logic enabled
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::strategy] update strategy: immediate
Aug 03 16:06:39 tutorial systemd[1]: Started zincati.service - Zincati Update Agent.
Aug 03 16:06:39 tutorial zincati[1843]: [INFO  zincati::update_agent::actor] reached steady state, periodically polling for updates
Aug 03 16:06:41 tutorial zincati[1843]: [INFO  zincati::cincinnati] current release detected as not a dead-end
----

One other interesting thing to do is view the logs from Ignition in case there is anything interesting there we may want to investigate:

----
journalctl -t ignition
----

And finally, of course we can use the `podman` (or `docker`) command to inspect the current state of containers on the system:

----
podman version
podman info
----

NOTE: `podman` commands can be run as root or as non-root user. `docker` commands need to be run as root via `sudo` unless the user has been added to the `docker` group.

NOTE: Running containers via `docker` and `podman` at the same time can cause issues and result in unexpected behaviour. Refer to the https://docs.fedoraproject.org/en-US/fedora-coreos/faq/#_can_i_run_containers_via_docker_and_podman_at_the_same_time[FAQ Entry] for more details.

NOTE: The Docker daemon is not started by default but running any `docker` command will start it as it is socket activated via systemd.

== Taking down the Virtual Machine

Let's now get rid of that virtual machine so we can start again from scratch. First escape out of the serial console by pressing `CTRL + ]` and then type:

----
virsh destroy fcos
virsh undefine --remove-all-storage fcos
----

You may now proceed with the xref:tutorial-services.adoc[second tutorial].
= Starting a script on first boot via a systemd service

NOTE: Make sure that you have completed the steps described in the xref:tutorial-setup.adoc[initial setup page] before starting this tutorial.

In this tutorial, we will run a script on the first boot via a systemd service. We will add the following to the Butane config from the previous scenario:

* Add a script at `/usr/local/bin/public-ipv4.sh`.
* Configure a systemd service to run the script on first boot.

== Writing the script

Let's write a small script that uses https://icanhazip.com/[icanhazip.com] to create an issue file to display as a prelogin message on the console and store it in `public-ipv4.sh`.

NOTE: This is only an example to show how to run a service on boot. Do not use this if you don't trust the owners of https://icanhazip.com/[icanhazip.com].

[source,bash]
----
cat <<'EOF' > public-ipv4.sh
#!/bin/bash
echo "Detected Public IPv4: is $(curl ipv4.icanhazip.com)" >  /etc/issue.d/50_public-ipv4.issue
EOF
----

This could be useful in cloud environments where you might have different public and private addresses.

We will store this script into `/usr/local/bin/public-ipv4.sh` when we provision the machine.

== Writing the systemd service

We need to call the script from the previous section by using a systemd unit. Let's write a systemd unit into the `issuegen-public-ipv4.service` file that does what we want, which is to execute on first boot and not again:

[source,bash]
----
cat <<'EOF' > issuegen-public-ipv4.service
[Unit]
Before=systemd-user-sessions.service
Wants=network-online.target
After=network-online.target
ConditionPathExists=!/var/lib/issuegen-public-ipv4

[Service]
Type=oneshot
ExecStart=/usr/local/bin/public-ipv4.sh
ExecStartPost=/usr/bin/touch /var/lib/issuegen-public-ipv4
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF
----

== Writing the Butane config and converting to Ignition

We can now create a Butane config that will include the script and systemd unit file contents by picking up the local `public-ipv4.sh` and `issuegen-public-ipv4.service` files using local file references. The final Butane config, stored in `services.bu`, will be:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: serial-getty@ttyS0.service
dropins:
- name: autologin-core.conf
contents: |
[Service]
# Override Execstart in main unit
ExecStart=
# Add new Execstart with `-` prefix to ignore failure
ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
- name: issuegen-public-ipv4.service
enabled: true
contents_local: issuegen-public-ipv4.service
storage:
files:
- path: /etc/hostname
mode: 0644
contents:
inline: |
tutorial
- path: /etc/profile.d/systemd-pager.sh
mode: 0644
contents:
inline: |
# Tell systemd to not use a pager when printing information
export SYSTEMD_PAGER=cat
- path: /usr/local/bin/public-ipv4.sh
mode: 0755
contents:
local: public-ipv4.sh
----

NOTE: Check the Butane https://coreos.github.io/butane/examples/[Examples] and https://coreos.github.io/butane/specs/[Configuration specifications] for more details about local file includes.

With the files `public-ipv4.sh`, `issuegen-public-ipv4.service`, and `services.bu` in the current working directory we can now convert to Ignition:

[source,bash]
----
butane --pretty --strict --files-dir=./ services.bu --output services.ign
----

== Testing

Just as before we will use the following to boot the instance:

[source,bash]
----
# Setup the correct SELinux label to allow access to the config
chcon --verbose --type svirt_home_t services.ign

# Start a Fedora CoreOS virtual machine
virt-install --name=fcos --vcpus=2 --ram=2048 --os-variant=fedora-coreos-stable \
--import --network=bridge=virbr0 --graphics=none \
--qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${PWD}/services.ign" \
--disk="size=20,backing_store=${PWD}/fedora-coreos.qcow2"
----

And view on the console that the `Detected Public IPv4` is shown in the console output right before you are dropped to a login prompt:

----
Fedora CoreOS 38.20230709.3.0
Kernel 6.3.11-200.fc38.x86_64 on an x86_64 (ttyS0)

SSH host key: SHA256:tYHKk26+NZ/+ZytWLXClGz813PQJDGP/2+AiuZ8fiqk (ECDSA)
SSH host key: SHA256:jJASZec/91zXd4or0uiFsvsfaLC6viLronfxIwQlNCs (ED25519)
SSH host key: SHA256:2XlSZAehEu666fmXeM8d47lpIJd92MBOqgMazT4GsVw (RSA)
enp1s0: 192.168.124.150 fe80::475a:7a10:2302:b670
Ignition: ran on 2023/08/03 16:40:45 UTC (this boot)
Ignition: user-provided config was applied
No SSH authorized keys provided by Ignition or Afterburn
Detected Public IPv4: is 3.252.102.80
tutorial login: core (automatic login)

Fedora CoreOS 38.20230709.3.0
[core@tutorial ~]$
----

And the service shows it was launched successfully:

----
[core@tutorial ~]$ systemctl status --full issuegen-public-ipv4.service
 issuegen-public-ipv4.service
Loaded: loaded (/etc/systemd/system/issuegen-public-ipv4.service; enabled; preset: enabled)
Drop-In: /usr/lib/systemd/system/service.d
10-timeout-abort.conf
Active: active (exited) since Thu 2023-08-03 16:40:55 UTC; 1min 7s ago
Process: 1423 ExecStart=/usr/local/bin/public-ipv4.sh (code=exited, status=0/SUCCESS)
Process: 1460 ExecStartPost=/usr/bin/touch /var/lib/issuegen-public-ipv4 (code=exited, status=0/SUCCESS)
Main PID: 1423 (code=exited, status=0/SUCCESS)
CPU: 84ms

Aug 03 16:40:55 tutorial systemd[1]: Starting issuegen-public-ipv4.service...
Aug 03 16:40:55 tutorial public-ipv4.sh[1424]:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Aug 03 16:40:55 tutorial public-ipv4.sh[1424]:                                  Dload  Upload   Total   Spent    Left  Speed
Aug 03 16:40:55 tutorial public-ipv4.sh[1424]: [158B blob data]
Aug 03 16:40:55 tutorial systemd[1]: Finished issuegen-public-ipv4.service.
----

== Cleanup

Now let's take down the instance for the next test. First, disconnect from the serial console by pressing `CTRL` + `]` and then destroy the machine:

----
virsh destroy fcos
virsh undefine --remove-all-storage fcos
----

You may now proceed with the xref:tutorial-containers.adoc[next tutorial].
= Setting up SSH access and starting containers at boot

NOTE: Complete all the steps described in the xref:tutorial-setup.adoc[initial setup page] before starting this tutorial. Make sure you have created the file `ssh-key.pub` following the instructions provided in the https://docs.fedoraproject.org/en-US/fedora-coreos/tutorial-setup/#_ssh_public_key[prerequisites] for the tutorial. We will use this key in the Butane configuration file that we are about to write.

In this tutorial, we will set up SSH access and start a container at boot. Fedora CoreOS is focused on running applications/services in containers thus we recommend trying to run containers and avoid modifying the host directly. Running containers and keeping a pristine host layer makes automatic updates more reliable and allows for separation of concerns with the Fedora CoreOS team responsible for the OS and end-user operators/sysadmins responsible for the applications.

As usual, we will set up console autologin, a hostname, systemd pager configuration, but we will also:

* Add an SSH Key for the `core` user from the local `ssh-key.pub` file.
* Add a systemd service (`failure.service`) that fails on boot.
* Add a running container via a Podman https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html[quadlet systemd.unit container file].
* This `etcd-member.container` will then be associated with a `etcd-member.service` on the running system.
* `etcd-member.service` will launch and manage the lifecycle of the container using `podman`.

== Writing the Butane config and converting to Ignition

Similarly to what we did in the second provisioning scenario, we will write the following Butane config in a file called `containers.bu`:

.Example with automatic serial console login, SSH key, and multiple systemd units
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys_local:
- ssh-key.pub
systemd:
units:
- name: serial-getty@ttyS0.service
dropins:
- name: autologin-core.conf
contents: |
[Service]
# Override Execstart in main unit
ExecStart=
# Add new Execstart with `-` prefix to ignore failure
ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
TTYVTDisallocate=no
- name: failure.service
enabled: true
contents: |
[Service]
Type=oneshot
ExecStart=/usr/bin/false
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
storage:
files:
- path: /etc/hostname
mode: 0644
contents:
inline: |
tutorial
- path: /etc/profile.d/systemd-pager.sh
mode: 0644
contents:
inline: |
# Tell systemd to not use a pager when printing information
export SYSTEMD_PAGER=cat
- path: /etc/containers/systemd/etcd-member.container
mode: 0644
contents:
inline: |
[Unit]
Description=Run a single node etcd
After=network-online.target
Wants=network-online.target

[Container]
ContainerName=etcd
Image=quay.io/coreos/etcd:v3.5.21
Network=host
Volume=etcd-data:/etcd-data
Exec=/usr/local/bin/etcd                                \
--name node1 --data-dir /etcd-data                  \
--initial-advertise-peer-urls http://127.0.0.1:2380 \
--listen-peer-urls http://127.0.0.1:2380            \
--advertise-client-urls http://127.0.0.1:2379       \
--listen-client-urls http://127.0.0.1:2379          \
--initial-cluster node1=http://127.0.0.1:2380

[Install]
WantedBy=multi-user.target
----

Run Butane to convert that to an Ignition config:

[source,bash]
----
butane --pretty --strict --files-dir=./ containers.bu --output containers.ign
----

Now let's provision it:

[source,bash]
----
# Setup the correct SELinux label to allow access to the config
chcon --verbose --type svirt_home_t containers.ign

# Start a Fedora CoreOS virtual machine
virt-install --name=fcos --vcpus=2 --ram=2048 --os-variant=fedora-coreos-stable \
--import --network=bridge=virbr0 --graphics=none \
--qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${PWD}/containers.ign" \
--disk="size=20,backing_store=${PWD}/fedora-coreos.qcow2"
----

On the console you will see:

----
Fedora CoreOS 38.20230709.3.0
Kernel 6.3.11-200.fc38.x86_64 on an x86_64 (ttyS0)

SSH host key: SHA256:T5V4oXMZ0UJ7WRGzNiUOkggO7p5yojTVBUxa6N3vIoQ (ECDSA)
SSH host key: SHA256:oBAvj2kaKKKK++gnchTbxpp/iphvX6EHr0EynwXZ19c (ED25519)
SSH host key: SHA256:Yg2fdA7GC1eoHtIjawDA+WffTKTuNy5ZhQHUJx5GRHk (RSA)
enp1s0: 192.168.124.119 fe80::9b5c:330d:2020:1c9e
Ignition: ran on 2023/08/03 18:17:45 UTC (this boot)
Ignition: user-provided config was applied
Ignition: wrote ssh authorized keys file for user: core
tutorial login: core (automatic login)

Fedora CoreOS 38.20230709.3.0
[systemd]
Failed Units: 1
failure.service
[core@tutorial ~]$
----

If you would like to connect via SSH, disconnect from the serial console by pressing `CTRL` + `]` and then use the reported IP address for the NIC from the serial console to log in using the `core` user via SSH:

----
$ ssh core@192.168.124.119
The authenticity of host '192.168.124.119 (192.168.124.119)' can't be established.
ED25519 key fingerprint is SHA256:oBAvj2kaKKKK++gnchTbxpp/iphvX6EHr0EynwXZ19c.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '192.168.124.119' (ED25519) to the list of known hosts.
Fedora CoreOS 38.20230709.3.0
Tracker: https://github.com/coreos/fedora-coreos-tracker
Discuss: https://discussion.fedoraproject.org/tag/coreos

Last login: Thu Aug  3 18:18:06 2023
[systemd]
Failed Units: 1
failure.service
----

The `Failed Units` message is coming from the https://github.com/coreos/console-login-helper-messages[console login helper messages] helpers. This particular helper shows us when `systemd` has services that are in a failed state. In this case we made `failure.service` with `ExecStart=/usr/bin/false`, so we intentionally created a service that will always fail in order to illustrate the helper messages.

Now that were up and dont have any real failures we can check out the status of `etcd-member.service`, which was generated from our `etcd-member.container` file.

----
[core@tutorial ~]$ systemctl status --full etcd-member.service
 etcd-member.service - Run a single node etcd
Loaded: loaded (/etc/containers/systemd/etcd-member.container; generated)
Drop-In: /usr/lib/systemd/system/service.d
10-timeout-abort.conf
Active: active (running) since Thu 2023-08-03 18:17:57 UTC; 2min 24s ago
Main PID: 1553 (conmon)
Tasks: 10 (limit: 2238)
Memory: 86.5M
CPU: 3.129s
CGroup: /system.slice/etcd-member.service
libpod-payload-31af97b0ef902b3b3b3d717bd98947b209701b9585db2129ca53f4b33962415e
 1555 /usr/local/bin/etcd ...
runtime
1553 /usr/bin/conmon ...

Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.745207 I | raft: b71f75320dc06a6c became candidate at term 2
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.745372 I | raft: b71f75320dc06a6c received MsgVoteResp from b71f75320dc06a6c at term 2
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.745499 I | raft: b71f75320dc06a6c became leader at term 2
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.745628 I | raft: raft.node: b71f75320dc06a6c elected leader b71f75320dc06a6c at term 2
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.746402 I | etcdserver: setting up the initial cluster version to 3.3
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.747906 N | etcdserver/membership: set the initial cluster version to 3.3
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.748211 I | etcdserver/api: enabled capabilities for version 3.3
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.748384 I | etcdserver: published {Name:node1 ClientURLs:[http://127.0.0.1:2379]} to cluster 1c45a069f3a1d796
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.748510 I | embed: ready to serve client requests
Aug 03 18:17:58 tutorial etcd[1553]: 2023-08-03 18:17:58.750778 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!
----

We can also inspect the state of the container that was run by the systemd service:

----
[core@tutorial ~]$ sudo podman ps -a
CONTAINER ID  IMAGE                       COMMAND               CREATED        STATUS        PORTS       NAMES
31af97b0ef90  quay.io/coreos/etcd:latest  /usr/local/bin/et...  4 minutes ago  Up 4 minutes              etcd
----

And we can set a key/value pair in etcd. For now lets set the key `fedora` to the value `fun`:

----
[core@tutorial ~]$ curl -L http://127.0.0.1:2379/v3/kv/put -X POST -d '{"key": "ZmVkb3Jh", "value": "ZnVu"}'
{"header":{"cluster_id":"2037210783374497686","member_id":"13195394291058371180","revision":"4","raft_term":"2"}}
[core@tutorial ~]$ curl -sL http://127.0.0.1:2379/v3/kv/range -X POST -d '{"key": "AA==", "range_end": "AA=="}' | jq
{
"header": {
"cluster_id": "2037210783374497686",
"member_id": "13195394291058371180",
"revision": "4",
"raft_term": "2"
},
"kvs": [
{
"key": "ZmVkb3Jh",
"create_revision": "2",
"mod_revision": "4",
"version": "2",
"value": "ZnVu"
}
],
"count": "2"
}
----

Looks like everything is working!

== Cleanup

Now let's take down the instance for the next test. Disconnect from the serial console by pressing `CTRL` + `]` or from SSH and then destroy the machine:

----
virsh destroy fcos
virsh undefine --remove-all-storage fcos
----

You may now proceed with the xref:tutorial-user-systemd-unit-on-boot.adoc[next tutorial].
= Launching a user-level systemd unit on boot

NOTE: Complete all the steps described in the xref:tutorial-setup.adoc[initial setup page] before starting this tutorial. Make sure you have created the file `ssh-key.pub` following the instructions provided in the https://docs.fedoraproject.org/en-US/fedora-coreos/tutorial-setup/#_ssh_public_key[prerequisites] for the tutorial. We will use this key in the Butane configuration file that we are about to write.

In this tutorial, we will set up a user level systemd unit for an unprivileged user. There are times when it's helpful to launch a user-level https://www.freedesktop.org/software/systemd/man/systemd.unit.html[systemd unit] without having to log in. For example, you may wish to launch a container that provides a network service or run an HPC job. For this setup, we will add the following to a Butane config:

* A user level systemd unit: `/home/sleeper/.config/systemd/user/linger-example.service`.
* Enable it as a user level systemd service.

== Setting up the systemd unit

In this example, we will launch a systemd service for the user `sleeper`. First, let's create a user:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: sleeper
----

This will also create the home directory for the `sleeper` user. Then we can add the systemd unit:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /home/sleeper/.config/systemd/user/linger-example.service
mode: 0644
contents:
inline: |
[Unit]
Description=A systemd user unit demo
[Service]
ExecStart=/usr/bin/sleep infinity
user:
name: sleeper
group:
name: sleeper
----

System services can be directly enabled in Butane configs but user level services have to be manually enabled for now:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
directories:
- path: /home/sleeper/.config/systemd/user/default.target.wants
mode: 0755
user:
name: sleeper
group:
name: sleeper
links:
- path: /home/sleeper/.config/systemd/user/default.target.wants/linger-example.service
user:
name: sleeper
group:
name: sleeper
target: /home/sleeper/.config/systemd/user/linger-example.service
hard: false
----

We set up lingering for the systemd user level instance so that it gets started directly on boot and stays running:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
files:
- path: /var/lib/systemd/linger/sleeper
mode: 0644
----

As the following directories do not exist yet, we will have to create them to tell Ignition to set the right ownership and permissions:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
storage:
directories:
- path: /home/sleeper/.config
mode: 0755
user:
name: sleeper
group:
name: sleeper
- path: /home/sleeper/.config/systemd
mode: 0755
user:
name: sleeper
group:
name: sleeper
- path: /home/sleeper/.config/systemd/user
mode: 0755
user:
name: sleeper
group:
name: sleeper
- path: /home/sleeper/.config/systemd/user/default.target.wants
mode: 0755
user:
name: sleeper
group:
name: sleeper
----

== Writing the Butane config and converting to Ignition

The final Butane config, stored in `user.bu`, will be:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys_local:
- ssh-key.pub
- name: sleeper
storage:
directories:
- path: /home/sleeper/.config
mode: 0755
user:
name: sleeper
group:
name: sleeper
- path: /home/sleeper/.config/systemd
mode: 0755
user:
name: sleeper
group:
name: sleeper
- path: /home/sleeper/.config/systemd/user
mode: 0755
user:
name: sleeper
group:
name: sleeper
- path: /home/sleeper/.config/systemd/user/default.target.wants
mode: 0755
user:
name: sleeper
group:
name: sleeper
files:
- path: /var/lib/systemd/linger/sleeper
mode: 0644
- path: /home/sleeper/.config/systemd/user/linger-example.service
mode: 0644
contents:
inline: |
[Unit]
Description=A systemd user unit demo
[Service]
ExecStart=/usr/bin/sleep infinity
user:
name: sleeper
group:
name: sleeper
links:
- path: /home/sleeper/.config/systemd/user/default.target.wants/linger-example.service
user:
name: sleeper
group:
name: sleeper
target: /home/sleeper/.config/systemd/user/linger-example.service
hard: false
----

This config can be converted to Ignition:

[source,bash]
----
butane --pretty --strict --files-dir=./ user.bu --output user.ign
----

== Testing

Just as before we will use the following to boot the instance:

[source,bash]
----
# Setup the correct SELinux label to allow access to the config
chcon --verbose --type svirt_home_t user.ign

# Start a Fedora CoreOS virtual machine
virt-install --name=fcos --vcpus=2 --ram=2048 --os-variant=fedora-coreos-stable \
--import --network=bridge=virbr0 --graphics=none \
--qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${PWD}/user.ign" \
--disk="size=20,backing_store=${PWD}/fedora-coreos.qcow2"
----

We can then verify that the unit has been started under the sleeper systemd user instance:

[source,bash]
----
[core@localhost ~]$ sudo machinectl shell sleeper@
Connected to the local host. Press ^] three times within 1s to exit session.
[sleeper@localhost ~]$ systemctl --user status
 localhost.localdomain
State: running
Units: 157 loaded (incl. loaded aliases)
Jobs: 0 queued
Failed: 0 units
Since: Thu 2023-08-03 18:31:27 UTC; 23s ago
systemd: 253.4-1.fc38
CGroup: /user.slice/user-1001.slice/user@1001.service
app.slice
 linger-example.service
   1589 /usr/bin/sleep infinity
init.scope
1489 /usr/lib/systemd/systemd --user
1496 "(sd-pam)"
[sleeper@localhost ~]$ systemctl --user status linger-example.service
 linger-example.service - A systemd user unit demo
Loaded: loaded (/var/home/sleeper/.config/systemd/user/linger-example.service; enabled; preset: disabled)
Drop-In: /usr/lib/systemd/user/service.d
10-timeout-abort.conf
Active: active (running) since Thu 2023-08-03 18:31:27 UTC; 38s ago
Main PID: 1589 (sleep)
Tasks: 1 (limit: 2238)
Memory: 224.0K
CPU: 1ms
CGroup: /user.slice/user-1001.slice/user@1001.service/app.slice/linger-example.service
1589 /usr/bin/sleep infinity

Aug 03 18:31:27 localhost.localdomain systemd[1489]: Started linger-example.service - A systemd user unit demo.
----

== Cleanup

You can then take down the instance. First, disconnect from the serial console by pressing `CTRL` + `]` and then destroy the machine:

----
virsh destroy fcos
virsh undefine --remove-all-storage fcos
----

You may now proceed with the xref:tutorial-updates.adoc[next tutorial].
= Testing Fedora CoreOS updates

NOTE: Make sure that you have completed the steps described in the xref:tutorial-setup.adoc[initial setup page] before starting this tutorial.

In this tutorial, we will not focus on provisioning but on what happens during updates and the options that are available in case of failures.

== Downloading an older Fedora CoreOS release

One of the defining features of Fedora CoreOS is automatic updates. To see them in action, we have to download an older Fedora CoreOS release. In this case we'll boot the `N-1` release of Fedora CoreOS, which can be seen from the https://fedoraproject.org/coreos/release-notes/[Fedora CoreOS release page] or by using the `releases.json` metadata:

[source,bash]
----
RELEASE=$(curl https://builds.coreos.fedoraproject.org/prod/streams/stable/releases.json | jq -r '.releases[-2].version')
curl -O https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/${RELEASE}/x86_64/fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz
curl -O https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/${RELEASE}/x86_64/fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz.sig
----

Once the archive has been downloaded, make sure to verify its integrity:

[source,bash]
----
curl https://fedoraproject.org/fedora.gpg | gpg --import
gpg --verify fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz.sig
----

TIP: Look for *"Good signature from"* in the output.

Once you have verified the archive, you can extract it with:

[source,bash]
----
unxz fedora-coreos-${RELEASE}-qemu.x86_64.qcow2.xz
----

To make the tutorial simpler, you should rename this image to a shorter name:

[source,bash]
----
mv fedora-coreos-${RELEASE}-qemu.x86_64.qcow2 fedora-coreos-older.qcow2
----

== Writing the Butane config and converting to Ignition

We will create a Butane config that will:

* Set up console autologin.
* Add an SSH Key for the `core` user from the local `ssh-key.pub` file.

Let's write this Butane config to a file called `updates.bu`:

[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
ssh_authorized_keys_local:
- ssh-key.pub
systemd:
units:
- name: serial-getty@ttyS0.service
dropins:
- name: autologin-core.conf
contents: |
[Service]
# Override Execstart in main unit
ExecStart=
# Add new Execstart with `-` prefix to ignore failure
ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
TTYVTDisallocate=no
storage:
files:
- path: /etc/profile.d/systemd-pager.sh
mode: 0644
contents:
inline: |
# Tell systemd to not use a pager when printing information
export SYSTEMD_PAGER=cat
----

TIP: Optionally you can replace the SSH pubkey in the yaml file with your own public key so you can log in to the booted instance. If you choose not to do this you'll still be auto logged in to the serial console.

Run Butane to convert that to an Ignition config:

[source,bash]
----
butane --pretty --strict --files-dir=./ updates.bu --output updates.ign
----

== Startup and initial update

Now let's provision it. Make sure that you are starting from the older Fedora CoreOS image in this step:

[source, bash]
----
# Setup the correct SELinux label to allow access to the config
chcon --verbose --type svirt_home_t updates.ign

# Start a Fedora CoreOS virtual machine
virt-install --name=fcos --vcpus=2 --ram=2048 --os-variant=fedora-coreos-stable \
--import --network=bridge=virbr0 --graphics=none \
--qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${PWD}/updates.ign" \
--disk="size=20,backing_store=${PWD}/fedora-coreos-older.qcow2"
----

Disconnect from the serial console by pressing `CTRL` + `]` and then use the reported IP address for the NIC from the serial console to log in using the `core` user via SSH:

As the system is not up to date, Zincati will notice this and will start updating the system. You should see the update process happening right away:

NOTE: All necessary network services may not be up and running during the initial check. In such a case, Zincati will check for updates again in about 5 minutes.

----
[core@localhost ~]$ systemctl status --full zincati.service
 zincati.service - Zincati Update Agent
Loaded: loaded (/usr/lib/systemd/system/zincati.service; enabled; preset: enabled)
Drop-In: /usr/lib/systemd/system/service.d
10-timeout-abort.conf
Active: active (running) since Thu 2023-08-03 19:52:41 UTC; 10s ago
Docs: https://github.com/coreos/zincati
Main PID: 1816 (zincati)
Status: "found update on remote: 38.20230709.3.0"
Tasks: 12 (limit: 2239)
Memory: 6.8M
CPU: 273ms
CGroup: /system.slice/zincati.service
1816 /usr/libexec/zincati agent -v
1873 rpm-ostree deploy --lock-finalization --skip-branch-check revision=552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9 --disallow-downgrade

Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::cli::agent] starting update agent (zincati 0.0.25)
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::cincinnati] Cincinnati service: https://updates.coreos.fedoraproject.org
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::cli::agent] agent running on node '87c9ec3e0a4045a19b74b54ae5ed986a', in update group 'default'
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] registering as the update driver for rpm-ostree
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] initialization complete, auto-updates logic enabled
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::strategy] update strategy: immediate
Aug 03 19:52:41 localhost.localdomain systemd[1]: Started zincati.service - Zincati Update Agent.
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] reached steady state, periodically polling for updates
Aug 03 19:52:42 localhost.localdomain zincati[1816]: [INFO  zincati::cincinnati] current release detected as not a dead-end
Aug 03 19:52:43 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] target release '38.20230709.3.0' selected, proceeding to stage it

[core@localhost ~]$ rpm-ostree status
State: idle
AutomaticUpdatesDriver: Zincati
DriverState: active; update staged: 38.20230709.3.0; reboot delayed due to active user sessions
Deployments:
fedora:fedora/x86_64/coreos/stable
Version: 38.20230709.3.0 (2023-07-24T12:25:01Z)
Commit: 552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464
Diff: 61 upgraded

 fedora:fedora/x86_64/coreos/stable
Version: 38.20230625.3.0 (2023-07-11T11:57:53Z)
Commit: e841d77aadb875bb801ac845a0d9b8a70b4224bdeb15e7d6c5bff1da932c0301
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464

[core@localhost ~]$ systemctl status --full zincati.service
 zincati.service - Zincati Update Agent
Loaded: loaded (/usr/lib/systemd/system/zincati.service; enabled; preset: enabled)
Drop-In: /usr/lib/systemd/system/service.d
10-timeout-abort.conf
Active: active (running) since Thu 2023-08-03 19:52:41 UTC; 1min 21s ago
Docs: https://github.com/coreos/zincati
Main PID: 1816 (zincati)
Status: "update staged: 38.20230709.3.0; reboot delayed due to active user sessions"
Tasks: 6 (limit: 2239)
Memory: 3.2M
CPU: 362ms
CGroup: /system.slice/zincati.service
1816 /usr/libexec/zincati agent -v

Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::cli::agent] agent running on node '87c9ec3e0a4045a19b74b54ae5ed986a', in update group 'default'
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] registering as the update driver for rpm-ostree
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] initialization complete, auto-updates logic enabled
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::strategy] update strategy: immediate
Aug 03 19:52:41 localhost.localdomain systemd[1]: Started zincati.service - Zincati Update Agent.
Aug 03 19:52:41 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] reached steady state, periodically polling for updates
Aug 03 19:52:42 localhost.localdomain zincati[1816]: [INFO  zincati::cincinnati] current release detected as not a dead-end
Aug 03 19:52:43 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] target release '38.20230709.3.0' selected, proceeding to stage it
Aug 03 19:52:55 localhost.localdomain zincati[1816]: [INFO  zincati::update_agent::actor] update staged: 38.20230709.3.0
Aug 03 19:52:55 localhost.localdomain zincati[1816]: [WARN  zincati::update_agent] interactive sessions detected, entering grace period (maximum 10 minutes)
----

Shortly after the update has been staged the system will reboot by default. However if it detects active user sessions it will wait and show a message to the user:

----
Broadcast message from Zincati at Thu 2023-08-03 19:52:55 UTC:
New update 38.20230709.3.0 is available and has been deployed.
If permitted by the update strategy, Zincati will reboot into this update when
all interactive users have logged out, or in 10 minutes, whichever comes
earlier. Please log out of all active sessions in order to let the auto-update
process continue.
----

Once the user sessions have ceased the reboot will continue. In this case we need to stop the autologin on ttyS0 service and log out of SSH as well:

----
[core@localhost ~]$ sudo systemctl stop serial-getty@ttyS0.service
[core@localhost ~]$ exit
logout
Connection to 192.168.124.222 closed.
----

TIP: You can monitor the serial console to see when the machine has performed the reboot via `virsh console fcos`.

When we log back in we can view the current version of Fedora CoreOS is now the latest one. The `rpm-ostree status` output will also show the older version, which still exists in case we need to rollback:

----
[core@localhost ~]$ rpm-ostree status
State: idle
AutomaticUpdatesDriver: Zincati
DriverState: active; periodically polling for updates (last checked Thu 2023-08-03 19:59:16 UTC)
Deployments:
 fedora:fedora/x86_64/coreos/stable
Version: 38.20230709.3.0 (2023-07-24T12:25:01Z)
Commit: 552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464

fedora:fedora/x86_64/coreos/stable
Version: 38.20230625.3.0 (2023-07-11T11:57:53Z)
Commit: e841d77aadb875bb801ac845a0d9b8a70b4224bdeb15e7d6c5bff1da932c0301
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464
----

NOTE: The currently booted deployment is denoted by the `` character.

You can view the differences between the two versions by running an `rpm-ostree db diff` command:

----
[core@localhost ~]$ rpm-ostree db diff
ostree diff commit from: rollback deployment (e841d77aadb875bb801ac845a0d9b8a70b4224bdeb15e7d6c5bff1da932c0301)
ostree diff commit to:   booted deployment (552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9)
Upgraded:
NetworkManager 1:1.42.6-1.fc38 -> 1:1.42.8-1.fc38
NetworkManager-cloud-setup 1:1.42.6-1.fc38 -> 1:1.42.8-1.fc38
NetworkManager-libnm 1:1.42.6-1.fc38 -> 1:1.42.8-1.fc38
NetworkManager-team 1:1.42.6-1.fc38 -> 1:1.42.8-1.fc38
NetworkManager-tui 1:1.42.6-1.fc38 -> 1:1.42.8-1.fc38
aardvark-dns 1.6.0-1.fc38 -> 1.7.0-1.fc38
...
----

== Reverting to the previous version

If the system is not functioning fully for whatever reason we can go back to the previous version:

[source,bash]
----
sudo rpm-ostree rollback --reboot
----

After logging back in after reboot we can see we are now booted back into the old deployment from before the upgrade occurred:

----
[core@localhost ~]$ rpm-ostree status
State: idle
AutomaticUpdatesDriver: Zincati
DriverState: active; periodically polling for updates (last checked Thu 2023-08-03 20:05:05 UTC)
Deployments:
 fedora:fedora/x86_64/coreos/stable
Version: 38.20230625.3.0 (2023-07-11T11:57:53Z)
Commit: e841d77aadb875bb801ac845a0d9b8a70b4224bdeb15e7d6c5bff1da932c0301
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464

fedora:fedora/x86_64/coreos/stable
Version: 38.20230709.3.0 (2023-07-24T12:25:01Z)
Commit: 552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9
GPGSignature: Valid signature by 6A51BBABBA3D5467B6171221809A8D7CEB10B464
----

And you can also verify that Zincati will not try to update to the new version we just rolled back from:

----
[core@localhost ~]$ systemctl status --full zincati.service
 zincati.service - Zincati Update Agent
Loaded: loaded (/usr/lib/systemd/system/zincati.service; enabled; preset: enabled)
Drop-In: /usr/lib/systemd/system/service.d
10-timeout-abort.conf
Active: active (running) since Thu 2023-08-03 20:05:05 UTC; 45s ago
Docs: https://github.com/coreos/zincati
Main PID: 813 (zincati)
Status: "periodically polling for updates (last checked Thu 2023-08-03 20:05:05 UTC)"
Tasks: 6 (limit: 2239)
Memory: 22.0M
CPU: 238ms
CGroup: /system.slice/zincati.service
813 /usr/libexec/zincati agent -v

Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::cincinnati] Cincinnati service: https://updates.coreos.fedoraproject.org
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::cli::agent] agent running on node '87c9ec3e0a4045a19b74b54ae5ed986a', in update group 'default'
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::update_agent::actor] registering as the update driver for rpm-ostree
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::update_agent::actor] found 1 other finalized deployment
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::update_agent::actor] deployment 38.20230709.3.0 (552de26fe0fe6a5e491f7a4163db125e3d44b144ae53a8f5f488e3f8481c46f9) will be excluded from being a future update target
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::update_agent::actor] initialization complete, auto-updates logic enabled
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::strategy] update strategy: immediate
Aug 03 20:05:05 localhost.localdomain systemd[1]: Started zincati.service - Zincati Update Agent.
Aug 03 20:05:05 localhost.localdomain zincati[813]: [INFO  zincati::update_agent::actor] reached steady state, periodically polling for updates
Aug 03 20:05:06 localhost.localdomain zincati[813]: [INFO  zincati::cincinnati] current release detected as not a dead-end
----

== Cleanup

Now let's clean up the instance. Disconnect from the machine and then destroy it:

----
virsh destroy fcos
virsh undefine --remove-all-storage fcos
----

* Reference pages
= Live ISO and PXE image reference

For an introduction to running Fedora CoreOS directly from RAM, see the xref:live-booting.adoc[provisioning guide].

== Passing the PXE rootfs to a machine

The Fedora CoreOS PXE image includes three components: a `kernel`, an `initramfs`, and a `rootfs`. All three are mandatory and the live PXE environment will not boot without them.

There are multiple ways to pass the `rootfs` to a machine:

- Specify only the `initramfs` file as the initrd in your PXE configuration, and pass an HTTP(S) or TFTP URL for the `rootfs` using the `coreos.live.rootfs_url=` kernel argument. This method requires 2 GiB of RAM, and is the recommended option unless you have special requirements.
- Specify both `initramfs` and `rootfs` files as initrds in your PXE configuration. This can be done via multiple `initrd` directives, or using additional `initrd=` parameters as kernel arguments. This method is slower than the first method and requires 4 GiB of RAM.
- Concatenate the `initramfs` and `rootfs` files together, and specify the combined file as the initrd. This method is slower and requires 4 GiB of RAM.

== Passing an Ignition config to a live PXE system

When booting Fedora CoreOS via live PXE, the kernel command line must include the arguments `ignition.firstboot ignition.platform.id=metal` to run Ignition. If running in a virtual machine, replace `metal` with the https://coreos.github.io/ignition/supported-platforms/[platform ID] for your platform, such as `qemu` or `vmware`.

There are several ways to pass an Ignition config when booting Fedora CoreOS via PXE:

- Add `ignition.config.url=<config-url>` to the kernel command line. Supported URL schemes include `http`, `https`, `tftp`, `s3`, and `gs`.

- If running virtualized, pass the Ignition config via the hypervisor, exactly as you would when booting from a disk image. Ensure the `ignition.platform.id` kernel argument is set to the https://coreos.github.io/ignition/supported-platforms/[platform ID] for your platform.

- Generate a customized version of the `initramfs` containing your Ignition config using `coreos-installer pxe customize`. For example, run:
+
[source,bash,subs="attributes"]
----
coreos-installer pxe customize --live-ignition config.ign -o custom-initramfs.img \
fedora-coreos-{stable-version}-live-initramfs.x86_64.img
----

- If you prefer to keep the Ignition config separate from the Fedora CoreOS `initramfs` image, generate a separate initrd with the low-level `coreos-installer pxe ignition wrap` command and pass it as an additional initrd. For example, run:
+
[source,bash]
----
coreos-installer pxe ignition wrap -i config.ign -o ignition.img
----
+
and then use a PXELINUX `APPEND` line similar to:
+
[source,subs="attributes"]
----
APPEND initrd=fedora-coreos-{stable-version}-live-initramfs.x86_64.img,fedora-coreos-{stable-version}-live-rootfs.x86_64.img,ignition.img ignition.firstboot ignition.platform.id=metal
----

== Passing network configuration to a live ISO or PXE system

On Fedora CoreOS, networking is typically configured via https://developer.gnome.org/NetworkManager/stable/nm-settings-keyfile.html[NetworkManager keyfiles]. If your network requires special configuration such as static IP addresses, and your Ignition config fetches resources from the network, you cannot simply include those keyfiles in your Ignition config, since that would create a circular dependency.

Instead, you can use `coreos-installer iso customize` or `coreos-installer pxe customize` with the `--network-keyfile` option to create a customized ISO image or PXE `initramfs` image which applies your network settings before running Ignition. For example:

[source,bash,subs="attributes"]
----
coreos-installer iso customize --network-keyfile custom.nmconnection -o custom.iso \
fedora-coreos-{stable-version}-live.x86_64.iso
----

If you're PXE booting and want to keep your network settings separate from the Fedora CoreOS `initramfs` image, you can also use the lower-level `coreos-installer pxe network wrap` command to create a separate initrd image, and pass that as an additional initrd. For example, run:

[source,bash]
----
coreos-installer pxe network wrap -k custom.nmconnection -o network.img
----

and then use a PXELINUX `APPEND` line similar to:

[source,subs="attributes"]
----
APPEND initrd=fedora-coreos-{stable-version}-live-initramfs.x86_64.img,fedora-coreos-{stable-version}-live-rootfs.x86_64.img,network.img ignition.firstboot ignition.platform.id=metal
----

== Passing kernel arguments to a live ISO system

If you want to modify the default kernel arguments of a live ISO system, you can use the `--live-karg-{append,replace,delete}` options to `coreos-installer iso customize`. For example, if you want to enable simultaneous multithreading (SMT) even on CPUs where that is insecure, you can run:

[source,bash,subs="attributes"]
----
coreos-installer iso customize --live-karg-delete mitigations=auto,nosmt -o custom.iso \
fedora-coreos-{stable-version}-live.x86_64.iso
----

== Extracting PXE artifacts from a live ISO image

If you want the Fedora CoreOS PXE artifacts and already have an ISO image, you can extract the PXE artifacts from it:

[source,bash,subs="attributes"]
----
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release iso extract pxe \
fedora-coreos-{stable-version}-live.x86_64.iso
----

The command will print the paths to the artifacts it extracted.

== Using the minimal ISO image

In some cases, you may want to boot the Fedora CoreOS ISO image on a machine equipped with Lights-Out Management (LOM) hardware. You can upload the ISO to the LOM controller as a virtual CD image, but the ISO may be larger than the LOM controller supports.

To avoid this problem, you can convert the ISO image to a smaller _minimal ISO image_ without the `rootfs`. Similar to the PXE image, the minimal ISO must fetch the `rootfs` from the network during boot.

Suppose you plan to host the `rootfs` image at `https://example.com/fedora-coreos-{stable-version}-live-rootfs.x86_64.img`. This command will extract a minimal ISO image and a `rootfs` from an ISO image, embedding a `coreos.live.rootfs_url` kernel argument with the correct URL:

[source,bash,subs="attributes"]
----
podman run --security-opt label=disable --pull=always --rm -v .:/data -w /data \
quay.io/coreos/coreos-installer:release iso extract minimal-iso \
--output-rootfs fedora-coreos-{stable-version}-live-rootfs.x86_64.img \
--rootfs-url https://example.com/fedora-coreos-{stable-version}-live-rootfs.x86_64.img \
fedora-coreos-{stable-version}-live.x86_64.iso \
fedora-coreos-{stable-version}-live-minimal.x86_64.iso
----
= Supported Platforms

Fedora CoreOS is provisioned via prebuilt disk images, and configured on first-boot via https://github.com/coreos/ignition[Ignition]. Each platform may require specific logic and components, thus dedicated images are provided for each supported environment. Additionally, a unique platform ID is available in the host environment for runtime introspection.

== Supported platforms

The currently supported platforms and their identifiers are listed below.

=== x86_64

* Aliyun/Alibaba Cloud (`aliyun`): Cloud platform. See xref:provisioning-aliyun.adoc[Booting on Alibaba Cloud].
* Amazon Web Services (`aws`): Cloud platform. See xref:provisioning-aws.adoc[Booting on AWS].
* Microsoft Azure (`azure`): Cloud platform. See xref:provisioning-azure.adoc[Booting on Azure].
* Microsoft Azure Stack (`azurestack`): Cloud platform.
* DigitalOcean (`digitalocean`): Cloud platform. See xref:provisioning-digitalocean.adoc[Booting on DigitalOcean].
* Exoscale (`exoscale`): Cloud platform. See xref:provisioning-exoscale.adoc[Booting on Exoscale].
* Google Cloud Platform (`gcp`): Cloud platform. See xref:provisioning-gcp.adoc[Booting on GCP].
* Hetzner (`hetzner`): Cloud platform. See xref:provisioning-hetzner.adoc[Booting on Hetzner].
* HyperV (`hyperv`): Hypervisor. See xref:provisioning-hyperv.adoc[Booting on HyperV].
* IBM Cloud (`ibmcloud`): Cloud platform. See xref:provisioning-ibmcloud.adoc[Booting on IBM Cloud].
* KubeVirt (`kubevirt`): Cloud platform. See xref:provisioning-kubevirt.adoc[Booting on KubeVirt].
* libvirt (`libvirt`): Hypervisor. See xref:provisioning-libvirt.adoc[Booting on libvirt]
* Bare metal (`metal`): With BIOS, UEFI or network boot, with standard or 4k Native disks. See xref:bare-metal.adoc[Installing on Bare Metal] or xref:live-booting-ipxe.adoc[Live-booting via iPXE].
* Nutanix (`nutanix`): Hypervisor. See xref:provisioning-nutanix.adoc[Booting on Nutanix].
* MacOS (`applehv`): Hypervisor. See xref:provisioning-applehv.adoc[Booting on MacOS].
* OpenStack (`openstack`): Cloud platform. See xref:provisioning-openstack.adoc[Booting on OpenStack].
* Oracle Cloud (`oraclecloud`): Cloud platform. See xref:provisioning-oraclecloud.adoc[Booting on Oracle Cloud].
* Proxmox (`proxmoxve`): Hypervisor. See xref:provisioning-proxmoxve.adoc[Booting on Proxmox]
* QEMU (`qemu`): Hypervisor. See xref:provisioning-qemu.adoc[Booting on QEMU]
* VirtualBox (`virtualbox`): Hypervisor. See xref:provisioning-virtualbox.adoc[Booting on VirtualBox].
* VMware ESXi, Fusion, and Workstation (`vmware`): Hypervisor. See xref:provisioning-vmware.adoc[Booting on VMware]. Fedora CoreOS images currently use https://kb.vmware.com/s/article/1003746[hardware version] 17, supporting VMware ESXi 7.0 or later, Fusion 12.0 or later, and Workstation 16.0 or later.
* Vultr (`vultr`): Cloud platform. See xref:provisioning-vultr.adoc[Booting on Vultr].

=== AArch64

* Amazon Web Services (`aws`): Cloud platform. See xref:provisioning-aws.adoc[Booting on AWS].
* Bare metal (`metal`): With UEFI or network boot, with standard or 4k Native disks. See xref:bare-metal.adoc[Installing on Bare Metal] or xref:live-booting-ipxe.adoc[Live-booting via iPXE] and xref:provisioning-raspberry-pi4.adoc[Booting on the Raspberry Pi 4].
* QEMU (`qemu`): Hypervisor. See xref:provisioning-libvirt.adoc[Booting on libvirt]
* OpenStack (`openstack`): Cloud platform. See xref:provisioning-openstack.adoc[Booting on OpenStack].

=== s390x

* IBM Cloud (`ibmcloud`): Cloud platform. See xref:provisioning-ibmcloud.adoc[Booting on IBM Cloud].
* Bare metal (`metal`): From disk or network boot. See xref:bare-metal.adoc[Installing on Bare Metal] or xref:live-booting-ipxe.adoc[Live-booting via iPXE].
* QEMU (`qemu`): Hypervisor. See xref:provisioning-libvirt.adoc[Booting on libvirt]
* OpenStack (`openstack`): Cloud platform. See xref:provisioning-openstack.adoc[Booting on OpenStack].

== Runtime introspection of platform IDs

Each Fedora CoreOS image boots with a platform-specific identifier, available on the kernel command-line. The name of the parameter is `ignition.platform.id`. The platform ID is consumed by OS components such as https://github.com/coreos/ignition[Ignition] and https://github.com/coreos/afterburn[Afterburn]. Additionally, it can be used in systemd units via https://www.freedesktop.org/software/systemd/man/systemd.unit.html#ConditionKernelCommandLine=[`ConditionKernelCommandLine=`].

See https://coreos.github.io/ignition/supported-platforms/[Ignition's Supported Platforms] and https://coreos.github.io/afterburn/platforms/[Afterburn's Supported Platforms] documentation pages for more details about which features are supported for each platform. Note that some platforms are currently supported by Ignition and Afterburn but are not yet supported by Fedora CoreOS.

The Platform ID can be introspected at runtime, as follows:

.CLI example of platform introspection
[source, bash]
----
$ grep -o ignition.platform.id='[[:alnum:]]*' /proc/cmdline

ignition.platform.id=aws
----
= Projects Using Fedora CoreOS

This a list of projects that are actively using Fedora CoreOS:

* https://docs.podman.io/en/latest/markdown/podman-machine.1.html[Podman Machine] uses Fedora CoreOS to run containers in a local environment (notably including Windows and macOS), and also has a https://docs.podman.io/en/latest/markdown/podman-machine-os.1.html[podman machine os] command that allows customization of Fedora CoreOS in a container-native way.
* https://www.okd.io[OKD] is the Community Distribution of Kubernetes that powers https://www.openshift.com/products/container-platform[Red Hat OpenShift Container Platform]. By default, Fedora CoreOS is the underlying OS used by the control plane nodes and the worker nodes.
* https://github.com/poseidon/typhoon[Typhoon] is a minimal and free Kubernetes distribution. Users of Typhoon have the option of using Fedora CoreOS as the underlying OS for their nodes.
* https://wiki.openstack.org/wiki/Magnum[OpenStack Magnum] is an OpenStack API service developed by the OpenStack Containers Team making container orchestration engines such as Docker Swarm, Kubernetes, and Apache Mesos available as first class resources in OpenStack. Fedora CoreOS is used as the underlying OS for nodes that are provisioned via Magnum.
* https://www.ovirt.org/develop/release-management/features/virt/coreos-ignition-support.html[oVirt] supports booting Fedora CoreOS nodes and has native support for https://github.com/coreos/ignition[Ignition] configurations.
* https://quay.io/[Quay.io] is using Fedora CoreOS in production to handle the job of building containers for their users.
= Signing keys and updates

All binary artifacts, ostree commits, and OS images belonging to Fedora and Fedora CoreOS (FCOS) are signed via GPG. The current set of trusted signing keys is available at https://fedoraproject.org/security/.

== Keys rotation and update barriers

At the beginning of every new Fedora major release cycle, a new signing key is generated and its public portion published on the Fedora website. The new key will be later used to sign new artifacts, replacing the currently used one. This is done in order to establish an automatic chain of trust from an older release to a more recent one, which can be possibly signed by a different newer key.

In order to make automatic updates of Fedora CoreOS work across major Fedora releases, the above set of embedded signing keys is refreshed at least once per Fedora release cycle. When that happens, an update barrier is put in place in the FCOS update graph.

The primary reason for such update barrier is to make sure that older (and possibly stale) instances automatically receive and trust newly generated keys. This is achieved by forcing such machines to progressively catch up on intermediate updates (signed by an already trusted key) before jumping to the latest published release.

== Example

Taking the Fedora 32 release cycle as an example, in the beginning FCOS images only know about signing keys for 32 and 33 majors:

----
$ grep OSTREE /etc/os-release
OSTREE_VERSION='32.20200615.3.0'

$ ls -v /usr/etc/pki/rpm-gpg/*primary | tail -3
/usr/etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-31-primary
/usr/etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-32-primary
/usr/etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-33-primary
----

Later in that release cycle, the signing key is generated for future Fedora 34 releases and added to FCOS. A barrier is put in place in FCOS update graph, for example on release `32.20200907.3.0`. Inspecting that image shows the following:

----
$ grep OSTREE /etc/os-release
OSTREE_VERSION='32.20200907.3.0'

$ ls -v /usr/etc/pki/rpm-gpg/*primary | tail -3
/usr/etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-32-primary
/usr/etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-33-primary
/usr/etc/pki/rpm-gpg/RPM-GPG-KEY-fedora-34-primary
----

With this barrier in place, older instances must first upgrade to `32.20200907.3.0` in order to make sure they know (and trust) the signing key for Fedora 34, before being able to upgrade to new releases based on that.
* Projects documentation
** https://coreos.github.io/afterburn/[Afterburn]
** https://coreos.github.io/butane/[Butane (Config Transpiler)]
** https://coreos.github.io/coreos-assembler/[CoreOS Assembler]
** https://coreos.github.io/coreos-installer/[CoreOS Installer]
** https://coreos.github.io/ignition/[Ignition]
** https://coreos.github.io/rpm-ostree/[rpm-ostree]
** https://coreos.github.io/zincati/[Zincati]
** https://ostreedev.github.io/ostree/[ostree]
* Migration notes
= Migrating from Fedora Atomic Host (FAH) to Fedora CoreOS (FCOS)

== Overview

https://www.projectatomic.io/[Fedora Atomic Host] was a system to deploy applications in containers. Existing FAH users are encouraged to migrate to FCOS, as the project has reached its end-of-life.

FAH used `cloud-init` for provisioning, which required users to provide a `cloud-config` file as userdata for configuration of the instance. Since FCOS Ignition and `cloud-init` are different and have overlapping feature sets, it is not trivial to convert `cloud-init` files to Ignition. Currently, there is no tool for this conversion, so you must manually convert `cloud-init` configs to Butane configs. Refer to link:https://coreos.github.io/butane/specs/[Butane Specification] for an explanation of the available configuration options.

== Converting `cloud-init` and `cloud-config` userdata

The following examples show the difference between FAH userdata and user configuration with Butane.

.Example of FAH userdata file:
----
#cloud-config
password: atomic
ssh_pwauth: True
chpasswd: { expire: False }

ssh_authorized_keys:
- ssh-rsa ...
----

This can be manually translated into a `passwd` node within the Butane config:

.Example of users:
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
passwd:
users:
- name: core
password_hash: "$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm..."
ssh_authorized_keys:
- "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq......."
groups: [ sudo, docker ]
----

NOTE: Fedora CoreOS disables password login over SSH by default. It is strongly recommended to only use key authentication. Setting passwords can be useful however for logging into the console directly.

== Converting storage definitions

With FAH, you could configure additional storage for the system with either `cloud-init` or  `docker-storage-setup` via the `/etc/sysconfig/docker-storage-setup` file. With FCOS, you should configure additional storage at provisioning time via Ignition in the `storage` node of the Butane config.
= Migrating from CoreOS Container Linux (CL) to Fedora CoreOS (FCOS)

Fedora CoreOS is the official successor of CoreOS Container Linux, which https://coreos.com/os/eol/[reached its end of life] on May 26, 2020. This page attempts to document the differences between CL and FCOS to ease the transition to FCOS.

:toc:

== Introduction

To migrate from CL to FCOS, you must convert your old Container Linux Configs, Ignition configs, or `cloud-config` files to a xref:producing-ign.adoc[Butane config] and adapt the contents for FCOS. Since many of the configuration details have changed, you should reference this page and the https://github.com/coreos/fedora-coreos-tracker/issues/159[CL migration issue] on GitHub.

== Installation changes

The following changes have been made to the installation process:

* The `coreos-install` script has been replaced with https://github.com/coreos/coreos-installer[`coreos-installer`]. It offers similar functionality.
* The `coreos.autologin` kernel command-line parameter is not currently supported in FCOS. For access recovery purposes, there are instructions available xref:access-recovery.adoc[here].
* Certain CL platforms, such as Vagrant, are not yet supported in FCOS. Refer to the https://fedoraproject.org/coreos/download/[Download page] to see the available image types.

== Software package changes

* `etcd` is not included in FCOS. Refer to xref:running-containers.adoc#running-etcd[Running etcd] for instructions to run it as a container on FCOS.
* `flannel` is not included in FCOS.
* The Podman container runtime is included in FCOS and is the recommended container runtime. The rkt container runtime is not included.
* FCOS does not have a recommended mechanism to select the version of `docker`.
* Network configuration is now handled by NetworkManager instead of `systemd-networkd`.
* For time synchronization, use https://docs.fedoraproject.org/en-US/fedora/rawhide/system-administrators-guide/servers/Configuring_NTP_Using_the_chrony_Suite/[`chronyd`] rather than `ntpd` or `systemd-timesyncd`.
* Automatic updates are now coordinated by Zincati, as described in the https://coreos.github.io/zincati/usage/auto-updates/[Zincati documentation]. The rollback mechanism (via grub) is now provided by https://coreos.github.io/rpm-ostree/[`rpm-ostree`].
* The functionality of the reboot manager (`locksmith`) is rolled into https://coreos.github.io/zincati/[Zincati].
* The `update-ssh-keys` tool is not provided on FCOS. sshd uses a xref:authentication.adoc#ssh-key-locations[helper program] to read key files directly out of `~/.ssh/authorized_keys.d`.

== Configuration changes

When writing Butane configs, note the following changes:

* `coreos-metadata` is now https://coreos.github.io/afterburn/[Afterburn]. The prefix of the metadata variable names has changed from `COREOS_` to `AFTERBURN_`, and the following platform names have changed:
** `EC2` is now `AWS`
** `GCE` is now `GCP`
+
For more info, see the https://coreos.github.io/afterburn/usage/attributes/[Afterburn documentation].

* By default, FCOS does not allow password logins via SSH. We recommend xref:authentication.adoc#using-an-ssh-key[configuring SSH keys] instead. If needed, you can xref:authentication.adoc#enabling-ssh-password-authentication[enable SSH password authentication].
* Because `usermod` is not yet fully-functional on FCOS, there is a `docker` group in the `/etc/group` file. This is a stop-gap measure to facilitate a smooth transition to FCOS. The team is working on a more functional `usermod`, at which time the `docker` group will no longer be included by default. See the https://github.com/coreos/fedora-coreos-tracker/issues/2[docker group issue].
* There is no way to create directories below the `/` directory. Changes are restricted to `/etc` and `/var`. Refer to the documentation for the `storage` node of the Butane config for details about writing directories and files to FCOS.
* Butane configs no longer have a separate section for network configuration. Use the Butane `files` section to write a https://developer.gnome.org/NetworkManager/stable/nm-settings-keyfile.html[NetworkManager key file] instead.

== Operator notes

* FCOS provides https://fedoramagazine.org/fedora-coreos-out-of-preview/[best-effort stability], and may occasionally include regressions or breaking changes for some use cases or workloads.
* CL had three release channels: `alpha`, `beta`, and `stable`. The FCOS production https://github.com/coreos/fedora-coreos-tracker/blob/main/Design.md#release-streams[release streams] are `next`, `testing`, and `stable`, with somewhat different semantics.
* In general, SELinux confinement should work the same as in Fedora.
* To deploy an Ignition config as part of a PXE image (a "custom OEM" in CL terminology), follow the https://coreos.com/os/docs/latest/booting-with-pxe.html#adding-a-custom-oem[same process] as in CL, but place the `config.ign` file in the root of the archive.
* In CL, metrics/telemetry data was collected by the update mechanism. In FCOS, nodes are counted (without unique identifiers) via the xref:counting.adoc[Count Me] mechanism.
* Cloud CLI clients are not included in FCOS. There is an initiative to create a "tools" container to run on FCOS.
* When opening an existing file in a sticky directory, the behavior differs from CL. See https://github.com/systemd/systemd/commit/2732587540035227fe59e4b64b60127352611b35[the relevant systemd commit].
* CL left Simultaneous Multi-Threading (SMT) enabled but advised users to turn it off if their systems were vulnerable to certain issues such as https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html[L1TF] or https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html[MDS]. By default, FCOS https://github.com/coreos/fedora-coreos-tracker/blob/main/Design.md#automatically-disable-smt-when-needed-to-address-vulnerabilities[automatically disables SMT] for vulnerable systems.
* In general, `docker` uses the default configuration from Fedora, which is different under many aspects. Notably the logging driver is set to `journald` and live-restore is enabled.

== Implementation notes
//* Partition layout differences. CL is at https://coreos.com/os/docs/latest/sdk-disk-partitions.html. I can't make heads or tails of the results of the discussions in https://github.com/coreos/fedora-coreos-tracker/issues/94.
* The default filesystem on CL was `ext4`. On FCOS, the default is `xfs`.
* While CL used systemd socket activation for `sshd`, FCOS starts `sshd` at startup by default.
* CL had an "OEM partition" at `/usr/share/oem` with a user-customizable GRUB config and some additional tools, but FCOS does not have this.
//* Filesystem resizing differences. Need more info on FCOS side.
= Fedora CoreOS Frequently Asked Questions

If you have questions other than those mentioned here or want to discuss
further, join us in our Matrix room,
link:https://chat.fedoraproject.org/#/room/#coreos:fedoraproject.org[#coreos:fedoraproject.org],
or on our https://discussion.fedoraproject.org/tag/coreos[discussion board].
Please refer back here as some questions and answers will likely get
updated.

== What is Fedora CoreOS?

Fedora CoreOS is an automatically updating, minimal, monolithic,
container-focused operating system, designed for clusters but also
operable standalone, optimized for Kubernetes but also great without it.
Its goal is to provide the best container host to run containerized
workloads securely and at scale.

== How does Fedora CoreOS relate to RHEL CoreOS?

Fedora CoreOS is a freely available, community distribution that is the
upstream basis for RHEL CoreOS. While Fedora CoreOS embraces a
variety of containerized use cases, RHEL CoreOS provides a
focused OS for OpenShift, released and life-cycled in tandem
with the platform.

== How does Fedora CoreOS relate to Fedora Bootc?

https://docs.fedoraproject.org/en-US/bootc/[Fedora Bootc] is
a Fedora project aimed at building Fedora and CentOS-based
https://containers.github.io/bootable/[bootable containers]. The goal is to
eventually have Fedora CoreOS build on top of Fedora Bootc both literally and in
the broader sense of being part of the same ecosystem. The roadmap for this can
be found https://github.com/coreos/fedora-coreos-tracker/issues/1726[on GitHub].

Fedora CoreOS adds
https://docs.fedoraproject.org/en-US/fedora-coreos/update-streams/[a stream model],
platform-specific integration and disk images, provisioning via Ignition, and
more tooling around automatic updates. The goal is also to have Fedora CoreOS
integrate more strongly with the Kubernetes ecosystem.

If you find yourself needing to heavily customize Fedora CoreOS beyond what
https://docs.fedoraproject.org/en-US/fedora-coreos/os-extensions/[OS extensions]
provide, it might make more sense to instead build on top of Fedora Bootc
directly. It will eventually be possible to layer Ignition and e.g. Zincati
on top if desired. These tools are intended to be used generically and are not
strictly tied to Fedora CoreOS.

== What are the communication channels around Fedora CoreOS?

We have the following new communication channels around Fedora CoreOS:

* Mailing list: https://lists.fedoraproject.org/archives/list/coreos@lists.fedoraproject.org/[coreos@lists.fedoraproject.org]
* Operational notices for Fedora CoreOS: https://lists.fedoraproject.org/archives/list/coreos-status@lists.fedoraproject.org/[coreos-status@lists.fedoraproject.org]
* Matrix: https://chat.fedoraproject.org/#/room/#coreos:fedoraproject.org[#coreos:fedoraproject.org]
* Forum at https://discussion.fedoraproject.org/tag/coreos
* Website at https://fedoraproject.org/coreos/
* Twitter at https://twitter.com/fedoracoreos[@fedoracoreos] (Fedora CoreOS news), https://twitter.com/fedora[@fedora] (all Fedora and other relevant news)

There is a community meeting that happens every week. See the https://calendar.fedoraproject.org/CoreOS/[Fedora CoreOS fedocal] for the most up-to-date information.

If you think you have found a problem with Fedora CoreOS, file an issue in our https://github.com/coreos/fedora-coreos-tracker/issues[issue tracker].

== Where can I download Fedora CoreOS?

Fedora CoreOS artifacts are available at https://fedoraproject.org/en/coreos/download/[fedoraproject.org].

== Does Fedora CoreOS update itself automatically?

Yes, Fedora CoreOS comes with automatic
updates and regular releases. Multiple update channels are provided
catering to different users' needs. Fedora CoreOS provides a node-update
service based on rpm-ostree technologies, with a server component that
can be optionally self-hosted.

== How are Fedora CoreOS nodes provisioned? Can I re-use existing cloud-init configurations?

Fedora CoreOS is provisioned with Ignition. Existing cloud-init
configurations are not supported and will need to be migrated into their
Ignition equivalent.

== What data persists across upgrades and reboots?

The directories `/etc` and `/var` are mounted as read-write which lets users
write and modify files.

The directory `/etc` may be changed by deployments, but will not override user
made changes. The content under `/var` is left untouched by rpm-ostree when
applying upgrades or rollbacks. For more information, refer to the
https://docs.fedoraproject.org/en-US/fedora-coreos/storage/#_mounted_filesystems[Mounted Filesystems]
section.

== Which container runtimes are available on Fedora CoreOS?

Fedora CoreOS includes Docker and podman by default.
Based on community engagement and support this list could
change over time.

== Can I run Kubernetes on Fedora CoreOS?

Yes. However, Fedora CoreOS does not include a specific container
orchestrator (or version of Kubernetes) by default.

== How do I run custom applications on Fedora CoreOS?

On Fedora CoreOS, containers are the way to install and configure any
software not provided by the base operating system. The package layering
mechanism provided by rpm-ostree will continue to exist for use in
debugging a Fedora CoreOS machine, but we strongly discourage its use.
For more about this, please refer to xref:running-containers.adoc[documentation].

== Where is my preferred tool for troubleshooting?

The FCOS image is kept minimal by design. Not every troubleshooting tool is
included by default. Instead, it is recommended to use the `toolbox` utility.

xref:debugging-with-toolbox.adoc[Debugging with Toolbx].

== How do I coordinate cluster-wide OS updates?

The Zincati update manager includes a
https://coreos.github.io/zincati/usage/updates-strategy/#lock-based-strategy[lock-based updates strategy]
that supports multiple backends.

OKD's https://github.com/openshift/machine-config-operator[Machine Config Operator (MCO)]
automatically handles updates of Fedora CoreOS in OKD clusters.
The MCO additionally covers reconciliation of machine configuration changes.

== How do I upload Fedora CoreOS to private AWS EC2 regions?

Fedora CoreOS today is only uploaded to the standard AWS regions. For regions
in other AWS partitions like GovCloud and AWS China, you must upload the images
yourself.

Note that Fedora CoreOS uses a unified BIOS/UEFI partition layout. As such, it
is not compatible with the `aws ec2 import-image` API (for more information,
see https://github.com/openshift/os/pull/396[related discussions]). Instead,
you must use `aws ec2 import-snapshot` combined with `aws ec2 register-image`.

To learn more about these APIs, see the AWS documentation for
https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-import-snapshot.html[importing snapshots]
and
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-ebs.html#creating-launching-ami-from-snapshot[creating EBS-backed AMIs].

== Can I run containers via docker and podman at the same time?

No. Running containers via `docker` and `podman` at the same time can cause
issues and unexpected behavior. We highly recommend against trying to use them
both at the same time.

It is worth noting that in Fedora CoreOS we have `docker.service`
disabled by default but it is easily started if anything communicates
with the `/var/run/docker.sock` because `docker.socket` is enabled by
default. This means that if a user runs any `docker` command (via
`sudo docker`) then the daemon will be activated.

In https://github.com/coreos/fedora-coreos-tracker/issues/408[coreos/fedora-coreos-tracker#408]
it was pointed out that because of socket activation users who are
using `podman` for containers could unintentionally start the docker
daemon. This could weaken the security of the system because of the
interaction of both container runtimes with the firewall on the system.
To prevent making this mistake you can disable `docker` completely by
masking the `docker.service` systemd unit.

.Example Butane config for disabling docker.service
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: docker.service
mask: true
----

== Are Fedora CoreOS x86_64 disk images hybrid BIOS+UEFI bootable?

The x86_64 images we provide can be used for either BIOS (legacy) boot or UEFI boot. They contain a hybrid BIOS/UEFI partition setup that allows them to be used for either. The exception to that is the `metal4k` 4k native image, which is targeted at disks with 4k sectors and https://github.com/coreos/coreos-assembler/blob/12029fea7798fa5d3535eafcf8c3d02f9a6095e4/src/cmd-buildextend-metal#L200-L202[does not have a BIOS boot partition] because 4k native disks are https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/hard-drives-and-partitions#advanced-format-drives[only supported with UEFI].

== What's the difference between Ignition and Butane configurations?

Ignition configuration is a low-level interface used to define the whole set of customizations for an instance.
It is primarily meant as a machine-friendly interface, with content encoded as JSON and a fixed structure defined via JSON Schema.
This JSON configuration is processed by each FCOS instance upon first boot.

Many high-level tools exist that can produce an Ignition configuration starting from their own specific input formats,
such as `terraform`, `matchbox`, `openshift-installer`, and Butane.

Butane is one such high-level tool.
It is primarily meant as a human-friendly interface, thus defining its own richer configuration entries and using YAML documents as input.
This YAML configuration is never directly processed by FCOS instances (only the resulting Ignition configuration is).

Although similar, Ignition configurations and Butane ones do not have the same structure; thus, converting between them is not just a direct YAML-to-JSON translation, but it involves additional logic.
Butane exposes several customization helpers (e.g. distribution specific entries and common abstractions) that are not present in Ignition and make the formats not interchangeable.
Additionally, the different formats (YAML for Butane, JSON for Ignition) help to avoid mixing up inputs by mistake.

== What is the format of the version number?

This is covered in detail in the https://github.com/coreos/fedora-coreos-tracker/blob/main/Design.md#version-numbers[design docs].

The summary is that Fedora CoreOS uses the format `X.Y.Z.A`

* `X` is the Fedora major version (i.e. `32`)
* `Y` is the datestamp that the package set was snapshotted from Fedora (i.e. `20200715`)
* `Z` is a code number used by official builds
** `1` for the `next` stream
** `2` for the `testing` stream
** `3` for the `stable` stream
* `A` is a revision number that is incremented for each new build with the same `X.Y.Z` parameters

The version numbering scheme is subject to change and is not intended to be parsed by machine.

== Why is the `dnsmasq.service` systemd unit masked?

We have found that the dnsmasq binary can be used for several host
applications, including podman and NetworkManager. For this reason we
include the dnsmasq package in the base OSTree layer, but we discourage
the use of the `dnsmasq.service` in the host by masking it with
`systemctl mask dnsmasq.service`.

_"Why do you mask the service?"_

dnsmasq is useful for running a DHCP/DNS/TFTP server for external clients
(i.e. not local to the host), too, but that is something we'd prefer users
to do in a container. Putting the service in a container insulates the
hosted service from breakage as a result of host layer changes. For
example, if NetworkManager and podman stopped using dnsmasq, we would
remove it from the host and the service you depend on would cease to
work.

_"But, I really want to use it!"_

We don't recommend it, but if you really want to use it you can just
unmask and enable it:

.Example Butane config for unmasking dnsmasq.service
[source,yaml,subs="attributes"]
----
variant: fcos
version: {butane-latest-stable-spec}
systemd:
units:
- name: dnsmasq.service
mask: false
enabled: true
----

For more information see
https://github.com/coreos/fedora-coreos-tracker/issues/519[the tracker issue discussion].

== Why is the `systemd-repart.service` systemd unit masked?

https://www.freedesktop.org/software/systemd/man/systemd-repart.html[systemd-repart]
is a tool to grow and add partitions to a partition table. On Fedora CoreOS, we
only support using Ignition to create partitions, filesystems and mount points,
thus systemd-repart is masked by default.

Ignition runs on first boot in the initramfs and is aware of Fedora CoreOS
specific disk layout. It is also capable of reconfiguring the root filesystem
(from xfs to ext4 for example), setting up LUKS, etc... See the
xref:storage.adoc[Configuring Storage] page for examples.

See the xref:faq.adoc#_why_is_the_dnsmasq_service_systemd_unit_masked[Why is the `dnsmasq.service` systemd unit masked]
entry for an example config to unmask this unit.

[#wifi-fw]
== How do I keep dropped wireless firmware?

Some Wi-Fi firmwares were split into subpackages in Fedora 39 and Fedora 40. Fedora CoreOS will keep them in until Fedora 41, but display a warning message in the console if `NetworkManager-wifi` is layered without any other Wi-Fi firmware packages layered.

To request the Wi-Fi firmware stay installed even when Fedora CoreOS drops these packages please follow the xref:sysconfig-enabling-wifi.adoc#_on_an_existing_fedora_coreos_system[steps to perform Wi-Fi enablement on an existing system].

Once the packages are requested you can now disable the warning so it won't be checked on subsequent boots.

[source, text]
----
sudo systemctl disable coreos-check-wireless-firmwares.service
----
